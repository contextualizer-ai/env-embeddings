# Bioproject Quality Analysis

Rank bioprojects by how well their MIxS metadata annotations match satellite imagery embeddings.

## Overview

This tool analyzes each bioproject's annotation quality by:
1. Grouping samples by bioproject
2. Computing pairwise similarities (Google Earth vs ENVO) within each project
3. Measuring agreement between satellite imagery and semantic annotations
4. Ranking bioprojects by quality score

**High quality score** = satellite imagery and ENVO annotations agree (both similar or both different)
**Low quality score** = disagreement between physical location and semantic annotation

## Usage

### Via CLI (most flexible)

```bash
uv run env-embeddings rank-bioprojects data/your_embeddings.csv \
  --envo-scale envo_broad_scale \
  --mongo-host localhost \
  --mongo-port 27017 \
  --output data/rankings.csv
```

**Options:**
- `--envo-scale`: Which ENVO dimension to analyze
  - `envo_broad_scale` (default): Broad environmental context
  - `envo_local_scale`: Fine-grained local features
  - `envo_medium`: Intermediate environmental medium
- `--mongo-host`: MongoDB hostname (default: localhost)
- `--mongo-port`: MongoDB port (default: 27017)
- `--output`: Output CSV path (default: auto-generated)

### Via just recipe (convenient)

```bash
# Use defaults (data/with_both_embeddings.csv, envo_broad_scale)
just rank-bioprojects

# Custom file
just rank-bioprojects data/custom.csv

# Custom file and ENVO scale
just rank-bioprojects data/custom.csv envo_local_scale
```

### Programmatically

```python
from env_embeddings.bioproject_quality import rank_bioprojects
from pathlib import Path

results = rank_bioprojects(
    csv_file=Path("data/biosamples_with_embeddings.csv"),
    output_file=Path("data/rankings.csv"),
    envo_scale='envo_broad_scale',
    mongo_host='localhost',
    mongo_port=27017
)
```

## Requirements

### Input Data

**CSV file must contain:**
- `google_earth_embeddings`: 64-dim satellite embedding vectors (as strings)
- `envo_broad_scale_embedding`: 1536-dim ENVO embeddings for broad scale
- `envo_local_scale_embedding`: 1536-dim ENVO embeddings for local scale
- `envo_medium_embedding`: 1536-dim ENVO embeddings for medium
- `accession`: Biosample accession (for MongoDB lookup)

Generated by:
```bash
uv run env-embeddings add-google-embeddings-csv input.csv --max-rows 1000
uv run env-embeddings add-envo-embeddings-csv output_with_google.csv
```

### MongoDB

**Required collection:** `ncbi_metadata.sra_biosamples_bioprojects`

**Schema:**
```javascript
{
  biosample_accession: "SAMD00123456",
  bioproject_accession: "PRJNA123456"
}
```

Start MongoDB locally:
```bash
mongod --dbpath /path/to/data --port 27017
```

## Output

### Console Output

```
================================================================================
BIOPROJECT QUALITY RANKING
================================================================================
Total bioprojects analyzed: 142
Using ENVO scale: envo_broad_scale

Top 10 highest quality bioprojects:
   bioproject  n_samples  quality_score  mean_disagreement  spearman_r
0  PRJNA123456         25          0.850              0.150       0.721
1  PRJNA234567         18          0.823              0.177       0.692
...

Top 10 lowest quality bioprojects:
   bioproject  n_samples  quality_score  mean_disagreement  spearman_r
132  PRJNA999888        12          0.312              0.688       0.145
133  PRJNA888777         8          0.289              0.711       0.092
...
```

### CSV Output

Saved to `data/<filename>_bioproject_rankings.csv`:

| Column | Description |
|--------|-------------|
| `bioproject` | Bioproject accession (e.g., PRJNA123456) |
| `n_samples` | Number of samples in this bioproject |
| `n_pairs` | Number of pairwise comparisons computed |
| `quality_score` | **0-1, higher is better** (1 - mean_disagreement) |
| `mean_disagreement` | Average \|GE_sim - ENVO_sim\| |
| `mean_ge_similarity` | Average pairwise Google Earth similarity |
| `mean_envo_similarity` | Average pairwise ENVO similarity |
| `pearson_r` | Pearson correlation (linear) |
| `pearson_p` | Pearson p-value |
| `spearman_r` | Spearman correlation (rank-based) |
| `spearman_p` | Spearman p-value |

## Interpretation

### Quality Score

**0.8-1.0: Excellent**
- Satellite imagery and ENVO annotations are highly consistent
- When samples are geographically close, they have similar ENVO terms
- When samples are geographically distant, they have different ENVO terms

**0.5-0.8: Good**
- Moderate agreement between physical location and annotations
- Some inconsistencies but generally reliable

**0.3-0.5: Poor**
- Significant disagreement between imagery and annotations
- May indicate:
  - Incorrect ENVO term selection
  - Copy-paste errors across samples
  - Generic/vague annotations
  - Samples from same location with different annotations

**< 0.3: Very Poor**
- Strong disagreement - likely systematic annotation problems
- Requires manual review and correction

### Correlation Metrics

**Pearson r** (linear correlation):
- Measures if similarity scores increase/decrease together linearly
- Sensitive to outliers

**Spearman ρ** (rank correlation):
- Measures if ranking of similarities matches
- More robust to outliers
- Often higher than Pearson (indicates non-linear relationship)

## Example Analysis Workflow

```bash
# 1. Generate embeddings for 1000 samples
uv run env-embeddings add-google-embeddings-csv \
  data/biosamples.csv \
  --max-rows 1000 \
  --random \
  --output data/with_google.csv

uv run env-embeddings add-envo-embeddings-csv \
  data/with_google.csv \
  --output data/with_both.csv

# 2. Rank bioprojects
just rank-bioprojects data/with_both.csv

# 3. Review results
# - High quality projects = good annotation practices
# - Low quality projects = need curation review

# 4. Try different ENVO scales
just rank-bioprojects data/with_both.csv envo_local_scale
just rank-bioprojects data/with_both.csv envo_medium
```

## Implementation Details

### Algorithm

For each bioproject with N samples:
1. Compute all N×(N-1)/2 pairwise similarities:
   - Google Earth: cosine similarity of 64-dim vectors
   - ENVO: cosine similarity of 1536-dim vectors
2. For each pair, compute disagreement = |GE_sim - ENVO_sim|
3. Quality score = 1 - mean(disagreements)
4. Compute Pearson and Spearman correlations

### Minimum Requirements

- **2+ samples per bioproject** (need pairs for comparison)
- **3+ pairs for correlations** (statistical validity)
- Bioprojects with < 2 samples are excluded from analysis

### Performance

- **Small dataset (100-500 samples)**: < 1 minute
- **Medium dataset (1,000-5,000 samples)**: 2-5 minutes
- **Large dataset (10,000+ samples)**: 10-30 minutes

Bottleneck: MongoDB query for biosample→bioproject mapping

## Troubleshooting

### "Error connecting to MongoDB"
```bash
# Check MongoDB is running
mongosh localhost:27017

# Verify collection exists
use ncbi_metadata
db.sra_biosamples_bioprojects.countDocuments()
```

### "No bioprojects found"
- Check CSV has `accession` column
- Verify accessions match MongoDB `biosample_accession` field
- Try without `--max-rows` to get more samples

### "Total bioprojects analyzed: 0"
- All bioprojects have < 2 samples
- Increase sample size with larger `--max-rows`
- Use `--random` sampling to get diverse bioprojects

## Related Documentation

- [FINDINGS.md](FINDINGS.md) - Analysis methodology and correlation findings
- [PERFORMANCE_OPTIMIZATIONS.md](PERFORMANCE_OPTIMIZATIONS.md) - Caching and sampling strategies
- [Issue #38](https://github.com/contextualizer-ai/env-embeddings/issues/38) - Performance improvements for large datasets
