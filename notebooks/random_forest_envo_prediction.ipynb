{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest: Predicting ENVO Local Scale from Google Earth Embeddings\n",
    "\n",
    "This notebook trains a Random Forest classifier to predict environmental ontology (ENVO) local scale terms from Google Earth Engine satellite embeddings.\n",
    "\n",
    "**Question**: Can geographic/satellite imagery data predict environmental classification labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/satisfying_biosamples_normalized_with_google_embeddings_with_envo_embeddings.csv')\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in key columns\n",
    "print(\"Missing values:\")\n",
    "print(df[['google_earth_embeddings', 'env_local_scale']].isna().sum())\n",
    "\n",
    "# Filter to rows with both embeddings and labels\n",
    "df_clean = df[df['google_earth_embeddings'].notna() & df['env_local_scale'].notna()].copy()\n",
    "print(f\"\\nRows with both Google Earth embeddings and env_local_scale: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_embedding(embedding_str):\n",
    "    \"\"\"\n",
    "    Parse embedding string to numpy array.\n",
    "    \n",
    "    Args:\n",
    "        embedding_str: String representation of embedding list\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of embedding values\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(embedding_str, str):\n",
    "            embedding_list = ast.literal_eval(embedding_str)\n",
    "        else:\n",
    "            embedding_list = embedding_str\n",
    "        return np.array(embedding_list, dtype=np.float32)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Parse Google Earth embeddings\n",
    "print(\"Parsing Google Earth embeddings...\")\n",
    "df_clean['ge_embedding'] = df_clean['google_earth_embeddings'].apply(parse_embedding)\n",
    "\n",
    "# Remove any rows where parsing failed\n",
    "df_clean = df_clean[df_clean['ge_embedding'].notna()].copy()\n",
    "\n",
    "print(f\"Rows with valid embeddings: {len(df_clean)}\")\n",
    "if len(df_clean) > 0:\n",
    "    embedding_dim = len(df_clean.iloc[0]['ge_embedding'])\n",
    "    print(f\"Google Earth embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine env_local_scale values\n",
    "print(\"=== TARGET VARIABLE: env_local_scale ===\")\n",
    "print(f\"\\nUnique values: {df_clean['env_local_scale'].nunique()}\")\n",
    "print(f\"\\nValue counts:\")\n",
    "value_counts = df_clean['env_local_scale'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nExample values:\")\n",
    "for val in df_clean['env_local_scale'].unique()[:10]:\n",
    "    print(f\"  - {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "value_counts.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Distribution of ENVO Local Scale Terms', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('ENVO Local Scale Term')\n",
    "ax.set_ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nClass balance analysis:\")\n",
    "print(f\"Most common class: {value_counts.iloc[0]} samples ({value_counts.iloc[0]/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Least common class: {value_counts.iloc[-1]} samples ({value_counts.iloc[-1]/len(df_clean)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix X from embeddings\n",
    "X = np.vstack(df_clean['ge_embedding'].values)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Create target vector y\n",
    "y = df_clean['env_local_scale'].values\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "print(f\"\\nNumber of samples: {len(X)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if stratification is possible\n",
    "min_class_count = value_counts.min()\n",
    "use_stratify = min_class_count >= 2  # Need at least 2 samples per class for stratification\n",
    "\n",
    "print(f\"Minimum class count: {min_class_count}\")\n",
    "print(f\"Using stratified split: {use_stratify}\")\n",
    "\n",
    "# Split the data\n",
    "test_size = 0.2\n",
    "if use_stratify:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest Classifier...\")\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = rf_classifier.predict(X_train)\n",
    "y_test_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=== MODEL PERFORMANCE ===\")\n",
    "print(f\"Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"\\nDifference (overfitting check): {train_accuracy - test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on training set\n",
    "if len(X_train) >= 5:  # Need enough samples for CV\n",
    "    cv_folds = min(5, len(X_train))\n",
    "    print(f\"\\n=== CROSS-VALIDATION ({cv_folds}-fold) ===\")\n",
    "    cv_scores = cross_val_score(rf_classifier, X_train, y_train, cv=cv_folds, scoring='accuracy')\n",
    "    print(f\"CV Accuracy scores: {cv_scores}\")\n",
    "    print(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "else:\n",
    "    print(\"\\nDataset too small for cross-validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\n=== CLASSIFICATION REPORT (Test Set) ===\")\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "class_labels = rf_classifier.classes_\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, yticklabels=class_labels,\n",
    "            ax=ax, cbar_kws={'label': 'Count'})\n",
    "ax.set_title('Confusion Matrix: Predicted vs Actual ENVO Local Scale', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "feature_names = [f\"GE_dim_{i}\" for i in range(len(feature_importances))]\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=== TOP 20 MOST IMPORTANT FEATURES ===\")\n",
    "print(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 20 feature importances\n",
    "top_n = 20\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "importance_df.head(top_n).plot(x='feature', y='importance', kind='barh', ax=ax, legend=False)\n",
    "ax.set_title(f'Top {top_n} Most Important Google Earth Embedding Dimensions', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature (Embedding Dimension)')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "y_test_proba = rf_classifier.predict_proba(X_test)\n",
    "max_probabilities = y_test_proba.max(axis=1)\n",
    "\n",
    "print(\"=== PREDICTION CONFIDENCE ===\")\n",
    "print(f\"Mean prediction confidence: {max_probabilities.mean():.3f}\")\n",
    "print(f\"Std prediction confidence: {max_probabilities.std():.3f}\")\n",
    "print(f\"Min prediction confidence: {max_probabilities.min():.3f}\")\n",
    "print(f\"Max prediction confidence: {max_probabilities.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction confidence distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(max_probabilities, bins=20, alpha=0.7, edgecolor='black')\n",
    "ax.axvline(max_probabilities.mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {max_probabilities.mean():.3f}')\n",
    "ax.set_title('Distribution of Prediction Confidence Scores', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Maximum Probability (Confidence)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions\n",
    "print(\"\\n=== EXAMPLE PREDICTIONS ===\")\n",
    "n_examples = min(10, len(y_test))\n",
    "for i in range(n_examples):\n",
    "    true_label = y_test[i]\n",
    "    pred_label = y_test_pred[i]\n",
    "    confidence = max_probabilities[i]\n",
    "    correct = \"✓\" if true_label == pred_label else \"✗\"\n",
    "    print(f\"\\n{correct} Sample {i+1}:\")\n",
    "    print(f\"  True: {true_label}\")\n",
    "    print(f\"  Predicted: {pred_label} (confidence: {confidence:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EXPERIMENT SUMMARY ===\")\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  - Total samples: {len(df_clean)}\")\n",
    "print(f\"  - Training samples: {len(X_train)}\")\n",
    "print(f\"  - Test samples: {len(X_test)}\")\n",
    "print(f\"  - Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"  - Feature dimensions: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\nModel:\")\n",
    "print(f\"  - Algorithm: Random Forest Classifier\")\n",
    "print(f\"  - Number of trees: {rf_classifier.n_estimators}\")\n",
    "print(f\"  - Max depth: {rf_classifier.max_depth}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  - Training accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"  - Test accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"  - Mean prediction confidence: {max_probabilities.mean():.3f}\")\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "if test_accuracy > 0.7:\n",
    "    print(\"  ✓ Google Earth embeddings show STRONG predictive power for ENVO local scale\")\n",
    "elif test_accuracy > 0.5:\n",
    "    print(\"  ~ Google Earth embeddings show MODERATE predictive power for ENVO local scale\")\n",
    "else:\n",
    "    print(\"  ✗ Google Earth embeddings show LIMITED predictive power for ENVO local scale\")\n",
    "\n",
    "if train_accuracy - test_accuracy > 0.2:\n",
    "    print(\"  ⚠ Significant overfitting detected - model may not generalize well\")\n",
    "elif train_accuracy - test_accuracy > 0.1:\n",
    "    print(\"  ⚠ Moderate overfitting - consider regularization\")\n",
    "else:\n",
    "    print(\"  ✓ Good generalization - minimal overfitting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
