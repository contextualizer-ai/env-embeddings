{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest: Multi-Source ENVO Prediction\n",
    "\n",
    "**Goal**: Predict ENVO environmental triad terms from satellite imagery across GOLD and NMDC.\n",
    "\n",
    "**Key Questions**:\n",
    "1. How well can we predict each ENVO scale (broad/local/medium)?\n",
    "2. Do different sources show different patterns?\n",
    "3. What's the impact of removing exact duplicates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "sys.path.insert(0, str(Path('../src').absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_embeddings.rf_analysis import (\n",
    "    load_source_data,\n",
    "    analyze_source,\n",
    "    create_comparison_table,\n",
    "    print_summary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data sources (GOLD and NMDC first, NCBI later)\n",
    "SOURCES = {\n",
    "    'GOLD': Path('../data/gold_flattened_biosamples_for_env_embeddings_202510061108_complete.csv'),\n",
    "    'NMDC': Path('../data/nmdc_flattened_biosample_for_env_embeddings_202510061052_complete.csv'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all sources\n",
    "datasets = {}\n",
    "for source_name, file_path in SOURCES.items():\n",
    "    df = load_source_data(file_path, source_name, deduplicate=True)\n",
    "    if df is not None:\n",
    "        datasets[source_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for all sources\n",
    "all_results = {}\n",
    "for source_name, df in datasets.items():\n",
    "    all_results[source_name] = analyze_source(df, source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_df = create_comparison_table(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDetailed Results:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print actionable summary\n",
    "print_summary(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Test Accuracy by Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for source in comparison_df['Source'].unique():\n",
    "    data = comparison_df[comparison_df['Source'] == source]\n",
    "    ax.plot(data['Scale'], data['Test_Acc'], \n",
    "            marker='o', linewidth=2, markersize=10, label=source)\n",
    "\n",
    "ax.set_title('Test Accuracy by ENVO Scale', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('ENVO Scale')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Average Accuracy by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_by_source = comparison_df.groupby('Source')['Test_Acc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "bars = ax.bar(avg_by_source.index, avg_by_source.values, \n",
    "              alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_title('Average Test Accuracy by Source', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Average Test Accuracy')\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{height:.3f}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Overfitting Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Color bars by overfitting severity\n",
    "colors = ['red' if x > 0.1 else 'orange' if x > 0.05 else 'green' \n",
    "          for x in comparison_df['Overfitting'].values]\n",
    "\n",
    "x_labels = [f\"{row['Source']}\\n{row['Scale']}\" \n",
    "            for _, row in comparison_df.iterrows()]\n",
    "\n",
    "bars = ax.bar(range(len(comparison_df)), comparison_df['Overfitting'].values,\n",
    "              color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xticks(range(len(comparison_df)))\n",
    "ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
    "ax.set_ylabel('Overfitting (Train - Test Accuracy)')\n",
    "ax.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "ax.axhline(y=0.1, color='red', linestyle='--', alpha=0.5, label='High (>0.1)')\n",
    "ax.axhline(y=0.05, color='orange', linestyle='--', alpha=0.5, label='Moderate (>0.05)')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('../results/rf_multi_source')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = output_dir / 'comparison_results.csv'\n",
    "comparison_df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
