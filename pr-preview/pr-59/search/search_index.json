{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"env-embeddings","text":"<p>Simple experiment to compare ENVO similarity to google embedding cosine similarity </p> <ul> <li>Auto-generated schema documentation</li> </ul>"},{"location":"BIOPROJECT_QUALITY/","title":"Bioproject Quality Analysis","text":"<p>Rank bioprojects by how well their MIxS metadata annotations match satellite imagery embeddings.</p>"},{"location":"BIOPROJECT_QUALITY/#overview","title":"Overview","text":"<p>This tool analyzes each bioproject's annotation quality by: 1. Grouping samples by bioproject 2. Computing pairwise similarities (Google Earth vs ENVO) within each project 3. Measuring agreement between satellite imagery and semantic annotations 4. Ranking bioprojects by quality score</p> <p>High quality score = satellite imagery and ENVO annotations agree (both similar or both different) Low quality score = disagreement between physical location and semantic annotation</p>"},{"location":"BIOPROJECT_QUALITY/#usage","title":"Usage","text":""},{"location":"BIOPROJECT_QUALITY/#via-cli-most-flexible","title":"Via CLI (most flexible)","text":"<pre><code>uv run env-embeddings rank-bioprojects data/your_embeddings.csv \\\n  --envo-scale envo_broad_scale \\\n  --mongo-host localhost \\\n  --mongo-port 27017 \\\n  --output data/rankings.csv\n</code></pre> <p>Options: - <code>--envo-scale</code>: Which ENVO dimension to analyze   - <code>envo_broad_scale</code> (default): Broad environmental context   - <code>envo_local_scale</code>: Fine-grained local features   - <code>envo_medium</code>: Intermediate environmental medium - <code>--mongo-host</code>: MongoDB hostname (default: localhost) - <code>--mongo-port</code>: MongoDB port (default: 27017) - <code>--output</code>: Output CSV path (default: auto-generated)</p>"},{"location":"BIOPROJECT_QUALITY/#via-just-recipe-convenient","title":"Via just recipe (convenient)","text":"<pre><code># Use defaults (data/with_both_embeddings.csv, envo_broad_scale)\njust rank-bioprojects\n\n# Custom file\njust rank-bioprojects data/custom.csv\n\n# Custom file and ENVO scale\njust rank-bioprojects data/custom.csv envo_local_scale\n</code></pre>"},{"location":"BIOPROJECT_QUALITY/#programmatically","title":"Programmatically","text":"<pre><code>from env_embeddings.bioproject_quality import rank_bioprojects\nfrom pathlib import Path\n\nresults = rank_bioprojects(\n    csv_file=Path(\"data/biosamples_with_embeddings.csv\"),\n    output_file=Path(\"data/rankings.csv\"),\n    envo_scale='envo_broad_scale',\n    mongo_host='localhost',\n    mongo_port=27017\n)\n</code></pre>"},{"location":"BIOPROJECT_QUALITY/#requirements","title":"Requirements","text":""},{"location":"BIOPROJECT_QUALITY/#input-data","title":"Input Data","text":"<p>CSV file must contain: - <code>google_earth_embeddings</code>: 64-dim satellite embedding vectors (as strings) - <code>envo_broad_scale_embedding</code>: 1536-dim ENVO embeddings for broad scale - <code>envo_local_scale_embedding</code>: 1536-dim ENVO embeddings for local scale - <code>envo_medium_embedding</code>: 1536-dim ENVO embeddings for medium - <code>accession</code>: Biosample accession (for MongoDB lookup)</p> <p>Generated by:</p> <pre><code>uv run env-embeddings add-google-embeddings-csv input.csv --max-rows 1000\nuv run env-embeddings add-envo-embeddings-csv output_with_google.csv\n</code></pre>"},{"location":"BIOPROJECT_QUALITY/#mongodb","title":"MongoDB","text":"<p>Required collection: <code>ncbi_metadata.sra_biosamples_bioprojects</code></p> <p>Schema:</p> <pre><code>{\n  biosample_accession: \"SAMD00123456\",\n  bioproject_accession: \"PRJNA123456\"\n}\n</code></pre> <p>Start MongoDB locally:</p> <pre><code>mongod --dbpath /path/to/data --port 27017\n</code></pre>"},{"location":"BIOPROJECT_QUALITY/#output","title":"Output","text":""},{"location":"BIOPROJECT_QUALITY/#console-output","title":"Console Output","text":"<pre><code>================================================================================\nBIOPROJECT QUALITY RANKING\n================================================================================\nTotal bioprojects analyzed: 142\nUsing ENVO scale: envo_broad_scale\n\nTop 10 highest quality bioprojects:\n   bioproject  n_samples  quality_score  mean_disagreement  spearman_r\n0  PRJNA123456         25          0.850              0.150       0.721\n1  PRJNA234567         18          0.823              0.177       0.692\n...\n\nTop 10 lowest quality bioprojects:\n   bioproject  n_samples  quality_score  mean_disagreement  spearman_r\n132  PRJNA999888        12          0.312              0.688       0.145\n133  PRJNA888777         8          0.289              0.711       0.092\n...\n</code></pre>"},{"location":"BIOPROJECT_QUALITY/#csv-output","title":"CSV Output","text":"<p>Saved to <code>data/&lt;filename&gt;_bioproject_rankings.csv</code>:</p> Column Description <code>bioproject</code> Bioproject accession (e.g., PRJNA123456) <code>n_samples</code> Number of samples in this bioproject <code>n_pairs</code> Number of pairwise comparisons computed <code>quality_score</code> 0-1, higher is better (1 - mean_disagreement) <code>mean_disagreement</code> Average |GE_sim - ENVO_sim| <code>mean_ge_similarity</code> Average pairwise Google Earth similarity <code>mean_envo_similarity</code> Average pairwise ENVO similarity <code>pearson_r</code> Pearson correlation (linear) <code>pearson_p</code> Pearson p-value <code>spearman_r</code> Spearman correlation (rank-based) <code>spearman_p</code> Spearman p-value"},{"location":"BIOPROJECT_QUALITY/#interpretation","title":"Interpretation","text":""},{"location":"BIOPROJECT_QUALITY/#quality-score","title":"Quality Score","text":"<p>0.8-1.0: Excellent - Satellite imagery and ENVO annotations are highly consistent - When samples are geographically close, they have similar ENVO terms - When samples are geographically distant, they have different ENVO terms</p> <p>0.5-0.8: Good - Moderate agreement between physical location and annotations - Some inconsistencies but generally reliable</p> <p>0.3-0.5: Poor - Significant disagreement between imagery and annotations - May indicate:   - Incorrect ENVO term selection   - Copy-paste errors across samples   - Generic/vague annotations   - Samples from same location with different annotations</p> <p>&lt; 0.3: Very Poor - Strong disagreement - likely systematic annotation problems - Requires manual review and correction</p>"},{"location":"BIOPROJECT_QUALITY/#correlation-metrics","title":"Correlation Metrics","text":"<p>Pearson r (linear correlation): - Measures if similarity scores increase/decrease together linearly - Sensitive to outliers</p> <p>Spearman \u03c1 (rank correlation): - Measures if ranking of similarities matches - More robust to outliers - Often higher than Pearson (indicates non-linear relationship)</p>"},{"location":"BIOPROJECT_QUALITY/#example-analysis-workflow","title":"Example Analysis Workflow","text":"<pre><code># 1. Generate embeddings for 1000 samples\nuv run env-embeddings add-google-embeddings-csv \\\n  data/biosamples.csv \\\n  --max-rows 1000 \\\n  --random \\\n  --output data/with_google.csv\n\nuv run env-embeddings add-envo-embeddings-csv \\\n  data/with_google.csv \\\n  --output data/with_both.csv\n\n# 2. Rank bioprojects\njust rank-bioprojects data/with_both.csv\n\n# 3. Review results\n# - High quality projects = good annotation practices\n# - Low quality projects = need curation review\n\n# 4. Try different ENVO scales\njust rank-bioprojects data/with_both.csv envo_local_scale\njust rank-bioprojects data/with_both.csv envo_medium\n</code></pre>"},{"location":"BIOPROJECT_QUALITY/#implementation-details","title":"Implementation Details","text":""},{"location":"BIOPROJECT_QUALITY/#algorithm","title":"Algorithm","text":"<p>For each bioproject with N samples: 1. Compute all N\u00d7(N-1)/2 pairwise similarities:    - Google Earth: cosine similarity of 64-dim vectors    - ENVO: cosine similarity of 1536-dim vectors 2. For each pair, compute disagreement = |GE_sim - ENVO_sim| 3. Quality score = 1 - mean(disagreements) 4. Compute Pearson and Spearman correlations</p>"},{"location":"BIOPROJECT_QUALITY/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>2+ samples per bioproject (need pairs for comparison)</li> <li>3+ pairs for correlations (statistical validity)</li> <li>Bioprojects with &lt; 2 samples are excluded from analysis</li> </ul>"},{"location":"BIOPROJECT_QUALITY/#performance","title":"Performance","text":"<ul> <li>Small dataset (100-500 samples): &lt; 1 minute</li> <li>Medium dataset (1,000-5,000 samples): 2-5 minutes</li> <li>Large dataset (10,000+ samples): 10-30 minutes</li> </ul> <p>Bottleneck: MongoDB query for biosample\u2192bioproject mapping</p>"},{"location":"BIOPROJECT_QUALITY/#troubleshooting","title":"Troubleshooting","text":""},{"location":"BIOPROJECT_QUALITY/#error-connecting-to-mongodb","title":"\"Error connecting to MongoDB\"","text":"<pre><code># Check MongoDB is running\nmongosh localhost:27017\n\n# Verify collection exists\nuse ncbi_metadata\ndb.sra_biosamples_bioprojects.countDocuments()\n</code></pre>"},{"location":"BIOPROJECT_QUALITY/#no-bioprojects-found","title":"\"No bioprojects found\"","text":"<ul> <li>Check CSV has <code>accession</code> column</li> <li>Verify accessions match MongoDB <code>biosample_accession</code> field</li> <li>Try without <code>--max-rows</code> to get more samples</li> </ul>"},{"location":"BIOPROJECT_QUALITY/#total-bioprojects-analyzed-0","title":"\"Total bioprojects analyzed: 0\"","text":"<ul> <li>All bioprojects have &lt; 2 samples</li> <li>Increase sample size with larger <code>--max-rows</code></li> <li>Use <code>--random</code> sampling to get diverse bioprojects</li> </ul>"},{"location":"BIOPROJECT_QUALITY/#related-documentation","title":"Related Documentation","text":"<ul> <li>FINDINGS.md - Analysis methodology and correlation findings</li> <li>PERFORMANCE_OPTIMIZATIONS.md - Caching and sampling strategies</li> <li>Issue #38 - Performance improvements for large datasets</li> </ul>"},{"location":"BULK_UPLOAD_WORKFLOW/","title":"Earth Engine Bulk Upload Workflow","text":"<p>This workflow processes satellite embeddings using Earth Engine's server-side processing, avoiding client-side loops and dramatically improving performance.</p>"},{"location":"BULK_UPLOAD_WORKFLOW/#performance-comparison","title":"Performance Comparison","text":"<p>Current approach (NMDC - 1828 unique coords): - One-by-one: ~12-13 minutes - \"Batched\" (still one-by-one due to tiling): ~10-12 minutes</p> <p>Bulk upload approach (NMDC - 1828 unique coords): - Estimated: 3-5 minutes total   - Upload: 10-20 seconds   - Server-side processing: 2-4 minutes (parallelized by Google)   - Download: 5-10 seconds</p> <p>For larger datasets (NCBI - 300K unique coords): - Current: ~40 hours - Bulk upload: 20-30 minutes</p>"},{"location":"BULK_UPLOAD_WORKFLOW/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"BULK_UPLOAD_WORKFLOW/#1-prepare-coordinates","title":"1. Prepare Coordinates","text":"<pre><code># Using justfile\njust prepare-ee-coords\n\n# Or directly\nuv run env-embeddings prepare-ee-coords data/nmdc_flattened_biosample_for_env_embeddings_202510061052.tsv\n</code></pre> <p>Output: - <code>data/nmdc_..._coords_for_ee.csv</code> - Unique coordinates (1828 rows) - <code>data/nmdc_..._coords_for_ee_mapping.csv</code> - Mapping to expand results back</p>"},{"location":"BULK_UPLOAD_WORKFLOW/#2-process-on-earth-engine","title":"2. Process on Earth Engine","text":"<p>For small datasets (&lt; 2K coords):</p> <pre><code>uv run env-embeddings bulk-process-ee \\\n  data/nmdc_flattened_biosample_for_env_embeddings_202510061052_coords_for_ee.csv \\\n  --description nmdc_embeddings\n</code></pre> <p>For large datasets (&gt; 10K coords), first upload as Earth Engine asset, then:</p> <pre><code>uv run env-embeddings bulk-process-ee \\\n  data/ncbi_coords_for_ee.csv \\\n  --asset projects/env-embeddings-2025/assets/ncbi_coords \\\n  --description ncbi_embeddings\n</code></pre> <p>Options: - <code>--export-to drive</code> - Export to Google Drive (default, easiest) - <code>--export-to gcs --bucket YOUR_BUCKET</code> - Export to Google Cloud Storage (faster for large files) - <code>--no-monitor</code> - Start task and exit (check status manually) - <code>--asset ASSET_ID</code> - Use pre-uploaded Earth Engine Table asset (for large datasets)</p> <p>What happens: 1. Uploads coordinates to Earth Engine as FeatureCollection 2. Configures server-side sampling (<code>.map()</code> over all points) 3. Starts export task 4. Monitors task until completion</p>"},{"location":"BULK_UPLOAD_WORKFLOW/#3-download-results","title":"3. Download Results","text":"<p>From Google Drive: - Go to: https://drive.google.com - Navigate to: <code>earth_engine_exports/</code> folder - Download: <code>nmdc_embeddings.csv</code></p> <p>From Google Cloud Storage:</p> <pre><code>gsutil cp gs://YOUR_BUCKET/nmdc_embeddings.csv data/ee_results.csv\n</code></pre>"},{"location":"BULK_UPLOAD_WORKFLOW/#4-convert-embeddings-format","title":"4. Convert Embeddings Format","text":"<p>The Earth Engine export has separate columns for each embedding dimension (A00, A01, ..., A63). We need to convert this to our format (single column with list).</p> <pre><code>uv run env-embeddings convert-ee-results data/ee_results.csv\n</code></pre> <p>Output: <code>data/ee_results_converted.csv</code></p>"},{"location":"BULK_UPLOAD_WORKFLOW/#5-merge-results-back","title":"5. Merge Results Back","text":"<pre><code># Using justfile\njust merge-ee-results\n\n# Or directly\nuv run env-embeddings merge-ee-results-cmd \\\n  data/nmdc_flattened_biosample_for_env_embeddings_202510061052_coords_for_ee_mapping.csv \\\n  data/ee_results_converted.csv \\\n  data/nmdc_flattened_biosample_for_env_embeddings_202510061052.tsv\n</code></pre> <p>Output: - <code>data/nmdc_flattened_biosample_for_env_embeddings_202510061052_with_embeddings.tsv</code></p>"},{"location":"BULK_UPLOAD_WORKFLOW/#troubleshooting","title":"Troubleshooting","text":""},{"location":"BULK_UPLOAD_WORKFLOW/#earth-engine-authentication","title":"Earth Engine Authentication","text":"<p>If you get authentication errors:</p> <pre><code>earthengine authenticate\n</code></pre> <p>Follow the prompts to authorize.</p>"},{"location":"BULK_UPLOAD_WORKFLOW/#task-fails-with-memory-limit-exceeded","title":"Task Fails with \"Memory limit exceeded\"","text":"<p>For very large datasets (&gt; 10K coordinates), use the Earth Engine asset upload workflow instead of processing in chunks. Upload your coordinates CSV as a Table asset via the Earth Engine Code Editor, then reference it with the <code>--asset</code> flag.</p>"},{"location":"BULK_UPLOAD_WORKFLOW/#export-takes-too-long","title":"Export Takes Too Long","text":"<p>Check task status at: https://code.earthengine.google.com/tasks</p> <p>You can cancel monitoring (Ctrl+C) and the task will continue running on Google's servers.</p>"},{"location":"BULK_UPLOAD_WORKFLOW/#files-created","title":"Files Created","text":"<pre><code>data/\n\u251c\u2500\u2500 nmdc_..._coords_for_ee.csv           # 1828 unique coords \u2192 Upload to EE\n\u251c\u2500\u2500 nmdc_..._coords_for_ee_mapping.csv   # 8434 rows \u2192 For result expansion\n\u251c\u2500\u2500 ee_results.csv                        # Downloaded from Drive/GCS\n\u251c\u2500\u2500 ee_results_converted.csv             # Converted to our format\n\u2514\u2500\u2500 nmdc_..._with_embeddings.tsv         # Final output (8434 rows with embeddings)\n</code></pre>"},{"location":"BULK_UPLOAD_WORKFLOW/#next-steps","title":"Next Steps","text":"<p>After getting embeddings, you can:</p> <ol> <li> <p>Add ENVO embeddings: <code>bash    uv run env-embeddings add-envo-embeddings-csv data/nmdc_..._with_embeddings.tsv</code></p> </li> <li> <p>Analyze bioproject quality: <code>bash    just rank-bioprojects</code></p> </li> </ol>"},{"location":"ISSUES_CREATED_SUMMARY/","title":"GitHub Issues Created - Random Forest Enhancement Roadmap","text":""},{"location":"ISSUES_CREATED_SUMMARY/#summary","title":"Summary","text":"<p>Created 7 new issues to address missing functionality and important considerations for Random Forest ENVO prediction.</p>"},{"location":"ISSUES_CREATED_SUMMARY/#issues-created","title":"Issues Created","text":""},{"location":"ISSUES_CREATED_SUMMARY/#priority-high","title":"Priority: HIGH","text":""},{"location":"ISSUES_CREATED_SUMMARY/#issue-54-handle-duplicate-coordinates-with-samedifferent-envo-annotations","title":"Issue #54: Handle duplicate coordinates with same/different ENVO annotations","text":"<p>URL: https://github.com/contextualizer-ai/env-embeddings/issues/54</p> <p>Problem: Multiple samples with identical coordinates/dates \u2192 identical GE embeddings but potentially different ENVO annotations</p> <p>Impact: - Creates impossible learning task (X \u2192 Y\u2081 AND X \u2192 Y\u2082) - Inflates dataset via pseudo-replication - Test set leakage if duplicates split across train/test</p> <p>Solutions proposed: 1. Deduplicate by coordinate (conservative) 2. Majority vote for conflicts 3. Weight duplicates lower 4. Flag and exclude conflicts only</p> <p>Recommendation: Start with deduplication for clean baseline (Option 1)</p> <p>Related to: #49 (needs this before RF training)</p>"},{"location":"ISSUES_CREATED_SUMMARY/#issue-51-implement-ontology-aware-evaluation-metrics-partial-credit","title":"Issue #51: Implement ontology-aware evaluation metrics (partial credit)","text":"<p>URL: https://github.com/contextualizer-ai/env-embeddings/issues/51</p> <p>Problem: Binary exact match doesn't give credit for \"close\" predictions (e.g., predicting parent term)</p> <p>Example:</p> <pre><code>True: ENVO:00000447 (marine biome)\nPredicted: ENVO:00000428 (biome) [parent term]\nCurrent score: 0.0\nBetter score: 0.75 (hierarchically close)\n</code></pre> <p>Implementation: - Use ENVO ontology graph (via <code>pronto</code> library) - Hierarchy-based scoring: exact=1.0, parent/child=0.75, sibling=0.5, distant=0.25 - Semantic similarity scoring: Use ENVO embeddings cosine similarity - New metrics: hierarchical_accuracy, semantic_accuracy, top_k_hierarchical</p> <p>Impact: More nuanced evaluation, distinguishes \"close misses\" from \"completely wrong\"</p> <p>Related to: #49 (improves evaluation), #31 (original classifier idea)</p>"},{"location":"ISSUES_CREATED_SUMMARY/#priority-medium-high","title":"Priority: MEDIUM-HIGH","text":""},{"location":"ISSUES_CREATED_SUMMARY/#issue-56-compare-rf-predictions-to-k-nn-baseline","title":"Issue #56: Compare RF predictions to k-NN baseline","text":"<p>URL: https://github.com/contextualizer-ai/env-embeddings/issues/56</p> <p>Question: Does supervised learning (RF) beat unsupervised baseline (k-NN)?</p> <p>k-NN baseline (already exists in similarity_analysis.ipynb): - Find k=5 most GE-similar samples - Majority vote their ENVO terms - Simple, interpretable, no training</p> <p>Comparison metrics: - Accuracy (exact match) - Hierarchical accuracy (from #51) - Prediction confidence - Runtime (k-NN slow at inference)</p> <p>Expected outcome: - If RF &gt;&gt; k-NN: ML justified - If RF \u2248 k-NN: Consider simpler k-NN approach - If k-NN &gt; RF: GE embeddings designed for similarity, not classification</p> <p>Related to: #49 (RF baseline), #51 (evaluation), similarity_analysis.ipynb</p>"},{"location":"ISSUES_CREATED_SUMMARY/#priority-medium","title":"Priority: MEDIUM","text":""},{"location":"ISSUES_CREATED_SUMMARY/#issue-52-multi-output-random-forest-for-joint-triad-prediction","title":"Issue #52: Multi-output Random Forest for joint triad prediction","text":"<p>URL: https://github.com/contextualizer-ai/env-embeddings/issues/52</p> <p>Current approach (#49): 3 independent classifiers (broad, local, medium)</p> <p>Proposed: Predict all 3 scales jointly to exploit dependencies</p> <p>Approaches: 1. Multi-output RF: sklearn native, predicts all 3 simultaneously 2. Chained classifiers: Use predicted broad as feature for local/medium 3. Joint encoding: Treat triad as single categorical (huge class space)</p> <p>Research questions: - Does joint prediction improve accuracy? - Can we use hierarchical constraints? (e.g., marine broad \u2192 no desert local) - Which triads are most/least predictable?</p> <p>Evaluation: - Per-scale accuracy vs independent (#49 baseline) - Triad-level accuracy (all 3 must match) - Constraint violation analysis</p> <p>Related to: #49 (provides baseline for comparison)</p>"},{"location":"ISSUES_CREATED_SUMMARY/#issue-53-analyze-ge-feature-importance-across-triad-scales","title":"Issue #53: Analyze GE feature importance across triad scales","text":"<p>URL: https://github.com/contextualizer-ai/env-embeddings/issues/53</p> <p>Question: Do different GE embedding dimensions predict different ENVO scales?</p> <p>Analysis: 1. Compare <code>feature_importances_</code> across 3 RF classifiers 2. Identify unique vs shared important features (Venn diagram) 3. Map GE dimensions to Earth Engine bands (blue, green, NIR, NDVI, etc.) 4. Test reduced feature sets (efficiency vs accuracy trade-off)</p> <p>Hypotheses: - Broad scale: Coarse features (band means, overall NDVI) - Local scale: Fine features (texture, variance, edges) - Some universal features: NDVI for vegetation</p> <p>Deliverables: - Side-by-side feature importance plots - Correlation analysis between scales - Physical interpretation (which bands matter?) - Efficiency recommendation (64 dims or specialize?)</p> <p>Related to: #49 (provides feature_importances_), earth_engine.py</p>"},{"location":"ISSUES_CREATED_SUMMARY/#issue-55-scale-rf-training-to-ncbi-complete-382955-samples","title":"Issue #55: Scale RF training to NCBI complete (382,955 samples)","text":"<p>URL: https://github.com/contextualizer-ai/env-embeddings/issues/55</p> <p>Goal: Test scalability on NCBI complete - 47x larger than NMDC</p> <p>Challenges: 1. Execution time: Estimated 45-90 minutes (vs 2-3 min for NMDC) 2. Class imbalance: More diverse ENVO terms, many with 1-2 samples 3. Data quality: Uncurated, more noise than NMDC 4. Duplicates: Higher rate expected (Issue #54)</p> <p>Optimizations: - Reduce trees: 100 \u2192 50 - Subsample per tree: <code>max_samples=0.5</code> - Skip cross-validation initially - Progress monitoring: <code>verbose=2</code></p> <p>Cross-dataset evaluation: - Train on NCBI, test on NMDC - Train on NMDC, test on NCBI - Does NMDC curation improve model quality?</p> <p>Expected results: - NCBI accuracy: 45-50% (vs NMDC 50-55%) - If NCBI &lt;&lt; NMDC: Too noisy for supervised learning - Runtime: 20-30 minutes (optimized)</p> <p>Related to: #49 (NMDC baseline), #54 (duplicate handling), #46 (NCBI dataset)</p>"},{"location":"ISSUES_CREATED_SUMMARY/#issue-57-document-envo-term-coverage-and-class-distribution","title":"Issue #57: Document ENVO term coverage and class distribution","text":"<p>URL: https://github.com/contextualizer-ai/env-embeddings/issues/57</p> <p>Goal: Understand what we can/can't predict based on data coverage</p> <p>Analysis: 1. Per-dataset statistics: Unique terms, class distribution, Gini coefficient 2. Cross-dataset comparison: Venn diagram of term overlap 3. ENVO ontology coverage: What % of 6000+ ENVO terms do we have? 4. Predictability analysis: Does sample count correlate with RF accuracy?</p> <p>Expected findings: - NMDC: ~200-400 unique terms per scale, ~5-10% ENVO coverage - NCBI: ~800-1500 unique terms, ~15-25% coverage, heavily skewed (Zipf's law) - GOLD: ~300-500 terms</p> <p>Implications: - Need <code>class_weight='balanced'</code> for skewed distributions - Filter rare classes (&lt; 5 samples) - Model only works for covered ENVO subset (closed-world)</p> <p>Deliverables: - <code>notebooks/envo_coverage_analysis.ipynb</code> - Class distribution visualizations (log scale histograms, Zipf plots) - Ontology tree showing covered branches - Recommendations for data collection</p> <p>Related to: #40 (original question), #49 (RF training), #55 (NCBI)</p>"},{"location":"ISSUES_CREATED_SUMMARY/#dependency-graph","title":"Dependency Graph","text":"<pre><code>Priority order and dependencies:\n\nHIGH (Do first):\n  #54 Duplicate handling \u2500\u2500\u2500\u2500\u2500\u2510\n                              \u251c\u2500\u2500&gt; #49 RF training (existing, update)\n  #51 Ontology-aware eval \u2500\u2500\u2500\u2500\u2518\n\nMEDIUM-HIGH (After #49):\n  #56 k-NN comparison \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; Needs #49 baseline\n\nMEDIUM (Parallel or after #49):\n  #53 Feature importance \u2500\u2500\u2500\u2500\u2500&gt; Can add to #49 or separate\n  #57 ENVO coverage \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; Parallel to #49\n  #52 Multi-output RF \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; After #49 baseline\n\nMEDIUM (Later):\n  #55 NCBI scaling \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; After #49, #54, #57\n</code></pre>"},{"location":"ISSUES_CREATED_SUMMARY/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"ISSUES_CREATED_SUMMARY/#phase-1-clean-foundation-current-pr-50","title":"Phase 1: Clean Foundation (Current PR #50)","text":"<ul> <li>[x] Merge RF notebook with NMDC dataset (#49)</li> <li>[x] Update dataset path</li> <li>[ ] Add duplicate handling (#54)</li> <li>[ ] Expand to all 3 scales (#49)</li> </ul>"},{"location":"ISSUES_CREATED_SUMMARY/#phase-2-enhanced-evaluation","title":"Phase 2: Enhanced Evaluation","text":"<ul> <li>[ ] Implement ontology-aware scoring (#51)</li> <li>[ ] Add k-NN comparison (#56)</li> <li>[ ] Feature importance analysis (#53)</li> </ul>"},{"location":"ISSUES_CREATED_SUMMARY/#phase-3-advanced-methods","title":"Phase 3: Advanced Methods","text":"<ul> <li>[ ] ENVO coverage analysis (#57)</li> <li>[ ] Multi-output RF (#52)</li> </ul>"},{"location":"ISSUES_CREATED_SUMMARY/#phase-4-production-scale","title":"Phase 4: Production Scale","text":"<ul> <li>[ ] NCBI complete training (#55)</li> <li>[ ] Cross-dataset validation</li> </ul>"},{"location":"ISSUES_CREATED_SUMMARY/#summary-statistics","title":"Summary Statistics","text":"<p>Total issues created: 7 - High priority: 2 (#54, #51) - Medium-high priority: 1 (#56) - Medium priority: 4 (#52, #53, #55, #57)</p> <p>Estimated implementation time: - #54: 2-4 hours (add to #49) - #51: 1-2 days (new module + integration) - #56: 4-8 hours (extract k-NN, compare) - #53: 2-4 hours (add to #49 notebook) - #57: 1-2 days (separate analysis notebook) - #52: 2-3 days (new approaches, evaluation) - #55: 2-3 days (optimization, cross-validation)</p> <p>Total: ~2-3 weeks of focused work (if done sequentially)</p>"},{"location":"ISSUES_CREATED_SUMMARY/#open-questions-for-discussion","title":"Open Questions for Discussion","text":"<ol> <li>Issue #52 (Multi-output): Worth the complexity? Or stick with independent classifiers?</li> <li>Issue #54 (Duplicates): Should we exclude conflicts entirely or try to learn from them?</li> <li>Issue #55 (NCBI): Is NCBI too noisy? Need curation step first?</li> <li>Issue #51 (Ontology): What hierarchy distance scoring makes sense? (1 hop=0.75, 2 hops=0.5?)</li> <li>Issue #56 (k-NN): If k-NN performs well, is RF worth the complexity?</li> </ol>"},{"location":"ISSUES_CREATED_SUMMARY/#next-steps","title":"Next Steps","text":"<p>Immediate (add to current PR #50): 1. Address #54 (duplicate handling) in RF notebook 2. Ensure #49 success criteria complete (3 classifiers trained)</p> <p>Short-term (next 1-2 weeks): 3. Implement #51 (ontology metrics) - high scientific value 4. Add #56 (k-NN comparison) - validates ML approach 5. Add #53 (feature importance) - easy addition to #49</p> <p>Medium-term (next month): 6. Create #57 (ENVO coverage analysis) 7. Experiment with #52 (multi-output RF) 8. Scale to #55 (NCBI complete)</p>"},{"location":"MISSING_ISSUES_ANALYSIS/","title":"Missing Issues Analysis: RF Enhancements","text":""},{"location":"MISSING_ISSUES_ANALYSIS/#summary","title":"Summary","text":"<p>After reviewing all issues, NONE of the following enhancements are tracked:</p> <ol> <li>\u274c RF performance on other two fields (env_broad_scale, env_medium)</li> <li>\u274c RF performance on combinations of fields (pairs or all three combined)</li> <li>\u274c Partial success scoring using ontology hierarchy (parent/child relationships)</li> </ol>"},{"location":"MISSING_ISSUES_ANALYSIS/#what-exists-vs-whats-missing","title":"What Exists vs What's Missing","text":""},{"location":"MISSING_ISSUES_ANALYSIS/#what-is-covered-issue-49","title":"\u2705 What IS Covered (Issue #49)","text":"<p>Issue #49 (\"Apply Random Forest ENVO predictor to NMDC complete dataset\") addresses: - Running RF on NMDC complete dataset (8,121 samples) - Expanding from <code>env_local_scale</code> only to all 3 MIxS scales:   - <code>env_broad_scale</code>   - <code>env_local_scale</code>   - <code>env_medium</code> - Training 3 separate classifiers (one per scale) - Comparing RF to k-NN baseline</p> <p>Success criteria from #49:</p> <pre><code>- [ ] Trains 3 classifiers: broad_scale, local_scale, medium\n- [ ] Reports accuracy, precision, recall, F1 per scale\n- [ ] Feature importance analysis for all 3 scales\n- [ ] Performance comparison: RF vs k-NN\n</code></pre> <p>This covers your question #1 \u2705</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#what-is-not-covered","title":"\u274c What is NOT Covered","text":""},{"location":"MISSING_ISSUES_ANALYSIS/#missing-issue-1-combinedjoint-prediction","title":"Missing Issue #1: Combined/Joint Prediction","text":"<p>Your question #2: \"How does it perform on combinations of the three fields?\"</p> <p>Not addressed: Training models that predict: - Pairs:   - broad_scale + local_scale jointly   - broad_scale + medium jointly   - local_scale + medium jointly - All three: broad_scale + local_scale + medium as a single multi-output prediction</p> <p>Current approach (Issue #49): 3 independent classifiers, each predicting one scale</p> <p>Missing approach: - Multi-output RandomForest - Predict all 3 scales simultaneously - Chained classifiers - Use predicted broad_scale as feature for predicting local_scale - Joint encoding - Treat triad as a single categorical variable (e.g., \"ENVO:X|ENVO:Y|ENVO:Z\")</p> <p>Research questions: - Does predicting all 3 together improve accuracy vs independent models? - Are certain triads more predictable from satellite imagery? - Can we use hierarchical constraints (e.g., if broad=marine, local can't be desert)?</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#missing-issue-2-ontology-aware-evaluation","title":"Missing Issue #2: Ontology-Aware Evaluation","text":"<p>Your question #3: \"Can we consider predictions of the parent or a child of the asserted value a 'partial success'?\"</p> <p>Not addressed: - Using ENVO ontology hierarchy in evaluation - Partial credit scoring - Semantic similarity metrics</p> <p>Current evaluation (Issue #49): Binary exact match - Predicted <code>ENVO:00000447</code> vs True <code>ENVO:00000447</code> \u2192 \u2705 Correct (1.0) - Predicted <code>ENVO:00000448</code> vs True <code>ENVO:00000447</code> \u2192 \u274c Wrong (0.0)</p> <p>Missing evaluation approaches:</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#2a-hierarchy-based-partial-credit","title":"2a. Hierarchy-based Partial Credit","text":"<p>Use ENVO ontology structure to assign partial scores:</p> <pre><code>def ontology_aware_score(predicted, true, ontology):\n    \"\"\"\n    Score based on ontology distance.\n\n    Examples:\n        - Exact match: 1.0\n        - Parent/child (1 hop): 0.75\n        - Grandparent/grandchild (2 hops): 0.5\n        - Sibling (same parent): 0.5\n        - Distant relative: 0.25\n        - Unrelated: 0.0\n    \"\"\"\n    if predicted == true:\n        return 1.0\n\n    distance = ontology.shortest_path(predicted, true)\n    relationship = ontology.get_relationship(predicted, true)\n\n    if relationship == \"parent\" or relationship == \"child\":\n        return 0.75\n    elif relationship == \"sibling\":\n        return 0.5\n    elif distance &lt;= 3:\n        return 0.25\n    else:\n        return 0.0\n</code></pre> <p>Example scenarios:</p> <pre><code>True: ENVO:00000447 (marine biome)\n  \u251c\u2500 Predicted ENVO:00000447 (marine biome) \u2192 1.0 \u2705 exact\n  \u251c\u2500 Predicted ENVO:01000048 (coastal sea water) \u2192 0.75 \u26a0\ufe0f child term\n  \u251c\u2500 Predicted ENVO:00000428 (biome) \u2192 0.75 \u26a0\ufe0f parent term\n  \u251c\u2500 Predicted ENVO:01000174 (terrestrial biome) \u2192 0.5 \u26a0\ufe0f sibling\n  \u2514\u2500 Predicted ENVO:00002011 (soil) \u2192 0.0 \u274c unrelated\n</code></pre>"},{"location":"MISSING_ISSUES_ANALYSIS/#2b-semantic-similarity","title":"2b. Semantic Similarity","text":"<p>Use ENVO embedding cosine similarity:</p> <pre><code>def semantic_similarity_score(predicted, true, envo_embeddings):\n    \"\"\"\n    Score based on embedding similarity between predicted and true terms.\n\n    Examples:\n        - Cosine similarity &gt; 0.9: High similarity (partial success)\n        - Cosine similarity 0.7-0.9: Moderate similarity\n        - Cosine similarity &lt; 0.7: Low similarity (failure)\n    \"\"\"\n    pred_emb = envo_embeddings[predicted]\n    true_emb = envo_embeddings[true]\n\n    similarity = cosine_similarity(pred_emb, true_emb)\n\n    # Convert similarity to score\n    if similarity &gt; 0.9:\n        return 0.9\n    elif similarity &gt; 0.7:\n        return 0.6\n    else:\n        return 0.0\n</code></pre>"},{"location":"MISSING_ISSUES_ANALYSIS/#2c-taxonomic-level-scoring","title":"2c. Taxonomic Level Scoring","text":"<p>Different penalties for errors at different granularity levels:</p> <pre><code># For env_broad_scale (coarse) - stricter evaluation\n#   Wrong biome = major error\n\n# For env_local_scale (fine) - more lenient\n#   Wrong local term but correct parent = minor error\n\n# Weighted average:\n#   broad_scale_score * 0.4 +\n#   medium_score * 0.3 +\n#   local_scale_score * 0.3\n</code></pre>"},{"location":"MISSING_ISSUES_ANALYSIS/#proposed-new-issues","title":"Proposed New Issues","text":""},{"location":"MISSING_ISSUES_ANALYSIS/#issue-multi-output-random-forest-for-joint-triad-prediction","title":"Issue: Multi-Output Random Forest for Joint Triad Prediction","text":"<p>Title: Train multi-output RF to predict environmental triad jointly (broad + local + medium)</p> <p>Description: Currently (Issue #49) we train 3 independent Random Forest classifiers. Test whether predicting all 3 MIxS scales simultaneously improves performance.</p> <p>Approaches: 1. Multi-output RandomForestClassifier - sklearn native support 2. Chained classifiers - Use broad_scale prediction as feature for medium/local 3. Joint encoding - Treat triad as single 553\u00b3 categorical space</p> <p>Research questions: - Does joint prediction improve accuracy? - Which triads are most predictable from GE embeddings? - Can hierarchical constraints help (e.g., marine broad \u2192 no desert local)?</p> <p>Comparison metrics: - Independent RF (3 models) vs Multi-output RF (1 model) - Accuracy per scale - Triad-level accuracy (all 3 correct) - Execution time - Feature importance differences</p> <p>Deliverables: - <code>notebooks/random_forest_joint_triad_prediction.ipynb</code> - Comparison table: independent vs joint - Visualization: triad confusion matrix</p> <p>Related: #49 (independent classifiers), #31 (original classifier idea)</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#issue-ontology-aware-evaluation-metrics-for-envo-predictions","title":"Issue: Ontology-Aware Evaluation Metrics for ENVO Predictions","text":"<p>Title: Implement partial credit scoring using ENVO hierarchy (parent/child relationships)</p> <p>Description: Current RF evaluation uses binary exact match (correct/incorrect). This penalizes semantically close predictions equally with completely wrong predictions.</p> <p>Example problem:</p> <pre><code>True: ENVO:00000447 (marine biome)\nPredicted: ENVO:00000428 (biome) [parent term]\nCurrent score: 0.0 (wrong)\nBetter score: 0.75 (close - parent term)\n</code></pre> <p>Implementation tasks:</p> <ol> <li>ENVO Ontology Integration</li> <li>Use <code>pronto</code> or <code>owlready2</code> to load ENVO.owl</li> <li>Build graph structure for relationship queries</li> <li> <p>Cache ontology in repo or download on-demand</p> </li> <li> <p>Hierarchy-Based Scoring</p> </li> <li>Exact match: 1.0</li> <li>Parent/child (1 hop): 0.75</li> <li>Grandparent/grandchild or sibling: 0.5</li> <li>Distant (3+ hops): 0.25</li> <li> <p>Unrelated: 0.0</p> </li> <li> <p>Semantic Similarity Scoring</p> </li> <li>Use ENVO embeddings (already have 1536-dim vectors)</li> <li>Cosine similarity between predicted and true term</li> <li> <p>Scale to 0-1 score</p> </li> <li> <p>New Metrics</p> </li> <li><code>hierarchical_accuracy</code>: Mean hierarchy-aware score</li> <li><code>semantic_accuracy</code>: Mean embedding-similarity score</li> <li><code>top_k_hierarchical</code>: True term in top-k by hierarchy distance</li> <li> <p>Per-scale analysis: Which scale benefits most from partial credit?</p> </li> <li> <p>Visualization</p> </li> <li>Heatmap: True vs Predicted with hierarchy coloring</li> <li>Scatter: Exact accuracy vs hierarchical accuracy per class</li> <li>Examples: \"Close misses\" that get partial credit</li> </ol> <p>Use cases: - More nuanced evaluation of RF performance - Identify when model is \"close\" vs \"completely wrong\" - Inform active learning (focus on truly confused samples, not close calls) - Better comparison to k-NN (which may predict related terms)</p> <p>Deliverables: - <code>src/env_embeddings/ontology_metrics.py</code> - Scoring functions - Updated RF notebook with hierarchy-aware metrics - Comparison: exact vs hierarchical accuracy - Analysis: Which ENVO terms have closest \"near misses\"?</p> <p>Related: #49 (RF training), #31 (original classifier), OLS integration in codebase</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#issue-analyze-rf-feature-importance-across-triad-components","title":"Issue: Analyze RF Feature Importance Across Triad Components","text":"<p>Title: Compare which GE embedding dimensions predict broad vs local vs medium scales</p> <p>Description: Issue #49 trains 3 independent RF classifiers (broad, local, medium). Analyze whether different satellite features predict different ENVO scales.</p> <p>Research questions: - Do different GE embedding dimensions matter for different scales? - Is broad_scale predicted by coarse features (low-freq bands)? - Is local_scale predicted by fine features (high-freq bands)? - Can we prune features per-scale for efficiency?</p> <p>Analysis tasks:</p> <ol> <li>Feature Importance Comparison</li> <li>Plot top 20 features for each scale side-by-side</li> <li>Identify unique vs shared important dimensions</li> <li> <p>Correlation: broad_importance vs local_importance</p> </li> <li> <p>Dimension Specialization</p> </li> <li>Which dimensions predict only broad? only local? only medium?</li> <li> <p>Which are universal (important for all 3)?</p> </li> <li> <p>Earth Engine Band Mapping</p> </li> <li>Map GE_dim_0 through GE_dim_63 to actual EE bands</li> <li>Identify: Are blue bands more important than IR? NDVI vs NDWI?</li> <li> <p>Physical interpretation of top features</p> </li> <li> <p>Efficiency Analysis</p> </li> <li>Can we train with fewer features per scale?</li> <li>Trade-off: accuracy vs speed</li> </ol> <p>Deliverables: - Feature importance comparison plots - Venn diagram: shared vs unique important features - Table: Top features per scale with EE band interpretation - Recommendation: Keep all 64 dims or specialize per scale?</p> <p>Related: #49 (trains 3 classifiers), EE embedding code</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#priority-ranking","title":"Priority Ranking","text":""},{"location":"MISSING_ISSUES_ANALYSIS/#high-priority","title":"High Priority","text":"<ol> <li>Ontology-Aware Evaluation \u2b50\u2b50\u2b50</li> <li>Most scientifically valuable</li> <li>Relatively easy to implement (ENVO already integrated)</li> <li>Improves evaluation quality immediately</li> <li>No retraining needed - just scoring changes</li> </ol>"},{"location":"MISSING_ISSUES_ANALYSIS/#medium-priority","title":"Medium Priority","text":"<ol> <li>Multi-Output RF \u2b50\u2b50</li> <li>Interesting research question</li> <li>Moderate implementation effort</li> <li>May or may not improve accuracy (experiment needed)</li> <li> <p>Requires retraining models</p> </li> <li> <p>Feature Importance Analysis \u2b50\u2b50</p> </li> <li>Good for interpretation</li> <li>Easy to add to existing #49 work</li> <li>Helps understand what satellite sees</li> <li>No additional training needed</li> </ol>"},{"location":"MISSING_ISSUES_ANALYSIS/#lower-priority","title":"Lower Priority","text":"<ol> <li>Pair-wise Prediction \u2b50</li> <li>Less clear value over independent or joint</li> <li>More complex to evaluate (which pairs matter?)</li> <li>Could be subset of multi-output work</li> </ol>"},{"location":"MISSING_ISSUES_ANALYSIS/#recommended-next-steps","title":"Recommended Next Steps","text":""},{"location":"MISSING_ISSUES_ANALYSIS/#option-1-add-to-issue-49-current-prbranch","title":"Option 1: Add to Issue #49 (Current PR/Branch)","text":"<p>Expand #49 success criteria to include: - [x] Train 3 independent classifiers - [ ] Implement ontology-aware scoring - [ ] Compare exact vs hierarchical accuracy - [ ] Analyze feature importance across scales</p> <p>Pros: Keep work consolidated, faster iteration Cons: Makes #49 larger in scope</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#option-2-create-separate-issues-better","title":"Option 2: Create Separate Issues (Better)","text":"<p>Keep #49 focused on basic RF training, create new issues for: - Issue #50: Ontology-aware evaluation (post #49) - Issue #51: Multi-output joint triad prediction (post #49) - Issue #52: Feature importance comparison (during/after #49)</p> <p>Pros: Clear separation of concerns, parallel work possible Cons: More issues to track</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#option-3-milestone-based","title":"Option 3: Milestone-Based","text":"<p>Create milestone \"RF Enhancements\" with sub-issues: - Milestone: Advanced RF Analysis   - #49: Basic 3-classifier training \u2190 current   - #50: Ontology evaluation   - #51: Joint prediction   - #52: Feature analysis</p> <p>Pros: Shows progression, organized roadmap Cons: Requires milestone management</p>"},{"location":"MISSING_ISSUES_ANALYSIS/#technical-implementation-notes","title":"Technical Implementation Notes","text":""},{"location":"MISSING_ISSUES_ANALYSIS/#ontology-aware-scoring","title":"Ontology-Aware Scoring","text":"<p>Dependencies needed:</p> <pre><code># Add to pyproject.toml\npronto = \"^2.5.0\"  # OBO/OWL parsing\nnetworkx = \"^3.0\"  # Graph operations (already have?)\n</code></pre> <p>Code structure:</p> <pre><code># src/env_embeddings/ontology_metrics.py\nfrom pronto import Ontology\nimport networkx as nx\n\nclass ENVOHierarchy:\n    def __init__(self, owl_path=\"data/envo.owl\"):\n        self.ontology = Ontology(owl_path)\n        self.graph = self._build_graph()\n\n    def distance(self, term1, term2):\n        \"\"\"Shortest path in ontology graph\"\"\"\n        return nx.shortest_path_length(self.graph, term1, term2)\n\n    def hierarchical_score(self, predicted, true):\n        \"\"\"Score based on ontology distance\"\"\"\n        if predicted == true:\n            return 1.0\n\n        try:\n            dist = self.distance(predicted, true)\n            if dist == 1:\n                return 0.75  # parent/child\n            elif dist == 2:\n                return 0.5   # grandparent or sibling\n            elif dist &lt;= 3:\n                return 0.25\n            else:\n                return 0.0\n        except nx.NetworkXNoPath:\n            return 0.0  # unrelated branches\n\n# Usage in notebook:\nenvo_hierarchy = ENVOHierarchy()\nscores = [envo_hierarchy.hierarchical_score(pred, true)\n          for pred, true in zip(y_pred, y_test)]\nhierarchical_accuracy = np.mean(scores)\n</code></pre>"},{"location":"MISSING_ISSUES_ANALYSIS/#multi-output-rf","title":"Multi-Output RF","text":"<p>sklearn native support:</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\n\n# Create targets matrix (n_samples, 3)\ny_multi = df[['env_broad_scale', 'env_local_scale', 'env_medium']].values\n\n# Train multi-output model\nrf_multi = MultiOutputClassifier(\n    RandomForestClassifier(n_estimators=100, max_depth=10)\n)\nrf_multi.fit(X_train, y_multi_train)\n\n# Predict all 3 scales at once\ny_pred_multi = rf_multi.predict(X_test)  # shape: (n_test, 3)\n\n# Evaluate per-scale and triad-level\nbroad_acc = accuracy_score(y_multi_test[:, 0], y_pred_multi[:, 0])\nlocal_acc = accuracy_score(y_multi_test[:, 1], y_pred_multi[:, 1])\nmedium_acc = accuracy_score(y_multi_test[:, 2], y_pred_multi[:, 2])\n\n# Triad-level accuracy (all 3 must match)\ntriad_correct = (y_multi_test == y_pred_multi).all(axis=1)\ntriad_acc = triad_correct.mean()\n</code></pre>"},{"location":"MISSING_ISSUES_ANALYSIS/#feature-importance-comparison","title":"Feature Importance Comparison","text":"<p>Simple implementation in existing notebook:</p> <pre><code># After training 3 classifiers in Issue #49\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, (scale, clf) in enumerate([\n    ('broad_scale', rf_broad),\n    ('local_scale', rf_local),\n    ('medium', rf_medium)\n]):\n    importances = clf.feature_importances_\n    top_20 = np.argsort(importances)[-20:]\n\n    axes[idx].barh(range(20), importances[top_20])\n    axes[idx].set_title(f'Top 20 Features: {scale}')\n    axes[idx].set_xlabel('Importance')\n    axes[idx].set_ylabel('GE Dimension')\n\nplt.tight_layout()\nplt.show()\n\n# Correlation analysis\nbroad_imp = rf_broad.feature_importances_\nlocal_imp = rf_local.feature_importances_\nmedium_imp = rf_medium.feature_importances_\n\nprint(f\"Broad-Local correlation: {np.corrcoef(broad_imp, local_imp)[0,1]:.3f}\")\nprint(f\"Broad-Medium correlation: {np.corrcoef(broad_imp, medium_imp)[0,1]:.3f}\")\nprint(f\"Local-Medium correlation: {np.corrcoef(local_imp, medium_imp)[0,1]:.3f}\")\n</code></pre>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/","title":"NMDC Complete Workflow - January 6, 2025","text":""},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#summary","title":"Summary","text":"<p>Successfully created complete NMDC dataset with both Google Earth Engine and ENVO embeddings ready for <code>notebooks/similarity_analysis.ipynb</code>.</p> <p>Final Output: <code>data/nmdc_flattened_biosample_for_env_embeddings_202510061052_with_embeddings_with_envo_embeddings.csv</code></p> <p>Statistics: - 8,434 rows (all with complete embeddings) - Google Earth embeddings: 64-dimensional (96.3% success rate via bulk upload) - ENVO embeddings: 1536-dimensional (3 columns: broad_scale, medium, local_scale) - Format: CSV (notebook-compatible)</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#complete-workflow","title":"Complete Workflow","text":""},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#step-1-prepare-coordinates-for-bulk-upload","title":"Step 1: Prepare Coordinates for Bulk Upload","text":"<p>Optimization: Process unique coordinates only (reduced 8,434 \u2192 1,828 unique coords, 78.3% reduction)</p> <pre><code>uv run env-embeddings prepare-ee-coords \\\n  data/nmdc_flattened_biosample_for_env_embeddings_202510061052.tsv\n</code></pre> <p>Output: - <code>data/nmdc_..._coords_for_ee.csv</code> (1,828 unique coordinates) - <code>data/nmdc_..._coords_for_ee_mapping.csv</code> (8,434 rows mapping sample \u2192 coord_id)</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#step-2-bulk-process-on-earth-engine","title":"Step 2: Bulk Process on Earth Engine","text":"<p>Method: Server-side processing using CLI command</p> <p>For small datasets (&lt; 2K coords):</p> <pre><code>uv run env-embeddings bulk-process-ee \\\n  data/nmdc_..._coords_for_ee.csv \\\n  --description nmdc_embeddings\n</code></pre> <p>For large datasets (&gt; 10K coords), first upload as Earth Engine asset, then:</p> <pre><code>uv run env-embeddings bulk-process-ee \\\n  data/ncbi_coords.csv \\\n  --asset projects/env-embeddings-2025/assets/ncbi_coords \\\n  --description ncbi_embeddings\n</code></pre> <p>Performance: ~6 minutes for 1,828 coordinates (vs 10-12 minutes one-by-one)</p> <p>Result: <code>nmdc_embeddings.csv</code> downloaded from Google Drive</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#step-3-convert-earth-engine-results","title":"Step 3: Convert Earth Engine Results","text":"<p>Earth Engine exports embeddings as 64 separate columns (A00-A63). Convert to single list column:</p> <pre><code>uv run env-embeddings convert-ee-results nmdc_embeddings.csv\n</code></pre> <p>Output: <code>data/nmdc_embeddings_converted.csv</code> (1,828 coords with embeddings as lists)</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#step-4-merge-results-back-to-original-samples","title":"Step 4: Merge Results Back to Original Samples","text":"<p>Expand 1,828 unique coordinate embeddings back to all 8,434 samples:</p> <pre><code>uv run env-embeddings merge-ee-results-cmd \\\n  data/nmdc_flattened_biosample_for_env_embeddings_202510061052_coords_for_ee_mapping.csv \\\n  data/nmdc_embeddings_converted.csv \\\n  data/nmdc_flattened_biosample_for_env_embeddings_202510061052.tsv\n</code></pre> <p>Output: <code>data/nmdc_..._with_embeddings.tsv</code> (8,434 rows with Google Earth embeddings)</p> <p>Success Rate: 8,121/8,434 (96.3%)</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#step-5-add-envo-embeddings","title":"Step 5: Add ENVO Embeddings","text":"<p>Optimization: Collect unique ENVO terms first (103 unique terms from 8,434 rows)</p> <pre><code>uv run env-embeddings add-envo-embeddings-csv \\\n  data/nmdc_flattened_biosample_for_env_embeddings_202510061052_with_embeddings.tsv\n</code></pre> <p>Performance: - Step 1: Collected 103 unique ENVO terms - Step 2: 88 terms in cache, 15 needed fetching (~7 seconds) - Step 3: Mapped back to 8,434 rows (~30 seconds)</p> <p>Output: <code>data/nmdc_..._with_embeddings_with_envo_embeddings.csv</code> (8,434 rows, all complete)</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#final-dataset-schema","title":"Final Dataset Schema","text":"<p>Columns:</p> <pre><code>- accession (sample ID)\n- collection_date (full date string)\n- latitude, longitude (numeric)\n- env_broad_scale, env_medium, env_local_scale (ENVO terms as text)\n- google_earth_embeddings (64-dim list as string)\n- envo_broad_scale_embedding (1536-dim list as string)\n- envo_medium_embedding (1536-dim list as string)\n- envo_local_scale_embedding (1536-dim list as string)\n</code></pre> <p>Usage in Notebook:</p> <pre><code>import pandas as pd\nimport ast\n\ndf = pd.read_csv('data/nmdc_..._with_embeddings_with_envo_embeddings.csv')\n\n# Parse embeddings\ndf['ge_embedding'] = df['google_earth_embeddings'].apply(ast.literal_eval)\ndf['envo_broad'] = df['envo_broad_scale_embedding'].apply(ast.literal_eval)\ndf['envo_medium'] = df['envo_medium_embedding'].apply(ast.literal_eval)\ndf['envo_local'] = df['envo_local_scale_embedding'].apply(ast.literal_eval)\n</code></pre>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#key-optimizations-made","title":"Key Optimizations Made","text":""},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#1-coordinate-deduplication","title":"1. Coordinate Deduplication","text":"<p>Reduced processing from 8,434 samples to 1,828 unique coordinates (78.3% reduction).</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#2-envo-term-deduplication","title":"2. ENVO Term Deduplication","text":"<p>Optimized from processing 8,434 rows \u00d7 3 columns = 25,302 lookups to 103 unique term lookups.</p> <p>Before optimization: - Slow: 3 cache lookups per row \u00d7 8,434 rows - Estimated time: 40+ minutes at ~3 rows/second</p> <p>After optimization: - Fast: 103 unique terms (88 cached, 15 fetched) - Actual time: ~37 seconds total</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#3-bulk-earth-engine-processing","title":"3. Bulk Earth Engine Processing","text":"<p>Server-side processing avoids quota limits and reduces processing time by 2x for NMDC (100x+ for larger datasets like NCBI).</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#file-inventory","title":"File Inventory","text":"<p>Input: - <code>data/nmdc_flattened_biosample_for_env_embeddings_202510061052.tsv</code> (809K, 8,434 rows)</p> <p>Intermediate: - <code>data/nmdc_..._coords_for_ee.csv</code> (63K, 1,828 unique coords) - <code>data/nmdc_..._coords_for_ee_mapping.csv</code> (490K, mapping file) - <code>nmdc_embeddings.csv</code> (2.2M, from Earth Engine) - <code>data/nmdc_embeddings_converted.csv</code> (2.3M, converted format) - <code>data/nmdc_..._with_embeddings.tsv</code> (11M, with GE embeddings)</p> <p>Final Output: - <code>data/nmdc_..._with_embeddings_with_envo_embeddings.csv</code> (CSV format for notebook)</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#justfile-targets","title":"Justfile Targets","text":"<pre><code># Prepare coordinates\njust prepare-ee-coords\n\n# After EE processing, convert results\njust convert-ee-results\n\n# Merge back to original\njust merge-ee-results\n\n# Add ENVO embeddings (uses optimized version)\nuv run env-embeddings add-envo-embeddings-csv &lt;input_tsv&gt;\n</code></pre>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#next-steps","title":"Next Steps","text":""},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#apply-to-gold-dataset","title":"Apply to GOLD Dataset","text":"<p>Use same workflow on <code>data/gold_flattened_biosamples_for_env_embeddings_202510061108.tsv</code> (871K file).</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#apply-to-ncbi-dataset","title":"Apply to NCBI Dataset","text":"<p>Use same workflow on NCBI dataset (~300K samples, 49M TSV). Estimated time with bulk upload: 20-30 minutes vs 40 hours one-by-one.</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#use-in-notebook","title":"Use in Notebook","text":"<p>The output CSV is now ready for <code>notebooks/similarity_analysis.ipynb</code> to compute cosine similarities between Google Earth and ENVO embeddings.</p>"},{"location":"NMDC_COMPLETE_WORKFLOW_20250106/#performance-summary","title":"Performance Summary","text":"<p>Total Time for NMDC: - Prepare coords: &lt;1 minute - Earth Engine bulk processing: ~6 minutes - Convert results: &lt;1 minute - Merge back: &lt;1 minute - Add ENVO embeddings: ~37 seconds - Total: ~9 minutes (vs ~40 minutes one-by-one)</p> <p>Scalability: - NMDC: 8,434 samples \u2192 1,828 unique coords (78.3% reduction) - NCBI: ~300K samples \u2192 estimated 50-100K unique coords (similar reduction) - Bulk processing scales linearly with unique coordinates, not total samples</p>"},{"location":"NOTEBOOK_GUIDE/","title":"Notebook Guide: Environmental Embeddings Analysis","text":""},{"location":"NOTEBOOK_GUIDE/#overview","title":"Overview","text":"<p>This repository contains Jupyter notebooks for analyzing environmental sample metadata using satellite imagery embeddings and environmental ontology (ENVO) embeddings. The notebooks can be run independently - they do not need to be executed in a specific order.</p>"},{"location":"NOTEBOOK_GUIDE/#notebooks","title":"Notebooks","text":""},{"location":"NOTEBOOK_GUIDE/#1-similarity_analysisipynb-803-kb-executed-with-outputs","title":"1. <code>similarity_analysis.ipynb</code> (803 KB, executed with outputs)","text":"<p>Purpose: Cross-validate environmental annotations with satellite imagery</p> <p>Research Question: Do environmental samples with similar satellite imagery have similar ENVO annotations? Can we use satellite data to detect metadata errors?</p> <p>Dataset: NMDC complete (8,121 samples) - <code>nmdc_flattened_biosample_for_env_embeddings_202510061052_complete.csv</code></p> <p>Key Analyses: 1. Pairwise Similarity Analysis    - Compares 1 million sample pairs    - Computes cosine similarity between:      - Google Earth embeddings (64-dim satellite imagery)      - ENVO embeddings (1536-dim for each of 3 scales)</p> <ol> <li>Degenerate Pair Filtering</li> <li>Removes near-duplicate pairs (similarity &gt; 0.95)</li> <li>Tests correlation robustness</li> <li> <p>679,048 pairs retained after filtering</p> </li> <li> <p>Multi-Scale ENVO Comparison</p> </li> <li>Tests all 3 MIxS scales:<ul> <li><code>env_broad_scale</code> (biome level)</li> <li><code>env_local_scale</code> (fine-grained) \u2192 BEST: r=0.307</li> <li><code>env_medium</code> (intermediate)</li> </ul> </li> <li> <p>Tests combined embeddings (concatenated &amp; weighted average)</p> </li> <li> <p>Outlier Detection</p> </li> <li>Finds pairs with high GE similarity but low ENVO similarity</li> <li>Identifies \"chronic outliers\" appearing in many disagreement pairs</li> <li> <p>Flags metadata quality issues</p> </li> <li> <p>k-NN Triad Prediction</p> </li> <li>Predicts ENVO triads from k=5 GE-similar samples</li> <li>Uses majority voting</li> <li>Serves as baseline for RF comparison</li> </ol> <p>Key Findings: - Local scale shows strongest correlation (r=0.307, \u03c1=0.214) - Broad scale shows weak negative correlation (r=-0.066) - Degenerate filtering reduced correlation by \u0394=-0.253 (indicates inflated original results)</p> <p>Use Cases: - Metadata quality control - Understanding GE \u2194 ENVO relationship - Baseline k-NN predictions for comparison</p>"},{"location":"NOTEBOOK_GUIDE/#2-random_forest_envo_predictionipynb-16-kb-template-without-outputs","title":"2. <code>random_forest_envo_prediction.ipynb</code> (16 KB, template without outputs)","text":"<p>Purpose: Train Random Forest classifiers to predict ENVO terms from satellite embeddings</p> <p>Research Question: Can geographic/satellite imagery data predict environmental classification labels?</p> <p>Dataset: - Currently configured: NMDC complete (8,121 samples) \u2705 UPDATED - Previously used: Accidental NCBI subset (~10,978 samples) - see executed version</p> <p>Key Analyses: 1. Binary Classification (currently only <code>env_local_scale</code>)    - Features: 64-dim Google Earth embeddings    - Target: ENVO local scale term (553 classes)    - Model: RandomForestClassifier (100 trees, max_depth=10)</p> <ol> <li>Train/Test Split</li> <li>80/20 split</li> <li> <p>Stratification if possible (min 2 samples per class)</p> </li> <li> <p>Model Evaluation</p> </li> <li>Training vs test accuracy (overfitting check)</li> <li>5-fold cross-validation</li> <li>Classification report (precision/recall per class)</li> <li> <p>Confusion matrix</p> </li> <li> <p>Feature Importance</p> </li> <li>Identifies which GE embedding dimensions matter most</li> <li> <p>Ranks top 20 features</p> </li> <li> <p>Prediction Confidence Analysis</p> </li> <li>Examines max probability distributions</li> <li>Shows low-confidence vs high-confidence predictions</li> </ol> <p>Current Limitations: - \u26a0\ufe0f Only predicts <code>env_local_scale</code> (should expand to all 3 scales) - \u26a0\ufe0f No comparison to k-NN baseline from <code>similarity_analysis.ipynb</code></p> <p>Expected Performance (based on executed version with ~10,978 samples): - Training accuracy: ~63.7% - Test accuracy: ~53.1% - Mean prediction confidence: ~20.3% - Finding: \"MODERATE predictive power\" with moderate overfitting</p> <p>Use Cases: - Predict missing ENVO terms from satellite imagery - Compare supervised (RF) vs unsupervised (k-NN) methods - Understand which environmental scales are most predictable</p> <p>Planned Improvements: 1. Expand to all 3 MIxS scales (broad, local, medium) 2. Add k-NN baseline comparison 3. Test on larger NCBI dataset (382,955 samples)</p>"},{"location":"NOTEBOOK_GUIDE/#3-random_forest_envo_prediction_executedipynb-626-kb-with-outputs","title":"3. <code>random_forest_envo_prediction_executed.ipynb</code> (626 KB, with outputs)","text":"<p>Purpose: Historical record of RF classifier trained on accidental NCBI subset</p> <p>Dataset: <code>satisfying_biosamples_normalized_with_google_embeddings_with_envo_embeddings.csv</code> - 10,978 samples (accidental NCBI subset from earlier pipeline) - NOT the complete datasets now available</p> <p>Results: - Test accuracy: 53.1% - 553 unique ENVO local_scale classes - Most common class: ENVO:00001998 (464 samples, 4.2%)</p> <p>Status: Reference only - shows what was possible with previous dataset</p>"},{"location":"NOTEBOOK_GUIDE/#conceptual-relationships","title":"Conceptual Relationships","text":""},{"location":"NOTEBOOK_GUIDE/#independent-approaches-to-same-problem","title":"Independent Approaches to Same Problem","text":"<pre><code>Research Goal: Understand relationship between satellite imagery and ENVO annotations\n                                  \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                           \u2502\n         Correlation Analysis          Classification Approach\n      (similarity_analysis.ipynb)  (random_forest_envo_prediction.ipynb)\n                    \u2502                           \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502               \u2502           \u2502             \u2502\n      Pairwise Cosine   k-NN Baseline   RF Supervised\n      Similarity        Prediction      Learning\n      (unsupervised)    (lazy learning) (eager learning)\n</code></pre>"},{"location":"NOTEBOOK_GUIDE/#complementary-insights","title":"Complementary Insights","text":"Aspect Similarity Analysis Random Forest Method Pairwise cosine similarity Supervised classification Question \"Do similar images have similar labels?\" \"Can we predict labels from images?\" Output Correlation coefficient (r) Classification accuracy (%) Strengths - Tests all 3 ENVO scales- Identifies outliers- k-NN baseline - Predicts for unlabeled samples- Feature importance- Handles multi-class Use Case Metadata quality control Missing metadata prediction"},{"location":"NOTEBOOK_GUIDE/#recommended-workflow","title":"Recommended Workflow","text":"<p>For metadata quality control: 1. Run <code>similarity_analysis.ipynb</code> first 2. Identify outliers and chronic offenders 3. Use k-NN predictions as first-pass suggestions</p> <p>For missing metadata prediction: 1. Run <code>random_forest_envo_prediction.ipynb</code> to train models 2. Compare RF accuracy to k-NN baseline from similarity analysis 3. Use whichever method has higher confidence for predictions</p> <p>For research/publication: 1. Run both notebooks on same dataset (NMDC complete) 2. Report correlation results AND classification accuracy 3. Show RF provides value over simple k-NN approach</p>"},{"location":"NOTEBOOK_GUIDE/#dataset-files","title":"Dataset Files","text":""},{"location":"NOTEBOOK_GUIDE/#current-complete-datasets-branch-46-complete-ncbi-dataset","title":"Current Complete Datasets (branch: 46-complete-ncbi-dataset)","text":"<pre><code>data/\n  nmdc_flattened_biosample_for_env_embeddings_202510061052_complete.csv\n    \u2192 8,121 samples (100% embedding coverage)\n    \u2192 Curated gold standard\n    \u2192 Used by: similarity_analysis.ipynb, random_forest_envo_prediction.ipynb\n\n  ncbi_flattened_biosamples_for_env_embeddings_202510061108_normalized_complete.csv\n    \u2192 382,955 samples (88.6% after coordinate dedup)\n    \u2192 Full NCBI BioSample\n    \u2192 Future: Scale RF training to this\n\n  gold_flattened_biosample_for_env_embeddings_202510061143_normalized_complete.csv\n    \u2192 10,401 samples\n    \u2192 GOLD database samples\n    \u2192 Future: Independent test set\n</code></pre>"},{"location":"NOTEBOOK_GUIDE/#legacy-dataset-historical-reference","title":"Legacy Dataset (historical reference)","text":"<pre><code>data/satisfying_biosamples_normalized_with_google_embeddings_with_envo_embeddings.csv\n  \u2192 ~10,978 samples (accidental NCBI subset)\n  \u2192 Used by: random_forest_envo_prediction_executed.ipynb\n  \u2192 No longer recommended for new analyses\n</code></pre>"},{"location":"NOTEBOOK_GUIDE/#required-columns","title":"Required Columns","text":"<p>All notebooks expect these columns:</p> Column Description Type <code>accession</code> Sample ID (SAMD, SAMN) string <code>collection_date</code> ISO date string <code>latitude</code> Decimal degrees float <code>longitude</code> Decimal degrees float <code>env_broad_scale</code> ENVO ID (biome) string <code>env_local_scale</code> ENVO ID (fine-grained) string <code>env_medium</code> ENVO ID (intermediate) string <code>google_earth_embeddings</code> 64-dim list (as string) string <code>envo_broad_scale_embedding</code> 1536-dim list (as string) string <code>envo_local_scale_embedding</code> 1536-dim list (as string) string <code>envo_medium_embedding</code> 1536-dim list (as string) string"},{"location":"NOTEBOOK_GUIDE/#execution-order","title":"Execution Order","text":"<p>Short answer: No specific order required - notebooks are independent.</p> <p>Recommended for first-time users: 1. <code>similarity_analysis.ipynb</code> - Start here to understand the data and GE\u2194ENVO relationship 2. <code>random_forest_envo_prediction.ipynb</code> - Then try supervised learning approach</p> <p>For iteration/development: - Modify and re-run either notebook independently - Both read from same data files, no dependencies between them</p>"},{"location":"NOTEBOOK_GUIDE/#next-steps-issue-49","title":"Next Steps (Issue #49)","text":"<p>Current branch <code>49-nmdc-rf-envo-prediction</code> is updating RF notebook to: 1. \u2705 Use NMDC complete dataset (done) 2. \u2b1c Expand to all 3 MIxS scales 3. \u2b1c Add k-NN baseline comparison 4. \u2b1c Validate against similarity_analysis findings</p> <p>See <code>RF_NOTEBOOK_READINESS.md</code> for detailed change requirements.</p>"},{"location":"RF_EXECUTION_ANALYSIS/","title":"Random Forest Notebook Execution Analysis","text":""},{"location":"RF_EXECUTION_ANALYSIS/#does-the-rf-notebook-downsample","title":"Does the RF notebook downsample?","text":"<p>Answer: NO - The notebook processes all data provided to it.</p>"},{"location":"RF_EXECUTION_ANALYSIS/#data-flow","title":"Data Flow","text":"<ol> <li> <p>Load: Reads entire CSV file    <code>python    df = pd.read_csv('../data/nmdc_flattened_biosample_for_env_embeddings_202510061052_complete.csv')</code></p> </li> <li> <p>Filter: Only removes rows with missing values    <code>python    df_clean = df[df['google_earth_embeddings'].notna() &amp; df['env_local_scale'].notna()].copy()</code></p> </li> <li>NMDC dataset: 8,121 rows \u2192 8,121 rows retained (100% success rate)</li> <li> <p>Previous NCBI subset: 10,978 rows \u2192 10,978 rows retained (100% success rate)</p> </li> <li> <p>Train/Test Split: Standard 80/20 split    <code>python    test_size = 0.2    X_train, X_test, y_train, y_test = train_test_split(X, y, ...)</code></p> </li> <li>NMDC (8,121): Train = 6,497, Test = 1,624</li> <li> <p>Previous (10,978): Train = 8,782, Test = 2,196</p> </li> <li> <p>Training: Uses ALL training samples</p> </li> <li>No random forest max_samples parameter (uses 100% by default)</li> <li>All 100 trees see all training data (with bootstrap sampling)</li> </ol>"},{"location":"RF_EXECUTION_ANALYSIS/#no-downsampling-found","title":"No Downsampling Found","text":"<p>Verified by grep search - no occurrence of: - <code>.sample()</code> calls - <code>.head()</code> limiting data (only used for display) - Manual row slicing beyond train/test split - <code>max_samples</code> parameter in RandomForestClassifier</p>"},{"location":"RF_EXECUTION_ANALYSIS/#execution-time-estimate","title":"Execution Time Estimate","text":""},{"location":"RF_EXECUTION_ANALYSIS/#previous-execution-random_forest_predict_envo-branch","title":"Previous Execution (random_forest_predict_envo branch)","text":"<p>Dataset: ~10,978 NCBI samples (accidental subset)</p> <p>Timeline from git commits:</p> <pre><code>035825d  2025-10-03 10:56:05  \"Notebook to build and test random forest...\"\n         (template created)\n\n73c7071  2025-10-03 10:58:51  \"Ran notebook\"\n         (execution complete)\n</code></pre> <p>Elapsed time: ~2.75 minutes (165 seconds)</p>"},{"location":"RF_EXECUTION_ANALYSIS/#execution-stages","title":"Execution Stages","text":"<p>The notebook has 32 cells. Major computational steps:</p> <ol> <li>Cell 2: Imports (&lt; 5 seconds)</li> <li>Cell 4: Load CSV (~5-10 seconds)</li> <li>Cell 7: Parse 10,978 embeddings (~10-15 seconds)</li> <li>Cell 16: Train Random Forest (bulk of time - estimated ~60-90 seconds)</li> <li>100 trees</li> <li>max_depth=10</li> <li>8,782 training samples</li> <li>64 features</li> <li>553 classes</li> <li>Cell 18: Predictions (~5-10 seconds)</li> <li>Cell 19: 5-fold cross-validation (~30-60 seconds)</li> <li>Cell 20: Classification report (~5 seconds)</li> <li>Cell 22: Confusion matrix plot (~5 seconds)</li> <li>Remaining cells: Analysis and visualization (~15-30 seconds)</li> </ol> <p>Total estimated: 2-3 minutes matches the git timeline</p>"},{"location":"RF_EXECUTION_ANALYSIS/#expected-nmdc-execution-time","title":"Expected NMDC Execution Time","text":"<p>Dataset: 8,121 samples (74% of previous size)</p> <p>Scaling factors: - Training samples: 6,497 vs 8,782 (74% of previous) - Number of classes: Unknown (likely similar ~500-600 ENVO terms) - Features: Same (64 dimensions) - Trees: Same (100) - Depth: Same (10)</p> <p>Random Forest training scales roughly O(n * log(n)) for sample count</p> <p>Estimated time: - Previous: ~165 seconds for 8,782 training samples - NMDC: ~120-140 seconds for 6,497 training samples - Total notebook runtime: ~2-2.5 minutes</p>"},{"location":"RF_EXECUTION_ANALYSIS/#timing-breakdown-nmdc-estimate","title":"Timing Breakdown (NMDC estimate)","text":"Cell/Stage Estimated Time Imports 2-5 sec Load CSV 5-10 sec Parse embeddings 8-12 sec Train RF 60-80 sec Predictions 5-8 sec Cross-validation 25-40 sec Reports/plots 10-20 sec TOTAL ~2-3 min"},{"location":"RF_EXECUTION_ANALYSIS/#bottlenecks","title":"Bottlenecks","text":"<ol> <li>Random Forest training - Dominant cost (~50-60% of runtime)</li> <li>5-fold CV - Second largest (~20-30% of runtime)</li> <li>Embedding parsing - Minor (~5-10% of runtime)</li> </ol>"},{"location":"RF_EXECUTION_ANALYSIS/#scaling-to-larger-datasets","title":"Scaling to Larger Datasets","text":"<p>NCBI complete (382,955 samples): - 47x more samples than NMDC - Training samples: ~306,364 (80% split) - Estimated RF training time: ~30-60 minutes (assuming linear scaling) - Estimated CV time: ~15-30 minutes - Total notebook runtime: ~45-90 minutes (rough estimate)</p> <p>Mitigation strategies for large datasets: 1. Reduce n_estimators (100 \u2192 50) 2. Limit max_samples (e.g., 50% of data per tree) 3. Skip cross-validation (just do train/test split) 4. Subsample for initial exploration, then full run for final model</p>"},{"location":"RF_EXECUTION_ANALYSIS/#jupyter-metadata","title":"Jupyter Metadata","text":"<p>Note: The executed notebook does NOT contain cell-level execution timing metadata. Standard Jupyter notebooks don't record this by default unless: - Using Jupyter Lab with timing extension - Using <code>%%time</code> magic in cells - Using nbconvert with timing options</p> <p>The git commit timeline provides only an upper bound on total execution time.</p>"},{"location":"RF_EXECUTION_ANALYSIS/#recommendations","title":"Recommendations","text":""},{"location":"RF_EXECUTION_ANALYSIS/#for-nmdc-8121-samples","title":"For NMDC (8,121 samples)","text":"<ul> <li>\u2705 Current configuration is fine</li> <li>Runtime: ~2-3 minutes</li> <li>No optimization needed</li> </ul>"},{"location":"RF_EXECUTION_ANALYSIS/#for-ncbi-complete-382955-samples","title":"For NCBI complete (382,955 samples)","text":"<ul> <li>\u26a0\ufe0f Current configuration may be slow (~45-90 min)</li> <li>Consider optimizations:</li> <li><code>n_estimators=50</code> (faster, still good performance)</li> <li><code>max_samples=0.5</code> (each tree sees 50% of data)</li> <li>Skip CV for initial runs</li> <li>Use <code>verbose=2</code> to monitor progress</li> </ul>"},{"location":"RF_EXECUTION_ANALYSIS/#for-developmentiteration","title":"For Development/Iteration","text":"<ul> <li>Always test on NMDC first (fast)</li> <li>Only run NCBI complete for final results</li> <li>Consider stratified sampling (e.g., 10% of NCBI) for prototyping</li> </ul>"},{"location":"RF_NOTEBOOK_READINESS/","title":"Random Forest Notebook Readiness Report","text":""},{"location":"RF_NOTEBOOK_READINESS/#question-is-the-random-forest-notebook-ready-to-run-on-nmdc-input","title":"Question: Is the random forest notebook ready to run on NMDC input?","text":"<p>Answer: NO - The notebook requires 3 updates before it can run on NMDC data.</p>"},{"location":"RF_NOTEBOOK_READINESS/#good-news-dataset-compatibility","title":"\u2705 Good News: Dataset Compatibility","text":"<p>The NMDC complete dataset has the exact columns the RF notebook expects:</p> <p>NMDC columns:</p> <pre><code>accession, collection_date, latitude, longitude, env_broad_scale, env_local_scale,\nenv_medium, google_earth_embeddings, envo_broad_scale_embedding,\nenvo_medium_embedding, envo_local_scale_embedding\n</code></pre> <p>RF notebook expects: - <code>google_earth_embeddings</code> \u2705 (present) - <code>env_local_scale</code> \u2705 (present) - <code>env_broad_scale</code> \u2705 (present - available but not used) - <code>env_medium</code> \u2705 (present - available but not used)</p>"},{"location":"RF_NOTEBOOK_READINESS/#required-changes","title":"\u274c Required Changes","text":""},{"location":"RF_NOTEBOOK_READINESS/#change-1-update-dataset-path-cell-4","title":"Change 1: Update Dataset Path (Cell 4)","text":"<p>Current:</p> <pre><code>df = pd.read_csv('../data/satisfying_biosamples_normalized_with_google_embeddings_with_envo_embeddings.csv',\n                 on_bad_lines='skip', engine='python')\n</code></pre> <p>Required:</p> <pre><code>df = pd.read_csv('../data/nmdc_flattened_biosample_for_env_embeddings_202510061052_complete.csv')\n</code></pre> <p>Impact: Critical - without this change, notebook will use wrong dataset</p>"},{"location":"RF_NOTEBOOK_READINESS/#change-2-expand-to-all-3-mixs-scales-multiple-cells","title":"Change 2: Expand to All 3 MIxS Scales (Multiple Cells)","text":"<p>Current: Only predicts <code>env_local_scale</code></p> <p>Required: Train 3 separate Random Forest classifiers: 1. <code>env_broad_scale</code> \u2192 Biome level prediction 2. <code>env_local_scale</code> \u2192 Fine-grained prediction (current) 3. <code>env_medium</code> \u2192 Intermediate prediction</p> <p>Cells to modify: - Cell 5: Check for all 3 columns, not just <code>env_local_scale</code> - Cell 9: Change target variable section to handle all 3 - Cell 12: Create 3 target vectors (y_broad, y_local, y_medium) - Cell 16-31: Duplicate training/evaluation for each scale</p> <p>Impact: Important - aligns with project goal of full triad prediction</p>"},{"location":"RF_NOTEBOOK_READINESS/#change-3-add-comparison-to-k-nn-baseline-new-section","title":"Change 3: Add Comparison to k-NN Baseline (New Section)","text":"<p>Current: Only evaluates RF accuracy in isolation</p> <p>Required: Compare RF predictions to the k-NN baseline from <code>similarity_analysis.ipynb</code></p> <p>Suggested new section:</p> <pre><code>## Compare to k-NN Baseline\n\n# For samples where RF has low confidence, compare to k-NN prediction\n# This validates whether RF is better than simple nearest-neighbor approach\n</code></pre> <p>Impact: Medium priority - provides scientific validation</p>"},{"location":"RF_NOTEBOOK_READINESS/#expected-results-after-update","title":"Expected Results After Update","text":""},{"location":"RF_NOTEBOOK_READINESS/#sample-size-comparison","title":"Sample Size Comparison","text":"<ul> <li>Previous RF work: 10,978 samples (accidental NCBI subset)</li> <li>NMDC dataset: 8,121 samples (curated gold standard)</li> </ul>"},{"location":"RF_NOTEBOOK_READINESS/#scale-coverage","title":"Scale Coverage","text":"<ul> <li>Previous: 1 classifier (local_scale only)</li> <li>After update: 3 classifiers (broad, local, medium)</li> </ul>"},{"location":"RF_NOTEBOOK_READINESS/#success-metrics","title":"Success Metrics","text":"<p>The updated notebook should report: 1. Training/test accuracy for each of 3 scales 2. Feature importance rankings (which GE embedding dims matter most) 3. Prediction confidence distributions 4. Comparison of RF vs k-NN baseline performance 5. Per-class precision/recall for common ENVO terms</p>"},{"location":"RF_NOTEBOOK_READINESS/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Update Cell 4 (dataset path) \u2192 Run to verify data loads correctly</li> <li>Run current notebook as-is on NMDC to get local_scale baseline</li> <li>Expand to 3 scales \u2192 Compare which scale is most predictable</li> <li>Add k-NN comparison \u2192 Validate RF provides value over simpler method</li> </ol>"},{"location":"RF_NOTEBOOK_READINESS/#dataset-statistics-from-similarity_analysisipynb","title":"Dataset Statistics (from similarity_analysis.ipynb)","text":"<pre><code>Total rows loaded: 8121\nColumns: ['accession', 'collection_date', 'latitude', 'longitude',\n          'env_broad_scale', 'env_local_scale', 'env_medium',\n          'google_earth_embeddings', 'envo_broad_scale_embedding',\n          'envo_medium_embedding', 'envo_local_scale_embedding']\n\n=== EMBEDDING DATA QUALITY ===\nRows with google_earth_embeddings: 8121\nRows with envo_broad_scale_embedding: 8121\nRows with envo_local_scale_embedding: 8121\nRows with envo_medium_embedding: 8121\n\nRows with ALL embeddings (GE + 3 ENVO): 8121\nSuccess rate: 100.0%\n</code></pre> <p>All 8,121 samples have complete embeddings - no data quality issues.</p>"},{"location":"about/","title":"About env-embeddings","text":"<p>Simple experiment to compare ENVO similarity to google embedding cosine similarity </p>"},{"location":"data_file_analysis_2025-10-03/","title":"Data File Analysis - 2025-10-03","text":""},{"location":"data_file_analysis_2025-10-03/#latest-data-file-satisfying_biosamples_normalized_with_google_embeddings_with_envo_embeddingscsv","title":"Latest Data File: <code>satisfying_biosamples_normalized_with_google_embeddings_with_envo_embeddings.csv</code>","text":"<p>Last Modified: Oct 3 05:44 (2025)</p> <p>Size: 3.5 GB</p>"},{"location":"data_file_analysis_2025-10-03/#distinguishing-characteristics","title":"Distinguishing Characteristics","text":"<ul> <li>34,129 rows (from 436,337 in the input - only ~7.8% passed)</li> <li>Contains both Google Earth embeddings AND ENVO embeddings</li> <li>Columns:</li> <li>accession</li> <li>collection_date</li> <li>env_broad_scale</li> <li>env_local_scale</li> <li>env_medium</li> <li>collection_date_raw</li> <li>lat_lon_raw</li> <li>latitude</li> <li>longitude</li> <li>google_earth_embeddings</li> <li>envo_broad_scale_embedding</li> <li>envo_medium_embedding</li> <li>envo_local_scale_embedding</li> </ul>"},{"location":"data_file_analysis_2025-10-03/#generated-from","title":"Generated From","text":"<ul> <li>Input: <code>data/satisfying_biosamples_normalized.csv</code> (436,337 rows, normalized biosample data with environmental context)</li> <li>Process: Output of adding both Google Earth embeddings and ENVO embeddings to the normalized biosamples</li> </ul>"},{"location":"data_file_analysis_2025-10-03/#usage","title":"Usage","text":"<p>Currently referenced in <code>notebooks/similarity_analysis.ipynb:78</code> for similarity analysis between Google and ENVO embeddings.</p>"},{"location":"data_file_analysis_2025-10-03/#notes","title":"Notes","text":"<p>This is the most complete dataset with both embedding types for comparison analysis.</p>"},{"location":"data_file_analysis_2025-10-03/#file-cleanup-analysis","title":"File Cleanup Analysis","text":""},{"location":"data_file_analysis_2025-10-03/#safe-to-delete-not-referenced-or-deprecated","title":"Safe to Delete (not referenced or deprecated):","text":"<ol> <li><code>data/satisfying_biosamples_normalized_with_google_embeddings.csv</code> - Intermediate file, superseded by the version with both embeddings</li> <li><code>data/satisfying_biosamples_normalized_with_google_embeddings_with_envo_embeddings.csv</code> - Only used in old notebook, likely superseded by <code>satisfying_biosamples_normalized.csv</code></li> </ol>"},{"location":"data_file_analysis_2025-10-03/#keep-actively-used","title":"Keep - Actively Used:","text":"<ol> <li>\u2705 <code>tests/input/example-samples.tsv</code> - Git tracked, used for tests</li> <li>\u2705 <code>data/satisfying_biosamples_normalized.csv</code> - Git tracked, referenced in justfile and docs</li> <li>\u2705 <code>data/biosamples_with_embeddings.csv</code> - Git tracked, used in <code>bioproject_quality.py:282</code> and BIOPROJECT_QUALITY.md</li> </ol>"},{"location":"data_file_analysis_2025-10-03/#keep-sample-data-git-tracked","title":"Keep - Sample Data (Git tracked):","text":"<ol> <li>\u2705 <code>data/date_and_latlon_samples_extended_SAMPLE.tsv</code> - Git tracked sample</li> <li>\u2705 <code>data/date_and_latlon_samples_extended_SAMPLE_with_embeddings.tsv</code> - Git tracked sample</li> <li>\u2705 <code>data/date_and_latlon_samples_extended_ONLY_ENVO_SAMPLE_100.tsv</code> - Git tracked sample</li> <li>\u2705 <code>data/date_and_latlon_samples_extended_ONLY_ENVO_SAMPLE_100_with_embeddings.tsv</code> - Git tracked sample</li> <li>\u2705 <code>data/date_and_latlon_samples_extended_ONLY_ENVO_SAMPLE_100_with_both_embeddings.tsv</code> - Git tracked, mentioned in docs/research/column_mapping.md</li> </ol>"},{"location":"analysis/FINDINGS/","title":"Analysis Findings: Satellite Imagery vs ENVO Annotations","text":""},{"location":"analysis/FINDINGS/#executive-summary","title":"Executive Summary","text":"<p>After rigorous filtering of near-duplicate pairs, the correlation between Google Earth satellite embeddings and ENVO (Environmental Ontology) annotations is weak (r = 0.167). This negative result is scientifically valuable and suggests that satellite imagery and semantic annotations capture fundamentally different aspects of environmental context.</p>"},{"location":"analysis/FINDINGS/#critical-finding-correlation-collapse","title":"\ud83d\udd34 Critical Finding: Correlation Collapse","text":""},{"location":"analysis/FINDINGS/#the-numbers","title":"The Numbers","text":"Metric Before Filtering After Filtering Change Pearson (broad_scale) 0.789 0.114 -0.675 Pearson (local_scale) 0.647 0.167 -0.480 Pearson (medium) 0.756 -0.054 -0.810"},{"location":"analysis/FINDINGS/#what-happened","title":"What Happened","text":"<p>31.7% of pairs were degenerate (similarity &gt; 0.95 on at least one metric): - 2,080 pairs with GE similarity &gt; 0.95 (20.8%) - 2,381 pairs with ENVO broad_scale &gt; 0.95 (23.8%) - 2,983 pairs with ENVO medium &gt; 0.95 (29.8%)</p> <p>These near-duplicate pairs created artificial correlation through the (1,1) anchor point problem: - Samples from same location \u2192 GE = 1.0 - Samples with same ENVO terms \u2192 ENVO = 1.0 - Many such pairs \u2192 inflates correlation coefficient</p> <p>After filtering: Only 6,826 pairs remain from 10,000 original pairs.</p>"},{"location":"analysis/FINDINGS/#why-this-matters","title":"Why This Matters","text":"<p>This demonstrates: 1. Methodological rigor is critical in similarity studies 2. Degenerate pair filtering is essential to avoid false positives 3. The original hypothesis (strong correlation) was wrong - and that's valuable science</p>"},{"location":"analysis/FINDINGS/#what-the-data-actually-shows","title":"\ud83d\udcca What the Data Actually Shows","text":""},{"location":"analysis/FINDINGS/#best-correlations-after-filtering","title":"Best Correlations (After Filtering)","text":"<p>Winner: Local scale (fine-grained environmental context) - Pearson r = 0.167 (weak linear) - Spearman \u03c1 = 0.336 (moderate rank) - Local scale = specific features like \"agricultural field\", \"coastal zone\", \"urban\"</p> <p>Runner-up: Concatenated (all three scales combined) - Pearson r = 0.161 - Spearman \u03c1 = 0.412 (highest of all methods) - 4608-dimensional vector (1536 \u00d7 3)</p>"},{"location":"analysis/FINDINGS/#key-observations","title":"Key Observations","text":"<p>1. Spearman &gt;&gt; Pearson everywhere</p> <pre><code>Local scale:  Spearman (0.336) vs Pearson (0.167)\nCombined:     Spearman (0.412) vs Pearson (0.161)\n</code></pre> <p>\u2192 Non-linear relationship between satellite and ENVO similarity</p> <p>2. Medium scale has negative Pearson correlation - Pearson r = -0.054 (statistically significant, p = 7.25e-06) - Spearman \u03c1 = 0.125 (still positive) - Medium = intermediate features like \"soil\", \"sea water\", \"sediment\" - Requires investigation: Why does linear correlation flip negative?</p> <p>3. Concatenation performs well on Spearman - Highest Spearman (0.412) but not highest Pearson (0.161) - Suggests combining scales helps for rank-based comparison - But doesn't improve linear correlation</p>"},{"location":"analysis/FINDINGS/#distribution-statistics-filtered-data","title":"Distribution Statistics (Filtered Data)","text":"<p>Google Earth similarity: - Mean: 0.203 - Std: 0.153 - Range: [-0.088, 0.950]</p> <p>ENVO similarities: | Scale | Mean | Std | Range | |-------|------|-----|-------| | Broad | 0.527 | 0.105 | [0.210, 0.950] | | Local | 0.311 | 0.080 | [0.164, 0.950] | | Medium | 0.348 | 0.062 | [0.134, 0.950] |</p> <p>Key insight: ENVO similarities are much higher and more constrained than GE similarities.</p>"},{"location":"analysis/FINDINGS/#dataset-limitations","title":"\ud83d\udea7 Dataset Limitations","text":""},{"location":"analysis/FINDINGS/#1-sample-size-too-small","title":"1. Sample Size Too Small","text":"<ul> <li>246 samples total</li> <li>6,826 valid pairs after filtering</li> <li>Maximum possible pairs: 30,135</li> <li>Only sampling 22.6% of possible space</li> </ul> <p>Consequence: May not capture full diversity of environmental contexts.</p>"},{"location":"analysis/FINDINGS/#2-geographic-clustering","title":"2. Geographic Clustering","text":"<p>Evidence from outlier analysis:</p> <pre><code>Top 10 chronic outliers:\n- 4 samples from (35.3202, 139.6500) - same location\n- 3 samples from (35.7000, 139.5000) - same location\n- Most samples from Japan (35\u00b0N, 139\u00b0E region)\n</code></pre> <p>Consequence: - Limited geographic diversity - Potential regional bias in annotations - Same-location samples inflate pair counts</p>"},{"location":"analysis/FINDINGS/#3-technical-replicates","title":"3. Technical Replicates","text":"<p>Many samples are: - Same location - Same date - Same ENVO terms - Different accessions (technical replicates, time series, depth profiles)</p> <p>Example from data:</p> <pre><code>SAMD00093579, SAMD00093580, SAMD00093581, SAMD00093583, SAMD00093585\nAll from: (35.3202, 139.6500), Date: 2017-06-13\nAll have: ENVO:01000008 | ENVO:00002123 | ENVO:01000157\n</code></pre> <p>Consequence: Creates many near-identical pairs even after degenerate filtering.</p>"},{"location":"analysis/FINDINGS/#4-limited-biome-diversity","title":"4. Limited Biome Diversity","text":"<p>Missing or underrepresented: - Marine environments - Desert biomes - Arctic/Antarctic - Agricultural diversity - Urban diversity</p> <p>Consequence: Results may not generalize to all environmental contexts.</p>"},{"location":"analysis/FINDINGS/#where-to-go-from-here","title":"\ud83c\udfaf Where to Go From Here","text":""},{"location":"analysis/FINDINGS/#option-1-scale-up-recommended-first-step","title":"Option 1: Scale Up (Recommended First Step)","text":"<p>Goal: Test if weak correlation is dataset-specific or fundamental.</p> <p>Action plan:</p> <pre><code># Process 1,000-5,000 more samples with:\n- Geographic diversity (all continents, latitudes)\n- Biome diversity (marine, terrestrial, urban, agricultural, extreme)\n- No technical replicates (deduplicate by location+date)\n- Balanced representation across ENVO terms\n</code></pre> <p>What we'll learn: - Does correlation improve with scale? - Is the dataset bias or true signal? - Which biomes/regions drive correlation?</p> <p>Expected outcome: - If r stays weak (&lt; 0.3) \u2192 correlation is real (weak) - If r improves (&gt; 0.5) \u2192 current dataset is biased - If r varies by biome \u2192 need stratified analysis</p>"},{"location":"analysis/FINDINGS/#option-2-pivot-to-classificationprediction","title":"Option 2: Pivot to Classification/Prediction","text":"<p>Reframe the question: - From: \"How correlated are they?\" (weak correlation) - To: \"Can we predict ENVO from satellite?\" (measure accuracy)</p> <p>Approach: Supervised Learning</p> <pre><code># Train a model\nX = google_earth_embeddings  # 64-dim\ny = envo_triads  # (broad, local, medium)\n\n# Multi-output classification\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Evaluate\naccuracy_broad = accuracy_score(y_test_broad, y_pred_broad)\naccuracy_local = accuracy_score(y_test_local, y_pred_local)\naccuracy_medium = accuracy_score(y_test_medium, y_pred_medium)\n</code></pre> <p>Why this is better: - Correlation measures linear relationship - Classification measures predictive power - Can handle non-linear patterns (we saw Spearman &gt; Pearson) - More interpretable for practical applications</p> <p>Baseline to beat: Random chance - If 10 unique broad_scale terms \u2192 random = 10% accuracy - If model achieves 40% \u2192 4\u00d7 better than random - Even 40% accuracy is useful for flagging suspect metadata</p> <p>Next level: k-NN approach - We already prototyped this in cell-19 - Find k nearest neighbors by GE similarity - Predict ENVO triad by majority vote - Measure accuracy, precision, recall</p>"},{"location":"analysis/FINDINGS/#option-3-focus-on-outliers-quality-control-tool","title":"Option 3: Focus on Outliers (Quality Control Tool)","text":"<p>Forget correlation strength, use disagreement as signal.</p> <p>High GE + Low ENVO = metadata quality issue</p> <p>Current results: - 16 outlier pairs (GE &gt; 0.8, ENVO &lt; 0.3) - 0.2% of filtered pairs - 14 unique samples involved</p> <p>Why so few outliers? - Dataset is too small - Geographic clustering means most samples are genuinely different locations - Need more data to find systematic errors</p> <p>Scaling up this approach:</p> <pre><code># With 10,000 samples\nmax_pairs = 10,000 * 9,999 / 2 = 49,995,000 pairs\n\n# Even 0.1% outliers = 49,995 suspicious pairs\n# Much better signal for quality control\n</code></pre> <p>Practical application: 1. Automated flagging: Submit new biosample \u2192 check GE vs ENVO 2. Curator review: Present top 100 disagreements per month 3. Correction suggestions: Use k-NN to suggest alternative triads 4. Training data curation: Identify high-quality samples (high agreement)</p>"},{"location":"analysis/FINDINGS/#option-4-investigate-technical-details","title":"Option 4: Investigate Technical Details","text":""},{"location":"analysis/FINDINGS/#a-why-is-medium-scale-negative","title":"A. Why is medium scale negative?","text":"<p>Observation: Pearson r = -0.054, but Spearman \u03c1 = +0.125</p> <p>Hypotheses: 1. Outliers: A few extreme pairs flip the linear trend 2. Non-monotonic relationship: Medium similarity increases then decreases with GE 3. Embedding quality: Medium ENVO terms are less well-embedded 4. Biological reality: Medium features (soil, water) vary independently of satellite view</p> <p>Investigation needed: - Scatter plot of GE vs medium similarity (look for patterns) - Stratify by medium term (is it specific terms driving negative correlation?) - Compare medium embedding quality to broad/local</p>"},{"location":"analysis/FINDINGS/#b-why-does-spearman-outperform-pearson","title":"B. Why does Spearman outperform Pearson?","text":"<p>Everywhere we see: Spearman &gt; Pearson (often 2\u00d7)</p> <p>Possible explanations: 1. Outliers: Linear correlation is sensitive, rank is robust 2. Non-linear relationship: Similarity might be exponential/logarithmic 3. Ceiling effects: ENVO similarities are bounded [0, 1], GE can be negative 4. Monotonic but not linear: Rank order is preserved but relationship curves</p> <p>Test: - Try log-transformed similarities - Try different embedding models - Look at scatter plots for curvature</p>"},{"location":"analysis/FINDINGS/#c-try-different-embeddings","title":"C. Try Different Embeddings","text":"<p>Google Earth variations:</p> <pre><code># Different zoom levels\nzoom_12 = 10m resolution (current)\nzoom_11 = 20m resolution (broader context)\nzoom_13 = 5m resolution (finer detail)\n\n# Temporal aggregates\nmedian_2015_2020 = multi-year composite (reduce seasonal variation)\nseasonal = separate summer/winter embeddings\n</code></pre> <p>ENVO variations:</p> <pre><code># Different embedding models\ntext-embedding-3-small (current, 1536-dim)\ntext-embedding-3-large (3072-dim, more expressive)\n\n# Raw term matching\nterm_overlap = Jaccard similarity on ENVO IDs\nsemantic_similarity = from ENVO ontology structure (not text)\n</code></pre>"},{"location":"analysis/FINDINGS/#option-5-write-it-up-publishable-negative-result","title":"Option 5: Write It Up (Publishable Negative Result)","text":"<p>Title: \"Weak Correlation Between Satellite Imagery and Ontological Annotations Highlights Complementary Environmental Perspectives\"</p> <p>Abstract:</p> <p>Environmental metadata curation relies on manual annotation using standardized ontologies like ENVO. We investigated whether satellite imagery could serve as an objective proxy for ENVO annotations by comparing Google Earth embeddings to ENVO text embeddings across 246 environmental samples. Initial analysis suggested strong correlation (r = 0.789), but rigorous filtering of near-duplicate pairs revealed this was an artifact. After removing degenerate pairs, correlation was weak (r = 0.167 for best-performing local scale). Spearman correlations consistently outperformed Pearson (\u03c1 = 0.336 vs r = 0.167), suggesting non-linear relationships. These findings demonstrate: (1) the critical importance of degenerate pair filtering in similarity studies, (2) satellite and semantic views capture complementary rather than redundant information, and (3) potential for satellite imagery in metadata quality control through outlier detection rather than direct correlation. We discuss dataset limitations and propose supervised learning approaches for future work.</p> <p>Contributions: 1. Methodological: Demonstrates degenerate pair problem in similarity studies 2. Negative result: Weak correlation is informative (not just absence of positive result) 3. Practical: k-NN prediction prototype for metadata suggestion 4. Dataset: 246 samples with dual embeddings (satellite + ENVO)</p> <p>Sections: 1. Introduction: Metadata quality problem in biology 2. Methods: Dual embedding approach, degenerate filtering 3. Results: Correlation collapse after filtering 4. Discussion: Why weak correlation matters, alternative approaches 5. Conclusion: Complementary perspectives, future directions</p>"},{"location":"analysis/FINDINGS/#recommended-path-forward","title":"\ud83d\udca1 Recommended Path Forward","text":""},{"location":"analysis/FINDINGS/#phase-1-validate-with-more-data-2-4-weeks","title":"Phase 1: Validate with More Data (2-4 weeks)","text":"<p>Objective: Is this dataset-specific or fundamental?</p> <p>Tasks: 1. Process 1,000 more samples (diverse geography/biomes)    - Note: Pipeline now uses random sampling instead of sequential to avoid bias    - <code>df.sample(n=max_rows, random_state=42)</code> ensures representative selection 2. Re-run correlation analysis with filtering 3. Compare results to current dataset</p> <p>Decision point: - If r &gt; 0.5 \u2192 dataset bias, continue scaling - If r &lt; 0.3 \u2192 weak correlation is real, pivot to classification</p>"},{"location":"analysis/FINDINGS/#phase-2-build-prediction-model-2-4-weeks","title":"Phase 2: Build Prediction Model (2-4 weeks)","text":"<p>Objective: Measure predictive power, not correlation</p> <p>Tasks: 1. Implement k-NN classifier (already prototyped) 2. Baseline: Random Forest, XGBoost 3. Evaluate accuracy, precision, recall per scale 4. Cross-validation (geographic stratification)</p> <p>Success metric: Accuracy &gt; 2\u00d7 random chance</p>"},{"location":"analysis/FINDINGS/#phase-3-quality-control-tool-4-8-weeks","title":"Phase 3: Quality Control Tool (4-8 weeks)","text":"<p>Objective: Practical application for metadata curation</p> <p>Tasks: 1. Outlier detection pipeline (GE vs ENVO disagreement) 2. Web interface for curator review 3. Automated suggestion system (k-NN predictions) 4. Integration with BioSample submission workflow</p> <p>Deliverable: Live tool for NCBI/EBI metadata curation</p>"},{"location":"analysis/FINDINGS/#phase-4-publication-4-8-weeks","title":"Phase 4: Publication (4-8 weeks)","text":"<p>Objective: Share findings with community</p> <p>Tasks: 1. Write manuscript (methods, results, discussion) 2. Create supplementary materials (notebook, data, code) 3. Submit to bioinformatics journal (e.g., Bioinformatics, NAR) 4. Preprint on bioRxiv</p> <p>Impact: Influence metadata standards and curation practices</p>"},{"location":"analysis/FINDINGS/#technical-considerations","title":"\ud83d\udd2c Technical Considerations","text":""},{"location":"analysis/FINDINGS/#statistical-power","title":"Statistical Power","text":"<p>Current: - 246 samples - 6,826 pairs (after filtering) - Power to detect r &gt; 0.3 at \u03b1 = 0.05</p> <p>Needed for r = 0.2 detection: - ~800 samples - ~320,000 pairs - Better for weak correlation studies</p>"},{"location":"analysis/FINDINGS/#computational-cost","title":"Computational Cost","text":"<p>Current pipeline: - Google Earth API: ~2 seconds per sample - ENVO embeddings: cached (instant for duplicates) - Pairwise similarities: O(n\u00b2) for n samples</p> <p>Scaling to 10,000 samples: - Embeddings: ~5 hours (Google Earth API limited) - Similarities: ~10 minutes (embarrassingly parallel) - Storage: ~500 MB (embeddings + metadata)</p> <p>Bottleneck: Google Earth Engine API rate limits</p>"},{"location":"analysis/FINDINGS/#geographic-stratification","title":"Geographic Stratification","text":"<p>For unbiased sampling:</p> <pre><code># Stratify by major biomes\nbiomes = {\n    'terrestrial': 0.4,  # 40% of samples\n    'marine': 0.3,       # 30%\n    'freshwater': 0.15,  # 15%\n    'urban': 0.10,       # 10%\n    'extreme': 0.05      # 5%\n}\n\n# Stratify by latitude\nlatitude_bins = {\n    'arctic': 66-90\u00b0,\n    'temperate_north': 23-66\u00b0,\n    'tropical': -23-23\u00b0,\n    'temperate_south': -66--23\u00b0,\n    'antarctic': -90--66\u00b0\n}\n</code></pre>"},{"location":"analysis/FINDINGS/#validation-strategy","title":"Validation Strategy","text":"<p>Cross-validation approaches: 1. K-fold: Random split (ignores geography) 2. Geographic CV: Hold out regions (tests generalization) 3. Biome CV: Hold out biome types (tests diversity) 4. Temporal CV: Hold out years (tests temporal stability)</p> <p>Recommended: Geographic CV to avoid spatial autocorrelation</p>"},{"location":"analysis/FINDINGS/#related-work","title":"\ud83d\udcda Related Work","text":""},{"location":"analysis/FINDINGS/#similar-studies","title":"Similar Studies","text":"<p>Satellite + Metadata: - iNaturalist species distribution models - GBIF occurrence data validation - Land cover classification from satellite</p> <p>Embedding Similarity: - Sentence embeddings for semantic similarity - Image embeddings for reverse image search - Cross-modal embedding alignment (vision + language)</p>"},{"location":"analysis/FINDINGS/#gaps-this-work-fills","title":"Gaps This Work Fills","text":"<ol> <li>Environmental metadata: First to compare satellite vs ontology embeddings</li> <li>Degenerate filtering: Demonstrates methodological pitfall</li> <li>Negative result: Published weak correlations are rare but valuable</li> <li>Practical tool: k-NN prediction for metadata curation</li> </ol>"},{"location":"analysis/FINDINGS/#lessons-learned","title":"\ud83c\udf93 Lessons Learned","text":""},{"location":"analysis/FINDINGS/#methodological","title":"Methodological","text":"<ol> <li>Always filter degenerate pairs in similarity studies</li> <li>Spearman before Pearson when relationship is unknown</li> <li>Report effect sizes (r values), not just p-values</li> <li>Visualize before correlating (scatter plots reveal non-linearity)</li> </ol>"},{"location":"analysis/FINDINGS/#scientific","title":"Scientific","text":"<ol> <li>Negative results are valuable when rigorously tested</li> <li>Weak correlation \u2260 no information (still useful for classification)</li> <li>Complementary is better than redundant (different perspectives matter)</li> </ol>"},{"location":"analysis/FINDINGS/#practical","title":"Practical","text":"<ol> <li>Start small, scale deliberately (246 \u2192 1,000 \u2192 10,000)</li> <li>Build infrastructure first (caching, filtering, reproducibility)</li> <li>Document as you go (notebook + markdown files)</li> </ol>"},{"location":"analysis/FINDINGS/#next-actions","title":"\ud83d\ude80 Next Actions","text":""},{"location":"analysis/FINDINGS/#immediate-this-week","title":"Immediate (This Week)","text":"<ul> <li>[ ] Review findings with collaborators</li> <li>[ ] Decide on direction (scale up vs pivot vs publish)</li> <li>[ ] Plan next dataset (if scaling up)</li> </ul>"},{"location":"analysis/FINDINGS/#short-term-this-month","title":"Short-term (This Month)","text":"<ul> <li>[ ] Process 500-1,000 more samples (if scaling)</li> <li>[ ] Implement classification model (if pivoting)</li> <li>[ ] Draft introduction (if publishing)</li> </ul>"},{"location":"analysis/FINDINGS/#long-term-next-quarter","title":"Long-term (Next Quarter)","text":"<ul> <li>[ ] Complete analysis on larger dataset</li> <li>[ ] Build quality control prototype</li> <li>[ ] Submit manuscript</li> </ul>"},{"location":"analysis/FINDINGS/#final-thoughts","title":"\ud83d\udcdd Final Thoughts","text":"<p>This analysis demonstrates the value of methodological rigor and the importance of negative results in science. The weak correlation between satellite imagery and ENVO annotations is not a failure\u2014it's a discovery that these two perspectives capture complementary aspects of environmental context.</p> <p>The satellite sees: Physical landscape, land cover, terrain ENVO captures: Biological context, ecosystem type, functional role</p> <p>Together, not separately, they provide comprehensive environmental metadata.</p> <p>The path forward depends on your goals: - Basic science: Understand why correlation is weak (scale up, investigate) - Applied tool: Build prediction/validation system (pivot to classification) - Community impact: Publish findings, influence metadata standards</p> <p>All three paths are valuable. The infrastructure you've built supports any direction.</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/","title":"Scientific Analysis: Methodological Concerns in similarity_analysis.ipynb","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#summary-of-critical-issues","title":"Summary of Critical Issues","text":"<p>The current notebook shows high correlation coefficients (r=0.811 for broad_scale), but several methodological concerns suggest these results may be inflated or unreliable:</p> <ol> <li>Self-pairs inflation (most critical)</li> <li>Sample size adequacy</li> <li>Sampling methodology</li> <li>Statistical independence</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#1-self-pairs-problem-inflated-correlations-from-identical-samples","title":"1. Self-Pairs Problem: Inflated Correlations from Identical Samples","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#the-issue","title":"The Issue","text":"<p>Looking at the output from cell-15 and cell-17, we see many pairs with similarity = 1.0 for BOTH Google Earth AND all ENVO types:</p> <pre><code>High similarity in both (GE &gt; 0.9, ENVO broad_scale &gt; 0.9): 212 pairs\n</code></pre> <p>Out of 1000 pairs, 212 have near-perfect agreement (&gt;0.9 on both metrics).</p> <p>From cell-17 output, we can see why:</p> <pre><code>Pair: SAMD00115491 vs SAMD00115487\nGE Similarity: 1.000\nENVO broad_scale: 1.000\nSample 1 - Lat/Lon: (35.7128, 139.7619), Date: 2018-02-11\nSample 2 - Lat/Lon: (35.7128, 139.7619), Date: 2018-02-11\nSample 1 ENVO: ENVO:00002030 | ENVO:00002006 | ENVO:00000241\nSample 2 ENVO: ENVO:00002030 | ENVO:00002006 | ENVO:00000241\n</code></pre> <p>These are essentially the same sample! Same location, same date, same ENVO terms \u2192 1.0 similarity on both axes.</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#why-this-inflates-correlation","title":"Why This Inflates Correlation","text":"<p>When you have many (1,1) points in a correlation analysis: - Pearson correlation is extremely sensitive to these perfect-agreement points - They create an artificial \"anchor\" at (1,1) that pulls the correlation line upward - This can make a weak/moderate correlation appear strong</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#evidence-in-the-data","title":"Evidence in the Data","text":"<p>From the current run with 246 samples: - 212/1000 pairs (21%) have GE &gt; 0.9 AND ENVO broad &gt; 0.9 - Many of these are likely self-pairs (same location, same ENVO terms) - The dataset has many samples from same locations (e.g., (35.7128, 139.7619) appears multiple times)</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#the-solution","title":"The Solution","text":"<p>Exclude self-pairs and near-duplicates:</p> <ol> <li>Remove identical samples: Don't compare samples with identical coordinates + ENVO terms</li> <li>Apply similarity threshold: Exclude pairs where EITHER metric &gt; 0.95 (likely duplicates)</li> <li>Re-calculate correlations on the cleaned dataset</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#2-sample-size-is-1000-pairs-enough","title":"2. Sample Size: Is 1000 Pairs Enough?","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#current-situation","title":"Current Situation","text":"<ul> <li>246 samples total</li> <li>1000 random pairs generated</li> <li>Maximum possible unique pairs: C(246, 2) = 30,135 pairs</li> <li>Current sampling: 1000/30,135 = 3.3% of all possible pairs</li> </ul>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#guidelines-for-pair-sampling","title":"Guidelines for Pair Sampling","text":"<p>From statistical literature on correlation analysis:</p> <p>Power Analysis for Correlation: - To detect r=0.8 with 80% power and \u03b1=0.05: n \u2265 12 pairs - To detect r=0.5 with 80% power: n \u2265 29 pairs - To detect r=0.3 with 80% power: n \u2265 84 pairs</p> <p>So 1000 pairs is MORE than sufficient for statistical power.</p> <p>But...</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#the-real-question-representative-sampling","title":"The Real Question: Representative Sampling","text":"<p>With 246 samples and 1000 pairs: - Average sample appears in: 1000 \u00d7 2 / 246 \u2248 8 pairs - Some samples may appear 0 times, others 20+ times (random variation) - This creates dependency between pairs (not truly independent observations)</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#recommendations","title":"Recommendations","text":"<p>Option 1: Sample ALL pairs (computational cost) - 30,135 pairs is not computationally prohibitive - Ensures complete coverage - Removes sampling bias</p> <p>Option 2: Stratified random sampling - Ensure each sample appears in at least N pairs (e.g., 5-10) - Better coverage than pure random sampling - Still computationally efficient</p> <p>Option 3: Increase to 5000-10000 pairs - Covers ~17-33% of all possible pairs - More stable estimates - Since computation is fast (you mentioned \"doesn't take too much time\"), this is easy to do</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#3-sampling-methodology-random-pairs-with-replacement","title":"3. Sampling Methodology: Random Pairs with Replacement","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#current-implementation-cell-8","title":"Current Implementation (Cell 8)","text":"<pre><code>n_pairs = 1000\nfor i in range(n_pairs):\n    idx1, idx2 = random.sample(range(n_samples), 2)\n    pairs.append((idx1, idx2))\n</code></pre> <p>This is sampling WITHOUT replacement within each pair (idx1 \u2260 idx2), but WITH replacement across pairs (same pair can be selected multiple times).</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#problems","title":"Problems","text":"<ol> <li>Duplicate pairs: (A, B) could be selected multiple times</li> <li>Asymmetric pairs: Both (A, B) and (B, A) could be selected (though similarity is symmetric, this wastes samples)</li> <li>Non-uniform coverage: Some pairs over-sampled, others never sampled</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#better-approach","title":"Better Approach","text":"<p>For small datasets (like 246 samples):</p> <pre><code># Generate ALL unique pairs\nfrom itertools import combinations\nall_pairs = list(combinations(range(n_samples), 2))\n\n# Optional: Sample from all_pairs if needed\nif len(all_pairs) &gt; max_pairs:\n    pairs = random.sample(all_pairs, max_pairs)\nelse:\n    pairs = all_pairs\n</code></pre> <p>Advantages: - No duplicate pairs - Guaranteed unique coverage - More statistically rigorous</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#4-dependency-between-pairs","title":"4. Dependency Between Pairs","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#the-problem","title":"The Problem","text":"<p>Pairs are not independent observations when samples are reused:</p> <p>Example: - Pair 1: (A, B) \u2192 similarity = 0.8 - Pair 2: (A, C) \u2192 similarity = 0.6 - Pair 3: (B, C) \u2192 similarity = 0.7</p> <p>All three pairs share samples, so they're not independent. This violates the independence assumption of correlation tests (Pearson, Spearman).</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#impact-on-p-values","title":"Impact on p-values","text":"<p>The reported p-values (e.g., p=6.81e-235) assume independent observations. With dependent pairs: - p-values are too small (overconfident) - Standard errors are underestimated - Confidence intervals are too narrow</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#correction-approaches","title":"Correction Approaches","text":"<p>Option 1: Permutation testing - Shuffle one embedding type, recalculate correlation, repeat 10,000 times - Empirical p-value from permutation distribution - Doesn't assume independence</p> <p>Option 2: Bootstrap confidence intervals - Resample samples (not pairs), generate new pairs, calculate correlation - Repeat 10,000 times - 95% CI from bootstrap distribution</p> <p>Option 3: Report effect size only - Don't rely on p-values - Focus on correlation magnitude and practical significance</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#5-proposed-sensitivity-analysis","title":"5. Proposed Sensitivity Analysis","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#test-1-exclude-high-similarity-pairs","title":"Test 1: Exclude High-Similarity Pairs","text":"<p>Remove pairs where either GE or ENVO similarity &gt; 0.95:</p> <pre><code># Exclude likely duplicates\npairs_filtered = pairs_df[\n    (pairs_df['ge_similarity'] &lt; 0.95) &amp;\n    (pairs_df['envo_broad_similarity'] &lt; 0.95)\n]\n\n# Recalculate correlation\npearson_filtered, p_filtered = pearsonr(\n    pairs_filtered['ge_similarity'],\n    pairs_filtered['envo_broad_similarity']\n)\n</code></pre> <p>Expected outcome: - If correlation drops significantly (e.g., 0.811 \u2192 0.4), the result is driven by duplicates - If correlation stays strong (0.811 \u2192 0.7+), the relationship is robust</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#test-2-exclude-perfect-matches","title":"Test 2: Exclude Perfect Matches","text":"<p>Remove pairs where both GE AND ENVO similarity = 1.0:</p> <pre><code>pairs_no_perfect = pairs_df[\n    ~((pairs_df['ge_similarity'] == 1.0) &amp;\n      (pairs_df['envo_broad_similarity'] == 1.0))\n]\n</code></pre>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#test-3-increase-sample-size","title":"Test 3: Increase Sample Size","text":"<p>Run with 10,000 pairs or ALL possible pairs:</p> <pre><code># Generate all unique pairs\nfrom itertools import combinations\nall_pairs = list(combinations(range(len(df_clean)), 2))\nprint(f\"Total unique pairs: {len(all_pairs)}\")\n</code></pre> <p>Check if correlation coefficient stabilizes or changes.</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#test-4-stratified-sampling","title":"Test 4: Stratified Sampling","text":"<p>Ensure diverse pairs by sampling across environmental categories:</p> <pre><code># Group by ENVO broad_scale term\ngroups = df_clean.groupby('env_broad_scale')\n\n# Sample pairs within and across groups\nwithin_group_pairs = []  # Same ENVO term\nacross_group_pairs = []  # Different ENVO terms\n</code></pre>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#6-additional-statistical-concerns","title":"6. Additional Statistical Concerns","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#multiple-testing","title":"Multiple Testing","text":"<p>The notebook tests 3 ENVO types (broad, medium, local) against GE. - With no correction, we expect 1/20 false positives at \u03b1=0.05 - Should apply Bonferroni correction: \u03b1_corrected = 0.05/3 = 0.0167 - Or report False Discovery Rate (FDR) adjusted p-values</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#distribution-of-similarities","title":"Distribution of Similarities","text":"<p>From cell-11 output:</p> <pre><code>Google Earth Embeddings:\n  Mean: 0.381\n  Range: [-0.088, 1.000]\n\nENVO broad_scale:\n  Mean: 0.636\n  Range: [0.210, 1.000]\n</code></pre> <p>GE similarities can be negative (cosine similarity range: [-1, 1]) ENVO similarities are all positive (OpenAI embeddings are typically [0, 1])</p> <p>This asymmetry might affect correlation: - Pearson correlation assumes linear relationship - If relationship is non-linear, Pearson may be misleading - Spearman (rank-based) is more robust</p> <p>Observation: Spearman is consistently lower (0.677 vs 0.811 for broad_scale) - Suggests relationship may not be perfectly linear - Or there are outliers influencing Pearson</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#7-recommendations","title":"7. Recommendations","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#immediate-actions-easy-wins","title":"Immediate Actions (Easy Wins)","text":"<ol> <li>Add sensitivity analysis cell:</li> <li>Exclude pairs with similarity &gt; 0.95 on either metric</li> <li>Exclude perfect (1.0, 1.0) pairs</li> <li> <p>Report correlations before/after filtering</p> </li> <li> <p>Increase to ALL pairs or 10,000 pairs:</p> </li> <li>You said computation is fast</li> <li>This is scientifically more rigorous</li> <li> <p>30,135 pairs is not computationally expensive</p> </li> <li> <p>Add robustness checks:</p> </li> <li>Bootstrap confidence intervals (1000 iterations)</li> <li>Permutation test for p-values</li> <li>Report effect sizes with CIs, not just p-values</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#medium-term-improvements","title":"Medium-Term Improvements","text":"<ol> <li>Stratified analysis:</li> <li>Separate correlations by ENVO term frequency</li> <li> <p>Do rare ENVO terms show different patterns?</p> </li> <li> <p>Visualize outliers:</p> </li> <li>Identify pairs driving the correlation</li> <li> <p>Are high-correlation points real or duplicates?</p> </li> <li> <p>Multiple testing correction:</p> </li> <li>Bonferroni or FDR correction for 3 ENVO types</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#long-term-requires-more-data","title":"Long-Term (Requires More Data)","text":"<ol> <li>Process more samples:</li> <li>Current: 246 samples</li> <li>You mentioned \"we can easily provide more now\"</li> <li> <p>Target: 1000+ samples for robust analysis</p> </li> <li> <p>Cross-validation:</p> </li> <li>Split data into train/test sets</li> <li>Check if correlation generalizes</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#8-concrete-next-steps","title":"8. Concrete Next Steps","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#create-new-notebook-cell-sensitivity-analysis","title":"Create New Notebook Cell: Sensitivity Analysis","text":"<pre><code>print(\"=== SENSITIVITY ANALYSIS ===\")\n\n# Original correlation\nprint(f\"\\nOriginal (all {len(pairs_df)} pairs):\")\nprint(f\"  Pearson r = {pearson_broad:.3f}\")\n\n# Test 1: Remove high-similarity pairs\npairs_filtered = pairs_df[\n    (pairs_df['ge_similarity'] &lt; 0.95) &amp;\n    (pairs_df['envo_broad_similarity'] &lt; 0.95)\n]\nr_filtered, p_filtered = pearsonr(\n    pairs_filtered['ge_similarity'],\n    pairs_filtered['envo_broad_similarity']\n)\nprint(f\"\\nAfter removing similarity &gt; 0.95 ({len(pairs_filtered)} pairs):\")\nprint(f\"  Pearson r = {r_filtered:.3f}\")\nprint(f\"  Change: {r_filtered - pearson_broad:.3f}\")\n\n# Test 2: Remove perfect matches\npairs_no_perfect = pairs_df[\n    ~((pairs_df['ge_similarity'] == 1.0) &amp;\n      (pairs_df['envo_broad_similarity'] == 1.0))\n]\nr_no_perfect, p_no_perfect = pearsonr(\n    pairs_no_perfect['ge_similarity'],\n    pairs_no_perfect['envo_broad_similarity']\n)\nprint(f\"\\nAfter removing (1.0, 1.0) pairs ({len(pairs_no_perfect)} pairs):\")\nprint(f\"  Pearson r = {r_no_perfect:.3f}\")\nprint(f\"  Change: {r_no_perfect - pearson_broad:.3f}\")\n\n# Test 3: Bootstrap confidence intervals\nfrom scipy.stats import bootstrap\ndata = (pairs_df['ge_similarity'].values, pairs_df['envo_broad_similarity'].values)\ndef correlation_statistic(x, y):\n    return pearsonr(x, y)[0]\n\n# This requires scipy &gt;= 1.7\n# res = bootstrap(data, correlation_statistic, n_resamples=1000, method='percentile')\n# print(f\"\\n95% Bootstrap CI: [{res.confidence_interval.low:.3f}, {res.confidence_interval.high:.3f}]\")\n</code></pre>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#modify-cell-8-sample-all-pairs","title":"Modify Cell 8: Sample ALL Pairs","text":"<pre><code>from itertools import combinations\n\n# Generate ALL unique pairs instead of random sample\nprint(f\"Generating all unique pairs from {n_samples} samples...\")\nall_possible_pairs = list(combinations(range(n_samples), 2))\nprint(f\"Total unique pairs: {len(all_possible_pairs)}\")\n\n# Use all pairs (computation is fast)\npairs = all_possible_pairs\n\n# Or limit if needed (but 30K pairs should be fast)\n# max_pairs = 10000\n# if len(all_possible_pairs) &gt; max_pairs:\n#     pairs = random.sample(all_possible_pairs, max_pairs)\n# else:\n#     pairs = all_possible_pairs\n</code></pre>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#9-expected-outcomes","title":"9. Expected Outcomes","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#if-correlation-remains-strong-after-filtering-r-07","title":"If correlation remains strong after filtering (r &gt; 0.7):","text":"<p>\u2705 The relationship is real and robust \u2705 Geographic and ontological similarities truly correlate \u2705 Results are publishable with proper caveats</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#if-correlation-drops-substantially-r-05","title":"If correlation drops substantially (r &lt; 0.5):","text":"<p>\u274c Original result was driven by duplicates/self-pairs \u274c Need to investigate why so many near-identical samples \u274c May need to deduplicate dataset before analysis</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#most-likely-scenario","title":"Most Likely Scenario:","text":"<ul> <li>Correlation drops moderately (0.811 \u2192 0.6-0.7)</li> <li>Still significant and meaningful</li> <li>More honest assessment of relationship strength</li> </ul>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#10-summary","title":"10. Summary","text":"<p>Critical Flaw: 21% of pairs are near-perfect matches (&gt;0.9, &gt;0.9), likely inflating correlation</p> <p>Quick Fix: 1. Filter out high-similarity pairs (&gt;0.95) 2. Use ALL 30,135 pairs instead of 1000 random pairs 3. Report correlation with and without filtering</p> <p>Scientific Rigor: - Current p-values are likely too optimistic (dependency issue) - Need bootstrap CIs or permutation tests - Should report effect sizes, not just significance</p> <p>Data Size: - 1000 pairs is statistically sufficient - But ALL pairs is better (more stable, no sampling bias) - Processing more samples (1000+ vs current 246) would strengthen conclusions</p> <p>Bottom Line: The high correlation (r=0.811) is suspicious until we verify it's not driven by duplicate/near-duplicate samples. The sensitivity analysis will reveal the truth.</p>"},{"location":"development/CI_SETUP/","title":"CI/CD and Quality Assurance Setup","text":"<p>This document describes the comprehensive CI/CD and code quality setup for the env-embeddings project.</p>"},{"location":"development/CI_SETUP/#overview","title":"Overview","text":"<p>The project now has a complete quality assurance pipeline that runs: - Tests with coverage reporting - Type checking with mypy - Linting with ruff - Dependency checking with deptry</p> <p>These checks run: 1. Locally via <code>just test</code> 2. On commit via pre-commit hooks 3. On push via pre-push hooks 4. On CI via GitHub Actions</p>"},{"location":"development/CI_SETUP/#quick-start","title":"Quick Start","text":""},{"location":"development/CI_SETUP/#install-pre-commit-hooks","title":"Install Pre-commit Hooks","text":"<pre><code># Install pre-commit tool (if not already installed)\nuv tool install pre-commit\n\n# Install the hooks\npre-commit install\npre-commit install --hook-type pre-push\n</code></pre>"},{"location":"development/CI_SETUP/#run-all-quality-checks","title":"Run All Quality Checks","text":"<pre><code>just test\n</code></pre> <p>This runs: 1. <code>pytest</code> with coverage (minimum 50% coverage) 2. <code>mypy</code> for type checking 3. <code>ruff</code> for linting 4. <code>deptry</code> for dependency analysis</p>"},{"location":"development/CI_SETUP/#test-commands","title":"Test Commands","text":""},{"location":"development/CI_SETUP/#core-commands","title":"Core Commands","text":"Command Description <code>just test</code> Run all quality checks (pytest-cov + mypy + ruff + deptry) <code>just pytest-cov</code> Run tests with coverage and timing <code>just pytest</code> Run tests only (no coverage) <code>just mypy</code> Run type checking <code>just ruff</code> Run linting <code>just deptry</code> Check for unused/missing dependencies"},{"location":"development/CI_SETUP/#advanced-commands","title":"Advanced Commands","text":"Command Description <code>just test-full</code> Run all checks including integration tests <code>just pytest-integration</code> Run integration tests <code>just doctest</code> Run doctests in src/"},{"location":"development/CI_SETUP/#test-coverage","title":"Test Coverage","text":"<p>Current coverage: 9% (baseline from existing tests)</p> <p>Coverage reports: - Terminal: Shows missing lines after test run - HTML: Generated in <code>htmlcov/</code> directory - Minimum: 50% coverage required for pre-push</p> <p>View HTML coverage:</p> <pre><code>open htmlcov/index.html\n</code></pre>"},{"location":"development/CI_SETUP/#pre-commit-hooks","title":"Pre-commit Hooks","text":""},{"location":"development/CI_SETUP/#installed-hooks","title":"Installed Hooks","text":"<p>On every commit: - <code>check-toml</code> - Validate TOML files - <code>check-yaml</code> - Validate YAML files - <code>end-of-file-fixer</code> - Ensure files end with newline - <code>trailing-whitespace</code> - Remove trailing whitespace - <code>yamllint</code> - Lint YAML files - <code>codespell</code> - Spell checking - <code>typos</code> - Typo detection - <code>ruff</code> - Linting with auto-fix - <code>ruff-format</code> - Code formatting - <code>uv-lock</code> - Keep uv.lock in sync - <code>mypy</code> - Type checking - <code>pytest</code> - Run tests</p> <p>On push: - <code>pytest-cov</code> - Run tests with 50% minimum coverage</p>"},{"location":"development/CI_SETUP/#manual-pre-commit-run","title":"Manual Pre-commit Run","text":"<pre><code># Run on all files\npre-commit run --all-files\n\n# Run specific hook\npre-commit run mypy --all-files\n\n# Skip hooks for a commit (not recommended)\ngit commit --no-verify\n</code></pre>"},{"location":"development/CI_SETUP/#github-actions","title":"GitHub Actions","text":"<p>The CI pipeline runs on: - Push to main - Pull requests</p> <p>Workflow: <code>.github/workflows/main.yaml</code></p> <p>Matrix testing across Python versions: - 3.10 - 3.11 - 3.12 - 3.13</p>"},{"location":"development/CI_SETUP/#configuration-files","title":"Configuration Files","text":""},{"location":"development/CI_SETUP/#pytest-pytestini-or-pyprojecttoml","title":"pytest (<code>pytest.ini</code> or <code>pyproject.toml</code>)","text":"<pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\n</code></pre>"},{"location":"development/CI_SETUP/#mypy-mypyini","title":"mypy (<code>mypy.ini</code>)","text":"<p>Key settings: - Type checking for <code>src/</code> and <code>tests/</code> - Ignores missing imports for external libraries without type stubs:   - pandas, pytest, typer, diskcache, tqdm, ols_client, global_land_mask, ee</p>"},{"location":"development/CI_SETUP/#ruff-pyprojecttoml","title":"ruff (<code>pyproject.toml</code>)","text":"<ul> <li>Modern, fast Python linter</li> <li>Replaces flake8, isort, and more</li> <li>Auto-fixes many issues</li> </ul>"},{"location":"development/CI_SETUP/#deptry-pyprojecttoml","title":"deptry (<code>pyproject.toml</code>)","text":"<pre><code>[tool.deptry]\nper_rule_ignores = {\n  DEP002 = [\"linkml-runtime\", \"numpy\", \"requests-cache\", \"global-land-mask\"],\n  DEP003 = [\"typing_extensions\"]\n}\n</code></pre>"},{"location":"development/CI_SETUP/#dependencies","title":"Dependencies","text":""},{"location":"development/CI_SETUP/#development-dependencies","title":"Development Dependencies","text":"<p>All testing tools are installed as dev dependencies:</p> <pre><code>uv sync --group dev\n</code></pre> <p>Installed tools: - <code>pytest</code> - Testing framework - <code>pytest-cov</code> - Coverage plugin - <code>mypy</code> - Type checker - <code>ruff</code> - Linter/formatter - <code>deptry</code> - Dependency checker</p>"},{"location":"development/CI_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/CI_SETUP/#failed-to-spawn-pytest","title":"\"Failed to spawn: pytest\"","text":"<p>Problem: Virtual environment may be corrupt or pointing to wrong path</p> <p>Solution:</p> <pre><code>rm -rf .venv\nuv sync --group dev\n</code></pre>"},{"location":"development/CI_SETUP/#mypy-cant-find-type-stubs","title":"Mypy can't find type stubs","text":"<p>Problem: External libraries missing type information</p> <p>Solution: Add to <code>mypy.ini</code>:</p> <pre><code>[mypy-package_name.*]\nignore_missing_imports = True\n</code></pre>"},{"location":"development/CI_SETUP/#deptry-false-positives","title":"Deptry false positives","text":"<p>Problem: Legitimate dependencies flagged as unused</p> <p>Solution: Add to <code>pyproject.toml</code>:</p> <pre><code>[tool.deptry]\nper_rule_ignores = {DEP002 = [\"package_name\"]}\n</code></pre>"},{"location":"development/CI_SETUP/#coverage-too-low","title":"Coverage too low","text":"<p>Problem: Tests don't cover enough code</p> <p>Solution: Write more tests or adjust minimum:</p> <pre><code># Adjust minimum in .pre-commit-config.yaml\nentry: uv run pytest --cov=src/env_embeddings --cov-fail-under=40\n</code></pre>"},{"location":"development/CI_SETUP/#best-practices","title":"Best Practices","text":""},{"location":"development/CI_SETUP/#1-run-tests-before-committing","title":"1. Run tests before committing","text":"<pre><code>just test\n</code></pre>"},{"location":"development/CI_SETUP/#2-let-pre-commit-auto-fix-issues","title":"2. Let pre-commit auto-fix issues","text":"<p>Pre-commit will auto-fix many issues (trailing whitespace, formatting, etc.)</p>"},{"location":"development/CI_SETUP/#3-dont-skip-hooks","title":"3. Don't skip hooks","text":"<p>Only use <code>--no-verify</code> in emergencies</p>"},{"location":"development/CI_SETUP/#4-keep-coverage-high","title":"4. Keep coverage high","text":"<p>Aim for &gt;80% coverage on new code</p>"},{"location":"development/CI_SETUP/#5-fix-type-errors","title":"5. Fix type errors","text":"<p>Don't just add <code># type: ignore</code> - fix the actual issue</p>"},{"location":"development/CI_SETUP/#6-update-dependencies-regularly","title":"6. Update dependencies regularly","text":"<pre><code>uv lock --upgrade\n</code></pre>"},{"location":"development/CI_SETUP/#example-workflow","title":"Example Workflow","text":"<pre><code># Make changes to code\nvim src/env_embeddings/earth_engine.py\n\n# Run tests locally\njust test\n\n# Tests pass! Stage changes\ngit add src/env_embeddings/earth_engine.py\n\n# Commit (pre-commit hooks run automatically)\ngit commit -m \"Add retry logic to earth engine\"\n\n# Push (pre-push hooks run automatically including coverage check)\ngit push origin feature-branch\n\n# GitHub Actions runs on the PR\n# All checks must pass before merge\n</code></pre>"},{"location":"development/CI_SETUP/#summary","title":"Summary","text":"<p>\u2705 Complete test suite with coverage reporting \u2705 Type safety with mypy \u2705 Code quality with ruff \u2705 Dependency health with deptry \u2705 Pre-commit hooks for immediate feedback \u2705 CI/CD with GitHub Actions \u2705 No workarounds - clean <code>uv run</code> commands</p> <p>The setup ensures high code quality without being overly restrictive. All checks are fast and provide clear error messages.</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/","title":"Performance Optimizations","text":""},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#problem-slow-google-earth-engine-failures","title":"Problem: Slow Google Earth Engine Failures","text":"<p>When processing large datasets, failures were taking a long time because: 1. Each failure required a network request to Google Earth Engine 2. Fallback year attempts doubled the network calls 3. Repeated failures for the same location/year kept making the same slow requests</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#solution-cached-failures-verbose-logging","title":"Solution: Cached Failures + Verbose Logging","text":""},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#1-cache-failures-fast-fail","title":"1. Cache Failures (Fast-Fail)","text":"<p>Before: - Ocean location at (34.24, 144.31, 2010) \u2192 slow API call \u2192 failure - Same location at (34.24, 144.31, 2010) \u2192 slow API call again \u2192 failure again</p> <p>After: - Ocean location at (34.24, 144.31, 2010) \u2192 slow API call \u2192 failure \u2192 cached as None - Same location at (34.24, 144.31, 2010) \u2192 instant cache lookup \u2192 failure (cached)</p> <p>Implementation:</p> <pre><code># earth_engine.py\nif use_cache and cache_key in _cache:\n    cached_value = _cache[cache_key]\n    if cached_value is None:\n        raise ValueError(f\"No embedding found... (cached failure)\")\n    return cached_value\n\n# Cache failures\nexcept ValueError as e:\n    if use_cache and \"No embedding found\" in str(e):\n        _cache[cache_key] = None  # Cache the failure\n    raise\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#2-verbose-logging","title":"2. Verbose Logging","text":"<p>Before (with tqdm):</p> <pre><code>Processing samples: 45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | 135/300 [02:15&lt;02:45, 1.00row/s]\n</code></pre> <p>No visibility into: - What coordinates are being processed - Whether cache or API was used - Why failures occurred</p> <p>After:</p> <pre><code>Processing samples: 45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | 135/300 [02:15&lt;02:45, 1.00row/s]\nRow 132: \u2713 Got embedding for (35.1180,138.9370) year=2017 [CACHE]\nRow 133: \u2717 No embedding for (34.2400,144.3100) year=1998 [API] - trying fallback year 2020\nRow 133: \u2717 No embedding for (34.2400,144.3100) year=2020 [API - FALLBACK] - SKIPPING\nRow 134: \u2713 Got embedding for (36.0540,140.1230) year=2015 [API]\nRow 135: \u2717 No embedding for (34.2400,144.3100) year=1998 [CACHED FAILURE] - trying fallback year 2020\nRow 135: \u2717 No embedding for (34.2400,144.3100) year=2020 [CACHED FAILURE - FALLBACK] - SKIPPING\n</code></pre> <p>Now you can see: - Coordinates: Exact lat/lon being processed - Year: Original year and fallback year - Source: <code>[CACHE]</code> vs <code>[API]</code> - instant vs slow - Status: \u2713 success vs \u2717 failure - Cached Failures: Instant failure on second attempt</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#3-performance-impact","title":"3. Performance Impact","text":"<p>Scenario: 300 rows with 50% ocean locations</p> <p>Before: - 150 ocean locations \u00d7 2 API calls (original + fallback) \u00d7 2 seconds = 600 seconds of API calls - Processing ocean samples multiple times \u2192 same slow failures</p> <p>After (first run): - 150 ocean locations \u00d7 2 API calls \u00d7 2 seconds = 600 seconds (same on first run) - But failures are cached</p> <p>After (second run or duplicate coordinates): - 150 ocean locations \u00d7 instant cache lookup = &lt; 1 second - 600x speedup for cached failures</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#4-statistics-tracking","title":"4. Statistics Tracking","text":"<p>The <code>ProcessingStats</code> class now correctly tracks: - Cache hits: Includes both successful cache hits AND cached failures - Cache misses: Only new API calls - API success: Successful new embeddings - API failures: Categorized by type (no coverage, rate limit, other)</p> <p>Example output:</p> <pre><code>=== Google Earth Embeddings Processing ===\nSuccessfully retrieved embeddings: 245\n  - From cache: 180\n  - From API: 65\n\nFailed to retrieve embeddings: 55\n  - No coverage/ocean: 50\n  - Rate limit (429): 0\n  - Other errors: 5\n\nSkipped rows:\n  - Already had embeddings: 0\n  - Invalid data (missing coords/dates): 0\n\nCache information:\n  - Total entries in cache: 350 (includes 50 cached failures)\n  - Cache directory: /Users/MAM/.cache/env-embeddings/google_earth\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#usage","title":"Usage","text":""},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#commands","title":"Commands","text":"<pre><code># Process 300 RANDOM rows - failures are now fast after first run\n# Note: Uses random sampling (not sequential) to avoid bias\nuv run env-embeddings add-google-embeddings-csv \\\n  data/satisfying_biosamples_normalized.csv \\\n  --max-rows 300 \\\n  --output data/temp_with_google_300.csv\n\nuv run env-embeddings add-envo-embeddings-csv \\\n  data/temp_with_google_300.csv \\\n  --max-rows 300 \\\n  --output data/with_both_embeddings_300.csv\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#random-sampling-new-in-v2","title":"Random Sampling (New in v2)","text":"<p>Both <code>add-google-embeddings-csv</code> and <code>add-envo-embeddings-csv</code> now use random sampling instead of sequential processing:</p> <p>Before:</p> <pre><code>df.head(max_rows)  # First N rows\n</code></pre> <p>After:</p> <pre><code>df.sample(n=min(max_rows, len(df)), random_state=42)  # Random N rows\n</code></pre> <p>Why this matters: - Avoids bias from data ordering (e.g., all samples from one region at start of file) - Provides representative sample across geographic/temporal diversity - Uses fixed random seed (42) for reproducibility - Critical for scientific validity when sampling subsets</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#what-to-expect","title":"What to Expect","text":"<p>First run with new data: - Ocean/no-coverage locations will be slow (2 API calls each) - But you'll see exactly which coordinates are failing and why - Failures are cached for next time</p> <p>Subsequent runs or duplicate coordinates: - Cached failures are instant - Progress bar moves much faster through known-bad locations - Verbose output shows <code>[CACHED FAILURE]</code> for instant failures</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#debugging-failed-requests","title":"Debugging Failed Requests","text":"<p>With the new verbose logging, you can:</p> <ol> <li> <p>Identify problem coordinates: <code>Row 133: \u2717 No embedding for (34.2400,144.3100) year=1998 [API] - SKIPPING</code>    \u2192 This is an ocean location, no coverage</p> </li> <li> <p>See year issues: <code>Row 142: \u2717 No embedding for (51.5074,-0.1278) year=1985 [API] - trying fallback year 2020    Row 142: \u2713 Got embedding for (51.5074,-0.1278) year=2020 [API - FALLBACK]</code>    \u2192 1985 has no data, but 2020 works</p> </li> <li> <p>Spot patterns:</p> </li> <li>Multiple failures with similar coordinates \u2192 might be ocean region</li> <li>All failures in certain year range \u2192 dataset coverage issue</li> <li>Specific error messages \u2192 can debug with Google Earth Engine docs</li> </ol>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#cache-management","title":"Cache Management","text":""},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#view-cache","title":"View Cache","text":"<pre><code># Google Earth cache\nls -lh ~/.cache/env-embeddings/google_earth/\n\n# ENVO cache\nls -lh ~/.cache/env-embeddings/envo/\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#clear-cache-if-needed","title":"Clear Cache (if needed)","text":"<pre><code>from env_embeddings.earth_engine import clear_cache\nclear_cache()\n</code></pre> <p>Or manually:</p> <pre><code>rm -rf ~/.cache/env-embeddings/google_earth/\nrm -rf ~/.cache/env-embeddings/envo/\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#benefits","title":"Benefits","text":"<p>\u2705 Fast failures - Cached failures are instant (600x faster) \u2705 Detailed logging - See exactly what's happening with each request \u2705 Better debugging - Coordinates, years, and error types visible \u2705 Cache visibility - Know when cache vs API is used \u2705 Accurate statistics - Proper tracking of cache hits, API calls, failures \u2705 No behavior change - Same results, just faster and more visible</p>"},{"location":"development/improvements_summary/","title":"Improvements Summary","text":""},{"location":"development/improvements_summary/#overview","title":"Overview","text":"<p>This document summarizes the major improvements made to the env-embeddings project to enhance performance, reliability, and usability when processing large-scale biological sample datasets.</p>"},{"location":"development/improvements_summary/#key-improvements","title":"Key Improvements","text":""},{"location":"development/improvements_summary/#1-removed-ocean-pre-filtering","title":"1. Removed Ocean Pre-filtering \u2705","text":"<p>Problem: The <code>global-land-mask</code> pre-filtering was too aggressive and skipped valid coastal samples that might have satellite coverage.</p> <p>Solution: Removed the ocean filtering step entirely. Google Earth Engine is now the source of truth for coverage - if a location has no satellite data, GEE will tell us directly. This maximizes coverage while still being efficient since failures are cached.</p> <p>Impact: Maximal coverage of samples, including coastal and shallow water locations that have satellite imagery.</p>"},{"location":"development/improvements_summary/#2-added-retry-logic-with-exponential-backoff","title":"2. Added Retry Logic with Exponential Backoff \u2705","text":"<p>Problem: No handling for HTTP 429 \"Too Many Requests\" errors from Google Earth Engine API.</p> <p>Solution: Implemented <code>_retry_with_exponential_backoff()</code> function in <code>earth_engine.py</code> that: - Detects rate limit errors (429, \"too many requests\", \"quota\") - Retries up to 5 times with exponential backoff (1s, 2s, 4s, 8s, 16s) - Immediately raises non-rate-limit errors without retry - Wraps all Earth Engine API calls</p> <p>Code Location: <code>src/env_embeddings/earth_engine.py:57-93</code></p> <p>Impact: Robust handling of rate limits, automatic recovery from temporary quota issues.</p>"},{"location":"development/improvements_summary/#3-comprehensive-statistics-tracking","title":"3. Comprehensive Statistics Tracking \u2705","text":"<p>Problem: Limited visibility into processing - only final counts were shown.</p> <p>Solution: Created <code>ProcessingStats</code> dataclass in <code>sample_processor.py</code> that tracks: - Cache hits vs cache misses - API successes - API failures by type:   - No coverage/ocean locations   - Rate limit (429) errors   - Other errors - Rows skipped (existing embeddings) - Rows skipped (invalid data)</p> <p>Code Location: <code>src/env_embeddings/sample_processor.py:16-46</code></p> <p>Impact: Detailed visibility into processing efficiency, cache effectiveness, and failure reasons.</p>"},{"location":"development/improvements_summary/#4-coordinate-normalization-for-cache-consistency","title":"4. Coordinate Normalization for Cache Consistency \u2705","text":"<p>Problem: Coordinates with different representations (35.1180 vs 35.118) would create duplicate cache entries for semantically identical locations.</p> <p>Solution: All coordinates are normalized to 4 decimal places before: - Creating cache keys - Making Earth Engine API calls</p> <p>Code Location: <code>src/env_embeddings/earth_engine.py:126-132</code></p> <p>Example:</p> <pre><code>lat_normalized = round(float(lat), 4)  # 35.1180 \u2192 35.118\nlon_normalized = round(float(lon), 4)  # 138.9370 \u2192 138.937\ncache_key = (lat_normalized, lon_normalized, year)\n</code></pre> <p>Impact: Reduced duplicate API calls, more efficient caching, consistent coordinate representation.</p>"},{"location":"development/improvements_summary/#5-automatic-filtering-of-incomplete-rows","title":"5. Automatic Filtering of Incomplete Rows \u2705","text":"<p>Problem: Output files contained rows with missing embeddings, requiring manual cleanup.</p> <p>Solution: Both <code>add_google_earth_embeddings_to_csv()</code> and <code>add_envo_embeddings_to_csv()</code> now: - Automatically filter out rows with missing embeddings before saving - Report filtering statistics (rows before/after, rows removed) - For ENVO: only keep rows with ALL 3 embeddings (broad_scale, medium, local_scale) - For Google Earth: only keep rows with valid satellite embeddings</p> <p>Code Locations: - Google Earth: <code>src/env_embeddings/sample_processor.py:604-626</code> - ENVO: <code>src/env_embeddings/sample_processor.py:791-815</code></p> <p>Impact: Clean output files ready for analysis, no manual post-processing needed.</p>"},{"location":"development/improvements_summary/#updated-workflow","title":"Updated Workflow","text":""},{"location":"development/improvements_summary/#step-1-add-google-earth-embeddings","title":"Step 1: Add Google Earth Embeddings","text":"<pre><code>uv run env-embeddings add-google-embeddings-csv \\\n  data/satisfying_biosamples_normalized.csv \\\n  --max-rows 50 \\\n  --output data/temp_with_google.csv\n</code></pre> <p>Output Statistics:</p> <pre><code>=== Google Earth Embeddings Processing ===\nSuccessfully retrieved embeddings: 45\n  - From cache: 12\n  - From API: 33\n\nFailed to retrieve embeddings: 5\n  - No coverage/ocean: 3\n  - Rate limit (429): 0\n  - Other errors: 2\n\nSkipped rows:\n  - Already had embeddings: 0\n  - Invalid data (missing coords/dates): 0\n\nCache information:\n  - Total entries in cache: 145\n  - Cache directory: /Users/MAM/.cache/env-embeddings/google_earth\n\nOutput filtering:\n  - Rows before filtering: 50\n  - Rows after filtering: 45\n  - Rows removed (incomplete embeddings): 5\n</code></pre>"},{"location":"development/improvements_summary/#step-2-add-envo-embeddings","title":"Step 2: Add ENVO Embeddings","text":"<pre><code>uv run env-embeddings add-envo-embeddings-csv \\\n  data/temp_with_google.csv \\\n  --max-rows 50 \\\n  --output data/with_both_embeddings.csv\n</code></pre> <p>Output Statistics:</p> <pre><code>=== ENVO Embeddings Processing ===\nSuccessfully retrieved embeddings: 135 (45 rows \u00d7 3 columns)\n  - From cache: 87\n  - From API: 48\n\nFailed to retrieve embeddings: 15\n  - No coverage/ocean: 0\n  - Rate limit (429): 0\n  - Other errors: 15\n\nCache information:\n  - Total unique ENVO terms cached: 78\n  - Successful embeddings: 65\n  - Failed lookups: 13\n  - Cache directory: /Users/MAM/.cache/env-embeddings/envo\n\nOutput filtering:\n  - Rows before filtering: 45\n  - Rows after filtering: 40\n  - Rows removed (incomplete embeddings): 5\n</code></pre>"},{"location":"development/improvements_summary/#combined-workflow-just-recipe","title":"Combined Workflow (Just Recipe)","text":"<pre><code>just add-both-embeddings\n</code></pre> <p>This runs both steps automatically with 50 test rows.</p>"},{"location":"development/improvements_summary/#technical-details","title":"Technical Details","text":""},{"location":"development/improvements_summary/#cache-implementation","title":"Cache Implementation","text":"<ul> <li>Library: <code>diskcache</code> (SQLite-backed persistent cache)</li> <li>Google Earth Cache: <code>~/.cache/env-embeddings/google_earth/</code></li> <li>ENVO Cache: <code>~/.cache/env-embeddings/envo/</code></li> <li>Cache Keys:</li> <li>Google Earth: <code>(lat_norm, lon_norm, year)</code></li> <li>ENVO: <code>envo_term</code> string (e.g., \"ENVO:00000428\")</li> </ul>"},{"location":"development/improvements_summary/#progress-bar-updates","title":"Progress Bar Updates","text":"<p>Real-time progress information shows: - Success count - Cache hits - Failed count</p> <p>Example: <code>Success: 42, Cache: 28, Failed: 3</code></p>"},{"location":"development/improvements_summary/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"development/improvements_summary/#cache-efficiency","title":"Cache Efficiency","text":"<ul> <li>First run: ~0.4 rows/second (making API calls)</li> <li>Subsequent runs: ~906 rows/second (from cache)</li> <li>2265x speedup with cache</li> </ul>"},{"location":"development/improvements_summary/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Automatic exponential backoff for 429 errors</li> <li>Default retry parameters: 5 attempts, initial 1s delay</li> <li>Maximum total wait time: ~31 seconds (1+2+4+8+16)</li> </ul>"},{"location":"development/improvements_summary/#coordinate-normalization-benefits","title":"Coordinate Normalization Benefits","text":"<ul> <li>Reduces cache size by eliminating duplicates</li> <li>Ensures consistent results across different input formats</li> <li>4 decimal places = ~11 meters precision (sufficient for 10m satellite resolution)</li> </ul>"},{"location":"development/improvements_summary/#files-modified","title":"Files Modified","text":"<ol> <li><code>src/env_embeddings/earth_engine.py</code></li> <li>Added retry logic with exponential backoff</li> <li>Added coordinate normalization</li> <li> <p>Improved error messages</p> </li> <li> <p><code>src/env_embeddings/sample_processor.py</code></p> </li> <li>Added <code>ProcessingStats</code> dataclass</li> <li>Removed ocean pre-filtering</li> <li>Added comprehensive statistics tracking</li> <li>Added automatic filtering of incomplete rows</li> <li> <p>Updated both CSV processing functions</p> </li> <li> <p><code>src/env_embeddings/envo_embeddings.py</code></p> </li> <li> <p>No changes (already had caching)</p> </li> <li> <p><code>project.justfile</code></p> </li> <li>Simplified <code>add-both-embeddings</code> recipe (hardcoded 50 rows)</li> </ol>"},{"location":"development/improvements_summary/#next-steps","title":"Next Steps","text":""},{"location":"development/improvements_summary/#for-production-runs","title":"For Production Runs","text":"<ol> <li>Remove <code>--max-rows</code> limit to process all samples</li> <li>Monitor cache statistics to track efficiency</li> <li>Check rate limit statistics to ensure no quota issues</li> <li>Verify final row counts match expectations</li> </ol>"},{"location":"development/improvements_summary/#potential-future-improvements","title":"Potential Future Improvements","text":"<ol> <li>Parallel processing for faster throughput</li> <li>Batch API requests to reduce round-trips</li> <li>Configurable retry parameters (max_retries, initial_delay)</li> <li>Progress persistence (resume from failure)</li> <li>Separate cache validation/cleaning commands</li> </ol>"},{"location":"development/improvements_summary/#testing-recommendations","title":"Testing Recommendations","text":"<pre><code># Test with small dataset (50 rows)\njust add-both-embeddings\n\n# Test with larger dataset (500 rows)\nuv run env-embeddings add-google-embeddings-csv data/satisfying_biosamples_normalized.csv \\\n  --max-rows 500 --output data/test_google_500.csv\n\nuv run env-embeddings add-envo-embeddings-csv data/test_google_500.csv \\\n  --max-rows 500 --output data/test_both_500.csv\n\n# Full production run (no max-rows limit)\nuv run env-embeddings add-google-embeddings-csv data/satisfying_biosamples_normalized.csv \\\n  --output data/full_with_google.csv\n\nuv run env-embeddings add-envo-embeddings-csv data/full_with_google.csv \\\n  --output data/full_with_both.csv\n</code></pre>"},{"location":"development/improvements_summary/#summary","title":"Summary","text":"<p>All requested improvements have been implemented:</p> <p>\u2705 Remove ocean pre-filter - Maximal coverage, let GEE be source of truth \u2705 Add retry logic - Exponential backoff for 429 errors \u2705 Detailed reporting - Cache hits, API success/failures by type \u2705 Track rate limits - 429 errors tracked separately \u2705 Filter incomplete rows - Automatic cleanup of output \u2705 Coordinate normalization - 4 decimal places for cache consistency</p> <p>The system is now fast, efficient, provides maximum coverage, and gives comprehensive visibility into the processing pipeline.</p>"},{"location":"research/RESEARCH_VISION/","title":"Research Vision: Environmental Context Quality Control and Prediction","text":""},{"location":"research/RESEARCH_VISION/#the-big-picture","title":"The Big Picture","text":"<p>You're building a quality control and recommendation system for environmental metadata using the divergence between satellite imagery (objective, physical) and ontological annotations (subjective, curator-provided).</p>"},{"location":"research/RESEARCH_VISION/#core-insight","title":"Core Insight","text":"<p>Disagreement between Google Earth embeddings and ENVO embeddings is informative:</p> <ul> <li>High GE similarity + High ENVO similarity \u2192 Good metadata, samples are genuinely similar</li> <li>High GE similarity + Low ENVO similarity \u2192 Metadata quality issue - physically similar locations have inconsistent annotations</li> <li>Low GE similarity + High ENVO similarity \u2192 Different locations can share environmental context (e.g., both \"forest\" but different continents)</li> </ul>"},{"location":"research/RESEARCH_VISION/#three-phase-research-pipeline","title":"Three-Phase Research Pipeline","text":""},{"location":"research/RESEARCH_VISION/#phase-1-find-optimal-envo-combination-current-work","title":"Phase 1: Find Optimal ENVO Combination \u2705 (Current Work)","text":"<p>Goal: Determine which combination of MIxS environmental context variables best predicts satellite-based similarity</p> <p>Approaches: 1. Individual scales: <code>broad_scale</code>, <code>medium</code>, <code>local_scale</code> 2. Concatenation: <code>[broad; medium; local]</code> (4608-dim) 3. Weighted average: <code>(broad + medium + local) / 3</code> (1536-dim) 4. Learned weights: <code>w1\u00b7broad + w2\u00b7medium + w3\u00b7local</code> (future)</p> <p>Success Metric: Highest Pearson correlation with GE embeddings after removing degenerate pairs</p> <p>Current Status: Implemented in notebook, will reveal best combination</p>"},{"location":"research/RESEARCH_VISION/#phase-2-identify-and-analyze-outliers-next-step","title":"Phase 2: Identify and Analyze Outliers \ud83c\udfaf (Next Step)","text":"<p>Goal: Find samples with suspicious metadata by detecting divergence between physical and semantic similarity</p>"},{"location":"research/RESEARCH_VISION/#2a-high-ge-low-envo-outliers-metadata-quality-issues","title":"2A. High GE / Low ENVO Outliers (Metadata Quality Issues)","text":"<p>Example Scenario:</p> <pre><code>Sample A: (35.7\u00b0N, 139.8\u00b0E, 2018) - ENVO: \"terrestrial biome | soil | agricultural field\"\nSample B: (35.7\u00b0N, 139.8\u00b0E, 2018) - ENVO: \"marine biome | sea water | coastal zone\"\n\nGE Similarity: 0.99 (same location, same date \u2192 nearly identical satellite view)\nENVO Similarity: 0.25 (completely different biomes!)\n\n\u2192 FLAG: One of these annotations is wrong\n</code></pre> <p>Detection Method:</p> <pre><code># Define outliers as pairs in the upper-right quadrant\noutliers_metadata_issue = pairs_filtered[\n    (pairs_filtered['ge_similarity'] &gt; 0.8) &amp;  # Physically very similar\n    (pairs_filtered['envo_best_similarity'] &lt; 0.3)  # Semantically very different\n]\n</code></pre> <p>Analysis Questions: 1. How many outlier pairs exist? 2. Do certain biosamples appear repeatedly in outlier pairs? (chronic bad metadata) 3. Are there patterns? (e.g., marine/terrestrial confusion, missing terms)</p>"},{"location":"research/RESEARCH_VISION/#2b-low-ge-high-envo-outliers-interesting-biology","title":"2B. Low GE / High ENVO Outliers (Interesting Biology)","text":"<p>Example Scenario:</p> <pre><code>Sample A: (34.0\u00b0N, 118.2\u00b0W) - California desert\nSample B: (31.2\u00b0N, 29.9\u00b0E) - Egyptian desert\n\nGE Similarity: 0.15 (different continents, different geology)\nENVO Similarity: 0.95 (both \"desert biome | sand | arid soil\")\n\n\u2192 VALID: Similar environment types in different locations\n</code></pre> <p>These are not errors - they show that ENVO terms successfully capture environmental similarity across geography.</p>"},{"location":"research/RESEARCH_VISION/#phase-3-metadata-prediction-and-improvement-future-work","title":"Phase 3: Metadata Prediction and Improvement \ud83d\ude80 (Future Work)","text":"<p>Goal: Use satellite imagery to propose or validate ENVO triads</p>"},{"location":"research/RESEARCH_VISION/#3a-predict-missing-triads","title":"3A. Predict Missing Triads","text":"<p>Scenario: Sample has coordinates/date but missing ENVO terms</p> <p>Method:</p> <pre><code>def predict_envo_triad(sample_coords, sample_date, training_data):\n    \"\"\"\n    Predict ENVO triad for a sample with missing metadata.\n\n    1. Get Google Earth embedding for target sample\n    2. Find k-nearest neighbors by GE embedding similarity\n    3. Aggregate their ENVO triads (majority vote or weighted average)\n    4. Return predicted broad/medium/local scale terms\n    \"\"\"\n\n    # Get GE embedding for target\n    target_ge_embedding = get_embedding(lat, lon, year)\n\n    # Find similar samples by satellite imagery\n    similarities = []\n    for train_sample in training_data:\n        sim = cosine_similarity(target_ge_embedding, train_sample.ge_embedding)\n        similarities.append((sim, train_sample))\n\n    # Get top-k most similar samples\n    top_k = sorted(similarities, reverse=True)[:10]\n\n    # Aggregate their ENVO terms (majority vote)\n    broad_candidates = [s.envo_broad for _, s in top_k]\n    medium_candidates = [s.envo_medium for _, s in top_k]\n    local_candidates = [s.envo_local for _, s in top_k]\n\n    return {\n        'broad_scale': most_common(broad_candidates),\n        'medium': most_common(medium_candidates),\n        'local_scale': most_common(local_candidates),\n        'confidence': top_k[0][0]  # similarity to nearest neighbor\n    }\n</code></pre> <p>Validation: - Hold out 20% of samples with known triads - Predict their triads using only GE embeddings - Measure accuracy: How often do we get the exact triad correct?</p>"},{"location":"research/RESEARCH_VISION/#3b-flag-suspect-triads","title":"3B. Flag Suspect Triads","text":"<p>Scenario: Sample has ENVO triad but it disagrees with satellite imagery</p> <p>Method:</p> <pre><code>def flag_suspect_triads(sample, training_data, threshold=0.7):\n    \"\"\"\n    Flag samples whose ENVO triad is inconsistent with their satellite imagery.\n\n    Returns:\n        List of alternative triad suggestions with confidence scores\n    \"\"\"\n\n    # Find samples with similar GE embeddings (physically similar)\n    similar_by_satellite = find_similar_samples(\n        sample.ge_embedding,\n        training_data,\n        similarity_threshold=0.8\n    )\n\n    # Check if their ENVO triads agree\n    triad_agreement = []\n    for sim_sample in similar_by_satellite:\n        envo_sim = cosine_similarity(\n            sample.envo_embedding,\n            sim_sample.envo_embedding\n        )\n        triad_agreement.append(envo_sim)\n\n    avg_agreement = mean(triad_agreement)\n\n    if avg_agreement &lt; threshold:\n        # This sample's triad disagrees with physically similar samples\n        # Propose alternative triads from the similar samples\n        alternative_triads = aggregate_triads(similar_by_satellite)\n\n        return {\n            'flagged': True,\n            'reason': f'Low ENVO agreement ({avg_agreement:.2f}) with physically similar samples',\n            'current_triad': sample.envo_triad,\n            'suggested_alternatives': alternative_triads,\n            'similar_samples': [s.accession for s in similar_by_satellite[:5]]\n        }\n\n    return {'flagged': False}\n</code></pre> <p>Use Cases: 1. Quality Control: Curators review flagged samples before publication 2. Batch Correction: Identify systematic errors (e.g., swapped broad/local scales) 3. Metadata Enrichment: Suggest triads for older samples lacking structured metadata</p>"},{"location":"research/RESEARCH_VISION/#3c-active-learning-for-metadata-improvement","title":"3C. Active Learning for Metadata Improvement","text":"<p>Goal: Prioritize which samples to manually review</p> <p>Method:</p> <pre><code>def prioritize_for_manual_review(samples):\n    \"\"\"\n    Rank samples by potential metadata impact.\n\n    Priority factors:\n    1. High GE/ENVO disagreement (likely error)\n    2. Many other samples depend on this one (high influence)\n    3. Rare environmental context (filling gaps in training data)\n    \"\"\"\n\n    scores = []\n    for sample in samples:\n        # Factor 1: Disagreement score\n        disagreement = calculate_ge_envo_disagreement(sample)\n\n        # Factor 2: Influence score (how many samples are similar to this one?)\n        influence = count_similar_samples(sample, threshold=0.8)\n\n        # Factor 3: Rarity score (is this an underrepresented environment?)\n        rarity = calculate_envo_rarity(sample.envo_triad)\n\n        # Combined priority score\n        priority = 0.5 * disagreement + 0.3 * influence + 0.2 * rarity\n        scores.append((priority, sample))\n\n    return sorted(scores, reverse=True)\n</code></pre>"},{"location":"research/RESEARCH_VISION/#concrete-next-steps","title":"Concrete Next Steps","text":""},{"location":"research/RESEARCH_VISION/#immediate-this-week","title":"Immediate (This Week)","text":"<ol> <li>Run updated notebook to determine best ENVO combination</li> <li>Add outlier detection cell to identify high-GE/low-ENVO pairs</li> <li>Analyze outlier patterns:</li> <li>Which samples appear most frequently?</li> <li>Are there systematic errors (e.g., marine/terrestrial swaps)?</li> <li>Manual inspection of top 10 outlier pairs</li> </ol>"},{"location":"research/RESEARCH_VISION/#short-term-next-sprint","title":"Short-term (Next Sprint)","text":"<ol> <li>Build triad prediction prototype:</li> <li>K-nearest neighbors by GE embedding</li> <li>Majority vote for ENVO terms</li> <li> <p>Cross-validation to measure accuracy</p> </li> <li> <p>Create suspect triad report:</p> </li> <li>List samples with low ENVO agreement among GE-similar samples</li> <li>Generate suggested alternative triads</li> <li>Export for manual review</li> </ol>"},{"location":"research/RESEARCH_VISION/#medium-term-research-paper","title":"Medium-term (Research Paper)","text":"<ol> <li>Systematic evaluation:</li> <li>Holdout test set (20% of samples)</li> <li>Measure triad prediction accuracy</li> <li> <p>Compare to baseline (random, most-frequent, etc.)</p> </li> <li> <p>Case studies:</p> </li> <li>Manual review of flagged samples</li> <li>Work with domain experts to validate predictions</li> <li> <p>Quantify metadata improvement</p> </li> <li> <p>Scale up:</p> </li> <li>Process full BioSample database (not just 246 samples)</li> <li>Build web interface for curators</li> <li>Integrate with submission pipelines</li> </ol>"},{"location":"research/RESEARCH_VISION/#example-analysis-outlier-investigation","title":"Example Analysis: Outlier Investigation","text":"<p>Notebook cell to add:</p> <pre><code>print(\"=== OUTLIER ANALYSIS: METADATA QUALITY ISSUES ===\")\n\n# Define outliers: High GE similarity but Low ENVO similarity\noutliers = pairs_filtered[\n    (pairs_filtered['ge_similarity'] &gt; 0.8) &amp;\n    (pairs_filtered['envo_best_similarity'] &lt; 0.3)  # Use best ENVO combination\n]\n\nprint(f\"\\nFound {len(outliers)} outlier pairs (high GE, low ENVO)\")\nprint(f\"  {len(outliers)/len(pairs_filtered)*100:.1f}% of all pairs\\n\")\n\n# Identify samples that appear frequently in outlier pairs\nfrom collections import Counter\n\noutlier_samples = []\nfor _, pair in outliers.iterrows():\n    outlier_samples.append(pair['accession_1'])\n    outlier_samples.append(pair['accession_2'])\n\nsample_counts = Counter(outlier_samples)\nchronic_outliers = sample_counts.most_common(10)\n\nprint(\"Samples appearing most frequently in outlier pairs:\")\nprint(\"(These may have incorrect metadata)\\n\")\nfor accession, count in chronic_outliers:\n    sample = df_clean[df_clean['accession'] == accession].iloc[0]\n    print(f\"{accession}: {count} outlier pairs\")\n    print(f\"  Location: ({sample['latitude']}, {sample['longitude']})\")\n    print(f\"  ENVO: {sample['env_broad_scale']} | {sample['env_medium']} | {sample['env_local_scale']}\")\n    print()\n\n# Show example outlier pairs\nprint(\"\\n=== EXAMPLE OUTLIER PAIRS (for manual inspection) ===\")\nfor idx, (_, pair) in enumerate(outliers.head(5).iterrows()):\n    print(f\"\\nOutlier {idx+1}:\")\n    print(f\"  GE Similarity: {pair['ge_similarity']:.3f}\")\n    print(f\"  ENVO Similarity: {pair['envo_best_similarity']:.3f}\")\n    print(f\"  \u0394 (disagreement): {pair['ge_similarity'] - pair['envo_best_similarity']:.3f}\")\n\n    sample1 = df_clean[df_clean['accession'] == pair['accession_1']].iloc[0]\n    sample2 = df_clean[df_clean['accession'] == pair['accession_2']].iloc[0]\n\n    print(f\"\\n  Sample 1: {sample1['accession']}\")\n    print(f\"    Location: ({sample1['latitude']:.4f}, {sample1['longitude']:.4f})\")\n    print(f\"    ENVO: {sample1['env_broad_scale']} | {sample1['env_medium']} | {sample1['env_local_scale']}\")\n\n    print(f\"\\n  Sample 2: {sample2['accession']}\")\n    print(f\"    Location: ({sample2['latitude']:.4f}, {sample2['longitude']:.4f})\")\n    print(f\"    ENVO: {sample2['env_broad_scale']} | {sample2['env_medium']} | {sample2['env_local_scale']}\")\n\n    print(f\"\\n  \u2192 DIAGNOSIS: Samples are physically very similar (GE={pair['ge_similarity']:.2f})\")\n    print(f\"               but have very different ENVO terms (ENVO={pair['envo_best_similarity']:.2f})\")\n    print(f\"               One of these metadata annotations is likely incorrect.\")\n</code></pre>"},{"location":"research/RESEARCH_VISION/#expected-outcomes","title":"Expected Outcomes","text":""},{"location":"research/RESEARCH_VISION/#scientific-contributions","title":"Scientific Contributions","text":"<ol> <li> <p>Quantify metadata quality: What % of BioSample metadata is inconsistent with satellite imagery?</p> </li> <li> <p>Automate quality control: Flag suspect samples before publication</p> </li> <li> <p>Metadata prediction: Provide triads for samples lacking structured annotations</p> </li> <li> <p>Training data curation: Identify high-quality samples for ML training</p> </li> </ol>"},{"location":"research/RESEARCH_VISION/#practical-impact","title":"Practical Impact","text":"<ol> <li> <p>Reduce curator burden: Automated suggestions reduce manual annotation time</p> </li> <li> <p>Improve data quality: Catch errors early in submission pipeline</p> </li> <li> <p>Enable better science: Higher quality metadata \u2192 better meta-analyses</p> </li> <li> <p>Fill metadata gaps: Millions of samples lack ENVO triads - we can predict them</p> </li> </ol>"},{"location":"research/RESEARCH_VISION/#technical-considerations","title":"Technical Considerations","text":""},{"location":"research/RESEARCH_VISION/#challenges","title":"Challenges","text":"<ol> <li>ENVO term granularity:</li> <li>\"forest\" vs \"temperate forest\" vs \"temperate coniferous forest\"</li> <li> <p>Should we compare at different ontology depths?</p> </li> <li> <p>Temporal changes:</p> </li> <li>Satellite imagery from 2018, sample collected in 2010</li> <li> <p>Land use changes (urban development, deforestation)</p> </li> <li> <p>Scale mismatch:</p> </li> <li>GE embeddings: 10m resolution</li> <li>ENVO broad_scale: biome-level (100s of km)</li> <li> <p>Which scale should agree?</p> </li> <li> <p>Training data bias:</p> </li> <li>Current 246 samples may not represent all environments</li> <li>Need larger, more diverse training set</li> </ol>"},{"location":"research/RESEARCH_VISION/#solutions","title":"Solutions","text":"<ol> <li>Hierarchical ENVO comparison:</li> <li>Compare at multiple ontology depths</li> <li> <p>Weight by specificity (more specific = higher weight)</p> </li> <li> <p>Temporal filtering:</p> </li> <li>Only use samples where collection_date \u2248 satellite_date</li> <li> <p>Or model temporal changes explicitly</p> </li> <li> <p>Multi-scale analysis:</p> </li> <li>Compare local_scale to local GE features</li> <li> <p>Compare broad_scale to regional GE patterns</p> </li> <li> <p>Active learning:</p> </li> <li>Iteratively expand training set</li> <li>Prioritize diverse environments</li> </ol>"},{"location":"research/RESEARCH_VISION/#summary-your-vision","title":"Summary: Your Vision","text":"<p>You're building a virtuous cycle for environmental metadata:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Satellite Imagery (Objective, Physical)            \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Google Earth Embeddings                            \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Compare with ENVO Embeddings (Semantic, Curated)   \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Identify Outliers (High GE / Low ENVO)             \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Flag Suspect Metadata                              \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Predict/Suggest Corrections                        \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Human Review &amp; Correction                          \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Improved Training Data                             \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Better Predictions (loop back to top)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This is quality control through cross-validation between independent data sources (satellite vs curator), using embedding similarity as the bridge.</p> <p>Brilliant research direction! \ud83d\ude80</p>"},{"location":"research/column_mapping/","title":"Column Mapping: Old TSV to New CSV","text":""},{"location":"research/column_mapping/#overview","title":"Overview","text":"<p>Mapping from <code>date_and_latlon_samples_extended_ONLY_ENVO_SAMPLE_100_with_both_embeddings.tsv</code> (old) to <code>satisfying_biosamples_normalized.csv</code> (new).</p>"},{"location":"research/column_mapping/#key-differences","title":"Key Differences","text":""},{"location":"research/column_mapping/#file-format","title":"File Format","text":"<ul> <li>Old: TSV (tab-separated)</li> <li>New: CSV (comma-separated)</li> </ul>"},{"location":"research/column_mapping/#dataset-size","title":"Dataset Size","text":"<ul> <li>Old: 99 rows</li> <li>New: 436,337 rows (much larger dataset)</li> </ul>"},{"location":"research/column_mapping/#column-mappings","title":"Column Mappings","text":""},{"location":"research/column_mapping/#date-fields","title":"Date Fields","text":"Old Column New Column Notes <code>date</code> <code>collection_date</code> Normalized date format <code>collection_date</code> <code>collection_date_raw</code> Raw date from source <p>Format: Both use ISO format (YYYY-MM-DD or YYYY-MM)</p>"},{"location":"research/column_mapping/#location-fields","title":"Location Fields","text":"Old Column New Column Notes <code>lat_lon</code> <code>lat_lon_raw</code> Combined lat/lon string (e.g., \"35.118 N 138.937 E\") <code>ncbi_lat_lon</code> <code>lat_lon_raw</code> NCBI version of lat/lon string N/A <code>latitude</code> NEW: Separate numeric latitude N/A <code>longitude</code> NEW: Separate numeric longitude <p>Key Difference: - Old file stores latitude and longitude in a single column as a formatted string (e.g., \"42.36 N 71.06 W\") - New file has both:   - <code>lat_lon_raw</code>: Original formatted string   - <code>latitude</code>: Numeric latitude (e.g., 35.118)   - <code>longitude</code>: Numeric longitude (e.g., 138.937)</p>"},{"location":"research/column_mapping/#accessionid-fields","title":"Accession/ID Fields","text":"Old Column New Column Notes <code>ncbi_biosample_accession_id</code> <code>accession</code> Biosample accession ID <code>genome_id</code> N/A MISSING in new file"},{"location":"research/column_mapping/#environmental-fields","title":"Environmental Fields","text":"Old Column New Column Notes <code>env_broad_med_local</code> <code>env_broad_scale</code>, <code>env_local_scale</code>, <code>env_medium</code> Split into 3 separate ENVO columns <p>Key Difference: - Old file: Combined environmental ontology terms in one column - New file: Separate columns for broad scale, local scale, and medium ENVO terms</p>"},{"location":"research/column_mapping/#missing-columns-in-new-file","title":"Missing Columns in New File","text":"<p>These columns exist in the old file but are NOT present in the new file: - <code>genome_id</code> - <code>ncbi_bioproject</code> - <code>domain</code>, <code>phylum</code>, <code>class</code>, <code>order</code>, <code>family</code>, <code>genus</code>, <code>species</code> (taxonomic hierarchy) - <code>geographic_location_harmonized</code> - <code>host_harmonized</code> - <code>isolation_source_harmonized</code> - <code>project_name</code> - <code>misc_attributes</code> - <code>google_earth_embeddings</code> - <code>envo_embeddings</code></p>"},{"location":"research/column_mapping/#notes-for-integration","title":"Notes for Integration","text":"<ol> <li>Embeddings will need to be generated for the new file (google_earth_embeddings and envo_embeddings)</li> <li>Latitude/longitude parsing is already done in new file (no need to parse string format)</li> <li>Genome ID missing - may need to join with another data source if genome info is required</li> <li>Environmental terms are more structured in new file (separate ENVO columns)</li> <li>Much larger dataset (436k vs 99 rows) - may impact processing time</li> </ol>"},{"location":"research/google_earth_embeddings/","title":"Google Earth Engine Embeddings","text":"<p>This document shows how to download Google Earth Engine embeddings using the methods from this codebase.</p>"},{"location":"research/google_earth_embeddings/#self-contained-code-snippet","title":"Self-Contained Code Snippet","text":"<pre><code>import ee\nfrom typing import List, Optional\n\ndef initialize_ee(project: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize Google Earth Engine authentication and session.\"\"\"\n    try:\n        ee.Authenticate()\n        if project:\n            ee.Initialize(project=project)\n        else:\n            ee.Initialize()\n    except Exception as e:\n        raise Exception(f\"Earth Engine initialization failed: {e}\")\n\ndef get_embedding(lat: float, lon: float, year: int, project: Optional[str] = None) -&gt; List[float]:\n    \"\"\"Return the 64-dimensional AlphaEarth embedding for the given lat/lon and year.\"\"\"\n    # Initialize if project provided\n    if project:\n        initialize_ee(project)\n\n    # Create point geometry\n    point = ee.Geometry.Point(lon, lat)\n\n    # Load Google's satellite embedding collection\n    collection = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL')\n\n    # Filter by year and location\n    start = f\"{year}-01-01\"\n    end = f\"{year+1}-01-01\"\n    filtered_collection = collection.filterDate(start, end).filterBounds(point)\n\n    # Get the first (and likely only) image for that year\n    image_for_year = filtered_collection.first()\n\n    # Sample the point to get embedding vector (bands A00-A63)\n    sampled = image_for_year.sample(region=point, scale=10).first()\n    band_dict = sampled.toDictionary().getInfo()\n\n    # Extract 64-dimensional embedding vector\n    embedding = [band_dict.get(f\"A{str(i).zfill(2)}\") for i in range(64)]\n\n    return embedding\n\n# Usage example:\n# initialize_ee(\"your-project-id\")\n# embedding = get_embedding(39.0372, -121.8036, 2024)\n# print(f\"64-dimensional vector: {embedding}\")\n</code></pre>"},{"location":"research/google_earth_embeddings/#key-points","title":"Key Points","text":"<ul> <li>Uses Google's <code>GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL</code> dataset</li> <li>Returns 64-dimensional AlphaEarth embeddings (bands A00-A63)</li> <li>Requires Earth Engine authentication and project setup</li> <li>Works with years 2017-2024 for best coverage</li> </ul>"},{"location":"research/google_earth_embeddings/#cli-usage","title":"CLI Usage","text":"<p>The codebase also provides a CLI interface:</p> <pre><code># Get embedding for specific coordinates and year\nuv run env-embeddings embedding --lat 39.0372 --lon -121.8036 --year 2024 --project env-embeddings-2025\n</code></pre> <p>This returns a 64-dimensional embedding vector representing satellite imagery features for that location and time.</p>"},{"location":"research/google_earth_embeddings/#setup-requirements","title":"Setup Requirements","text":"<ol> <li>Google Cloud project with Earth Engine API enabled</li> <li>Earth Engine authentication (<code>ee.Authenticate()</code>)</li> <li>Project ID for initialization</li> <li>Python package: <code>earthengine-api&gt;=1.6.8</code></li> </ol>"}]}