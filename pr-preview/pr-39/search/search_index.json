{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"env-embeddings","text":"<p>Simple experiment to compare ENVO similarity to google embedding cosine similarity </p> <ul> <li>Auto-generated schema documentation</li> </ul>"},{"location":"about/","title":"About env-embeddings","text":"<p>Simple experiment to compare ENVO similarity to google embedding cosine similarity </p>"},{"location":"analysis/FINDINGS/","title":"Analysis Findings: Satellite Imagery vs ENVO Annotations","text":""},{"location":"analysis/FINDINGS/#executive-summary","title":"Executive Summary","text":"<p>After rigorous filtering of near-duplicate pairs, the correlation between Google Earth satellite embeddings and ENVO (Environmental Ontology) annotations is weak (r = 0.167). This negative result is scientifically valuable and suggests that satellite imagery and semantic annotations capture fundamentally different aspects of environmental context.</p>"},{"location":"analysis/FINDINGS/#critical-finding-correlation-collapse","title":"\ud83d\udd34 Critical Finding: Correlation Collapse","text":""},{"location":"analysis/FINDINGS/#the-numbers","title":"The Numbers","text":"Metric Before Filtering After Filtering Change Pearson (broad_scale) 0.789 0.114 -0.675 Pearson (local_scale) 0.647 0.167 -0.480 Pearson (medium) 0.756 -0.054 -0.810"},{"location":"analysis/FINDINGS/#what-happened","title":"What Happened","text":"<p>31.7% of pairs were degenerate (similarity &gt; 0.95 on at least one metric): - 2,080 pairs with GE similarity &gt; 0.95 (20.8%) - 2,381 pairs with ENVO broad_scale &gt; 0.95 (23.8%) - 2,983 pairs with ENVO medium &gt; 0.95 (29.8%)</p> <p>These near-duplicate pairs created artificial correlation through the (1,1) anchor point problem: - Samples from same location \u2192 GE = 1.0 - Samples with same ENVO terms \u2192 ENVO = 1.0 - Many such pairs \u2192 inflates correlation coefficient</p> <p>After filtering: Only 6,826 pairs remain from 10,000 original pairs.</p>"},{"location":"analysis/FINDINGS/#why-this-matters","title":"Why This Matters","text":"<p>This demonstrates: 1. Methodological rigor is critical in similarity studies 2. Degenerate pair filtering is essential to avoid false positives 3. The original hypothesis (strong correlation) was wrong - and that's valuable science</p>"},{"location":"analysis/FINDINGS/#what-the-data-actually-shows","title":"\ud83d\udcca What the Data Actually Shows","text":""},{"location":"analysis/FINDINGS/#best-correlations-after-filtering","title":"Best Correlations (After Filtering)","text":"<p>Winner: Local scale (fine-grained environmental context) - Pearson r = 0.167 (weak linear) - Spearman \u03c1 = 0.336 (moderate rank) - Local scale = specific features like \"agricultural field\", \"coastal zone\", \"urban\"</p> <p>Runner-up: Concatenated (all three scales combined) - Pearson r = 0.161 - Spearman \u03c1 = 0.412 (highest of all methods) - 4608-dimensional vector (1536 \u00d7 3)</p>"},{"location":"analysis/FINDINGS/#key-observations","title":"Key Observations","text":"<p>1. Spearman &gt;&gt; Pearson everywhere</p> <pre><code>Local scale:  Spearman (0.336) vs Pearson (0.167)\nCombined:     Spearman (0.412) vs Pearson (0.161)\n</code></pre> <p>\u2192 Non-linear relationship between satellite and ENVO similarity</p> <p>2. Medium scale has negative Pearson correlation - Pearson r = -0.054 (statistically significant, p = 7.25e-06) - Spearman \u03c1 = 0.125 (still positive) - Medium = intermediate features like \"soil\", \"sea water\", \"sediment\" - Requires investigation: Why does linear correlation flip negative?</p> <p>3. Concatenation performs well on Spearman - Highest Spearman (0.412) but not highest Pearson (0.161) - Suggests combining scales helps for rank-based comparison - But doesn't improve linear correlation</p>"},{"location":"analysis/FINDINGS/#distribution-statistics-filtered-data","title":"Distribution Statistics (Filtered Data)","text":"<p>Google Earth similarity: - Mean: 0.203 - Std: 0.153 - Range: [-0.088, 0.950]</p> <p>ENVO similarities: | Scale | Mean | Std | Range | |-------|------|-----|-------| | Broad | 0.527 | 0.105 | [0.210, 0.950] | | Local | 0.311 | 0.080 | [0.164, 0.950] | | Medium | 0.348 | 0.062 | [0.134, 0.950] |</p> <p>Key insight: ENVO similarities are much higher and more constrained than GE similarities.</p>"},{"location":"analysis/FINDINGS/#dataset-limitations","title":"\ud83d\udea7 Dataset Limitations","text":""},{"location":"analysis/FINDINGS/#1-sample-size-too-small","title":"1. Sample Size Too Small","text":"<ul> <li>246 samples total</li> <li>6,826 valid pairs after filtering</li> <li>Maximum possible pairs: 30,135</li> <li>Only sampling 22.6% of possible space</li> </ul> <p>Consequence: May not capture full diversity of environmental contexts.</p>"},{"location":"analysis/FINDINGS/#2-geographic-clustering","title":"2. Geographic Clustering","text":"<p>Evidence from outlier analysis:</p> <pre><code>Top 10 chronic outliers:\n- 4 samples from (35.3202, 139.6500) - same location\n- 3 samples from (35.7000, 139.5000) - same location\n- Most samples from Japan (35\u00b0N, 139\u00b0E region)\n</code></pre> <p>Consequence: - Limited geographic diversity - Potential regional bias in annotations - Same-location samples inflate pair counts</p>"},{"location":"analysis/FINDINGS/#3-technical-replicates","title":"3. Technical Replicates","text":"<p>Many samples are: - Same location - Same date - Same ENVO terms - Different accessions (technical replicates, time series, depth profiles)</p> <p>Example from data:</p> <pre><code>SAMD00093579, SAMD00093580, SAMD00093581, SAMD00093583, SAMD00093585\nAll from: (35.3202, 139.6500), Date: 2017-06-13\nAll have: ENVO:01000008 | ENVO:00002123 | ENVO:01000157\n</code></pre> <p>Consequence: Creates many near-identical pairs even after degenerate filtering.</p>"},{"location":"analysis/FINDINGS/#4-limited-biome-diversity","title":"4. Limited Biome Diversity","text":"<p>Missing or underrepresented: - Marine environments - Desert biomes - Arctic/Antarctic - Agricultural diversity - Urban diversity</p> <p>Consequence: Results may not generalize to all environmental contexts.</p>"},{"location":"analysis/FINDINGS/#where-to-go-from-here","title":"\ud83c\udfaf Where to Go From Here","text":""},{"location":"analysis/FINDINGS/#option-1-scale-up-recommended-first-step","title":"Option 1: Scale Up (Recommended First Step)","text":"<p>Goal: Test if weak correlation is dataset-specific or fundamental.</p> <p>Action plan:</p> <pre><code># Process 1,000-5,000 more samples with:\n- Geographic diversity (all continents, latitudes)\n- Biome diversity (marine, terrestrial, urban, agricultural, extreme)\n- No technical replicates (deduplicate by location+date)\n- Balanced representation across ENVO terms\n</code></pre> <p>What we'll learn: - Does correlation improve with scale? - Is the dataset bias or true signal? - Which biomes/regions drive correlation?</p> <p>Expected outcome: - If r stays weak (&lt; 0.3) \u2192 correlation is real (weak) - If r improves (&gt; 0.5) \u2192 current dataset is biased - If r varies by biome \u2192 need stratified analysis</p>"},{"location":"analysis/FINDINGS/#option-2-pivot-to-classificationprediction","title":"Option 2: Pivot to Classification/Prediction","text":"<p>Reframe the question: - From: \"How correlated are they?\" (weak correlation) - To: \"Can we predict ENVO from satellite?\" (measure accuracy)</p> <p>Approach: Supervised Learning</p> <pre><code># Train a model\nX = google_earth_embeddings  # 64-dim\ny = envo_triads  # (broad, local, medium)\n\n# Multi-output classification\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n# Evaluate\naccuracy_broad = accuracy_score(y_test_broad, y_pred_broad)\naccuracy_local = accuracy_score(y_test_local, y_pred_local)\naccuracy_medium = accuracy_score(y_test_medium, y_pred_medium)\n</code></pre> <p>Why this is better: - Correlation measures linear relationship - Classification measures predictive power - Can handle non-linear patterns (we saw Spearman &gt; Pearson) - More interpretable for practical applications</p> <p>Baseline to beat: Random chance - If 10 unique broad_scale terms \u2192 random = 10% accuracy - If model achieves 40% \u2192 4\u00d7 better than random - Even 40% accuracy is useful for flagging suspect metadata</p> <p>Next level: k-NN approach - We already prototyped this in cell-19 - Find k nearest neighbors by GE similarity - Predict ENVO triad by majority vote - Measure accuracy, precision, recall</p>"},{"location":"analysis/FINDINGS/#option-3-focus-on-outliers-quality-control-tool","title":"Option 3: Focus on Outliers (Quality Control Tool)","text":"<p>Forget correlation strength, use disagreement as signal.</p> <p>High GE + Low ENVO = metadata quality issue</p> <p>Current results: - 16 outlier pairs (GE &gt; 0.8, ENVO &lt; 0.3) - 0.2% of filtered pairs - 14 unique samples involved</p> <p>Why so few outliers? - Dataset is too small - Geographic clustering means most samples are genuinely different locations - Need more data to find systematic errors</p> <p>Scaling up this approach:</p> <pre><code># With 10,000 samples\nmax_pairs = 10,000 * 9,999 / 2 = 49,995,000 pairs\n\n# Even 0.1% outliers = 49,995 suspicious pairs\n# Much better signal for quality control\n</code></pre> <p>Practical application: 1. Automated flagging: Submit new biosample \u2192 check GE vs ENVO 2. Curator review: Present top 100 disagreements per month 3. Correction suggestions: Use k-NN to suggest alternative triads 4. Training data curation: Identify high-quality samples (high agreement)</p>"},{"location":"analysis/FINDINGS/#option-4-investigate-technical-details","title":"Option 4: Investigate Technical Details","text":""},{"location":"analysis/FINDINGS/#a-why-is-medium-scale-negative","title":"A. Why is medium scale negative?","text":"<p>Observation: Pearson r = -0.054, but Spearman \u03c1 = +0.125</p> <p>Hypotheses: 1. Outliers: A few extreme pairs flip the linear trend 2. Non-monotonic relationship: Medium similarity increases then decreases with GE 3. Embedding quality: Medium ENVO terms are less well-embedded 4. Biological reality: Medium features (soil, water) vary independently of satellite view</p> <p>Investigation needed: - Scatter plot of GE vs medium similarity (look for patterns) - Stratify by medium term (is it specific terms driving negative correlation?) - Compare medium embedding quality to broad/local</p>"},{"location":"analysis/FINDINGS/#b-why-does-spearman-outperform-pearson","title":"B. Why does Spearman outperform Pearson?","text":"<p>Everywhere we see: Spearman &gt; Pearson (often 2\u00d7)</p> <p>Possible explanations: 1. Outliers: Linear correlation is sensitive, rank is robust 2. Non-linear relationship: Similarity might be exponential/logarithmic 3. Ceiling effects: ENVO similarities are bounded [0, 1], GE can be negative 4. Monotonic but not linear: Rank order is preserved but relationship curves</p> <p>Test: - Try log-transformed similarities - Try different embedding models - Look at scatter plots for curvature</p>"},{"location":"analysis/FINDINGS/#c-try-different-embeddings","title":"C. Try Different Embeddings","text":"<p>Google Earth variations:</p> <pre><code># Different zoom levels\nzoom_12 = 10m resolution (current)\nzoom_11 = 20m resolution (broader context)\nzoom_13 = 5m resolution (finer detail)\n\n# Temporal aggregates\nmedian_2015_2020 = multi-year composite (reduce seasonal variation)\nseasonal = separate summer/winter embeddings\n</code></pre> <p>ENVO variations:</p> <pre><code># Different embedding models\ntext-embedding-3-small (current, 1536-dim)\ntext-embedding-3-large (3072-dim, more expressive)\n\n# Raw term matching\nterm_overlap = Jaccard similarity on ENVO IDs\nsemantic_similarity = from ENVO ontology structure (not text)\n</code></pre>"},{"location":"analysis/FINDINGS/#option-5-write-it-up-publishable-negative-result","title":"Option 5: Write It Up (Publishable Negative Result)","text":"<p>Title: \"Weak Correlation Between Satellite Imagery and Ontological Annotations Highlights Complementary Environmental Perspectives\"</p> <p>Abstract:</p> <p>Environmental metadata curation relies on manual annotation using standardized ontologies like ENVO. We investigated whether satellite imagery could serve as an objective proxy for ENVO annotations by comparing Google Earth embeddings to ENVO text embeddings across 246 environmental samples. Initial analysis suggested strong correlation (r = 0.789), but rigorous filtering of near-duplicate pairs revealed this was an artifact. After removing degenerate pairs, correlation was weak (r = 0.167 for best-performing local scale). Spearman correlations consistently outperformed Pearson (\u03c1 = 0.336 vs r = 0.167), suggesting non-linear relationships. These findings demonstrate: (1) the critical importance of degenerate pair filtering in similarity studies, (2) satellite and semantic views capture complementary rather than redundant information, and (3) potential for satellite imagery in metadata quality control through outlier detection rather than direct correlation. We discuss dataset limitations and propose supervised learning approaches for future work.</p> <p>Contributions: 1. Methodological: Demonstrates degenerate pair problem in similarity studies 2. Negative result: Weak correlation is informative (not just absence of positive result) 3. Practical: k-NN prediction prototype for metadata suggestion 4. Dataset: 246 samples with dual embeddings (satellite + ENVO)</p> <p>Sections: 1. Introduction: Metadata quality problem in biology 2. Methods: Dual embedding approach, degenerate filtering 3. Results: Correlation collapse after filtering 4. Discussion: Why weak correlation matters, alternative approaches 5. Conclusion: Complementary perspectives, future directions</p>"},{"location":"analysis/FINDINGS/#recommended-path-forward","title":"\ud83d\udca1 Recommended Path Forward","text":""},{"location":"analysis/FINDINGS/#phase-1-validate-with-more-data-2-4-weeks","title":"Phase 1: Validate with More Data (2-4 weeks)","text":"<p>Objective: Is this dataset-specific or fundamental?</p> <p>Tasks: 1. Process 1,000 more samples (diverse geography/biomes)    - Note: Pipeline now uses random sampling instead of sequential to avoid bias    - <code>df.sample(n=max_rows, random_state=42)</code> ensures representative selection 2. Re-run correlation analysis with filtering 3. Compare results to current dataset</p> <p>Decision point: - If r &gt; 0.5 \u2192 dataset bias, continue scaling - If r &lt; 0.3 \u2192 weak correlation is real, pivot to classification</p>"},{"location":"analysis/FINDINGS/#phase-2-build-prediction-model-2-4-weeks","title":"Phase 2: Build Prediction Model (2-4 weeks)","text":"<p>Objective: Measure predictive power, not correlation</p> <p>Tasks: 1. Implement k-NN classifier (already prototyped) 2. Baseline: Random Forest, XGBoost 3. Evaluate accuracy, precision, recall per scale 4. Cross-validation (geographic stratification)</p> <p>Success metric: Accuracy &gt; 2\u00d7 random chance</p>"},{"location":"analysis/FINDINGS/#phase-3-quality-control-tool-4-8-weeks","title":"Phase 3: Quality Control Tool (4-8 weeks)","text":"<p>Objective: Practical application for metadata curation</p> <p>Tasks: 1. Outlier detection pipeline (GE vs ENVO disagreement) 2. Web interface for curator review 3. Automated suggestion system (k-NN predictions) 4. Integration with BioSample submission workflow</p> <p>Deliverable: Live tool for NCBI/EBI metadata curation</p>"},{"location":"analysis/FINDINGS/#phase-4-publication-4-8-weeks","title":"Phase 4: Publication (4-8 weeks)","text":"<p>Objective: Share findings with community</p> <p>Tasks: 1. Write manuscript (methods, results, discussion) 2. Create supplementary materials (notebook, data, code) 3. Submit to bioinformatics journal (e.g., Bioinformatics, NAR) 4. Preprint on bioRxiv</p> <p>Impact: Influence metadata standards and curation practices</p>"},{"location":"analysis/FINDINGS/#technical-considerations","title":"\ud83d\udd2c Technical Considerations","text":""},{"location":"analysis/FINDINGS/#statistical-power","title":"Statistical Power","text":"<p>Current: - 246 samples - 6,826 pairs (after filtering) - Power to detect r &gt; 0.3 at \u03b1 = 0.05</p> <p>Needed for r = 0.2 detection: - ~800 samples - ~320,000 pairs - Better for weak correlation studies</p>"},{"location":"analysis/FINDINGS/#computational-cost","title":"Computational Cost","text":"<p>Current pipeline: - Google Earth API: ~2 seconds per sample - ENVO embeddings: cached (instant for duplicates) - Pairwise similarities: O(n\u00b2) for n samples</p> <p>Scaling to 10,000 samples: - Embeddings: ~5 hours (Google Earth API limited) - Similarities: ~10 minutes (embarrassingly parallel) - Storage: ~500 MB (embeddings + metadata)</p> <p>Bottleneck: Google Earth Engine API rate limits</p>"},{"location":"analysis/FINDINGS/#geographic-stratification","title":"Geographic Stratification","text":"<p>For unbiased sampling:</p> <pre><code># Stratify by major biomes\nbiomes = {\n    'terrestrial': 0.4,  # 40% of samples\n    'marine': 0.3,       # 30%\n    'freshwater': 0.15,  # 15%\n    'urban': 0.10,       # 10%\n    'extreme': 0.05      # 5%\n}\n\n# Stratify by latitude\nlatitude_bins = {\n    'arctic': 66-90\u00b0,\n    'temperate_north': 23-66\u00b0,\n    'tropical': -23-23\u00b0,\n    'temperate_south': -66--23\u00b0,\n    'antarctic': -90--66\u00b0\n}\n</code></pre>"},{"location":"analysis/FINDINGS/#validation-strategy","title":"Validation Strategy","text":"<p>Cross-validation approaches: 1. K-fold: Random split (ignores geography) 2. Geographic CV: Hold out regions (tests generalization) 3. Biome CV: Hold out biome types (tests diversity) 4. Temporal CV: Hold out years (tests temporal stability)</p> <p>Recommended: Geographic CV to avoid spatial autocorrelation</p>"},{"location":"analysis/FINDINGS/#related-work","title":"\ud83d\udcda Related Work","text":""},{"location":"analysis/FINDINGS/#similar-studies","title":"Similar Studies","text":"<p>Satellite + Metadata: - iNaturalist species distribution models - GBIF occurrence data validation - Land cover classification from satellite</p> <p>Embedding Similarity: - Sentence embeddings for semantic similarity - Image embeddings for reverse image search - Cross-modal embedding alignment (vision + language)</p>"},{"location":"analysis/FINDINGS/#gaps-this-work-fills","title":"Gaps This Work Fills","text":"<ol> <li>Environmental metadata: First to compare satellite vs ontology embeddings</li> <li>Degenerate filtering: Demonstrates methodological pitfall</li> <li>Negative result: Published weak correlations are rare but valuable</li> <li>Practical tool: k-NN prediction for metadata curation</li> </ol>"},{"location":"analysis/FINDINGS/#lessons-learned","title":"\ud83c\udf93 Lessons Learned","text":""},{"location":"analysis/FINDINGS/#methodological","title":"Methodological","text":"<ol> <li>Always filter degenerate pairs in similarity studies</li> <li>Spearman before Pearson when relationship is unknown</li> <li>Report effect sizes (r values), not just p-values</li> <li>Visualize before correlating (scatter plots reveal non-linearity)</li> </ol>"},{"location":"analysis/FINDINGS/#scientific","title":"Scientific","text":"<ol> <li>Negative results are valuable when rigorously tested</li> <li>Weak correlation \u2260 no information (still useful for classification)</li> <li>Complementary is better than redundant (different perspectives matter)</li> </ol>"},{"location":"analysis/FINDINGS/#practical","title":"Practical","text":"<ol> <li>Start small, scale deliberately (246 \u2192 1,000 \u2192 10,000)</li> <li>Build infrastructure first (caching, filtering, reproducibility)</li> <li>Document as you go (notebook + markdown files)</li> </ol>"},{"location":"analysis/FINDINGS/#next-actions","title":"\ud83d\ude80 Next Actions","text":""},{"location":"analysis/FINDINGS/#immediate-this-week","title":"Immediate (This Week)","text":"<ul> <li>[ ] Review findings with collaborators</li> <li>[ ] Decide on direction (scale up vs pivot vs publish)</li> <li>[ ] Plan next dataset (if scaling up)</li> </ul>"},{"location":"analysis/FINDINGS/#short-term-this-month","title":"Short-term (This Month)","text":"<ul> <li>[ ] Process 500-1,000 more samples (if scaling)</li> <li>[ ] Implement classification model (if pivoting)</li> <li>[ ] Draft introduction (if publishing)</li> </ul>"},{"location":"analysis/FINDINGS/#long-term-next-quarter","title":"Long-term (Next Quarter)","text":"<ul> <li>[ ] Complete analysis on larger dataset</li> <li>[ ] Build quality control prototype</li> <li>[ ] Submit manuscript</li> </ul>"},{"location":"analysis/FINDINGS/#final-thoughts","title":"\ud83d\udcdd Final Thoughts","text":"<p>This analysis demonstrates the value of methodological rigor and the importance of negative results in science. The weak correlation between satellite imagery and ENVO annotations is not a failure\u2014it's a discovery that these two perspectives capture complementary aspects of environmental context.</p> <p>The satellite sees: Physical landscape, land cover, terrain ENVO captures: Biological context, ecosystem type, functional role</p> <p>Together, not separately, they provide comprehensive environmental metadata.</p> <p>The path forward depends on your goals: - Basic science: Understand why correlation is weak (scale up, investigate) - Applied tool: Build prediction/validation system (pivot to classification) - Community impact: Publish findings, influence metadata standards</p> <p>All three paths are valuable. The infrastructure you've built supports any direction.</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/","title":"Scientific Analysis: Methodological Concerns in similarity_analysis.ipynb","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#summary-of-critical-issues","title":"Summary of Critical Issues","text":"<p>The current notebook shows high correlation coefficients (r=0.811 for broad_scale), but several methodological concerns suggest these results may be inflated or unreliable:</p> <ol> <li>Self-pairs inflation (most critical)</li> <li>Sample size adequacy</li> <li>Sampling methodology</li> <li>Statistical independence</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#1-self-pairs-problem-inflated-correlations-from-identical-samples","title":"1. Self-Pairs Problem: Inflated Correlations from Identical Samples","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#the-issue","title":"The Issue","text":"<p>Looking at the output from cell-15 and cell-17, we see many pairs with similarity = 1.0 for BOTH Google Earth AND all ENVO types:</p> <pre><code>High similarity in both (GE &gt; 0.9, ENVO broad_scale &gt; 0.9): 212 pairs\n</code></pre> <p>Out of 1000 pairs, 212 have near-perfect agreement (&gt;0.9 on both metrics).</p> <p>From cell-17 output, we can see why:</p> <pre><code>Pair: SAMD00115491 vs SAMD00115487\nGE Similarity: 1.000\nENVO broad_scale: 1.000\nSample 1 - Lat/Lon: (35.7128, 139.7619), Date: 2018-02-11\nSample 2 - Lat/Lon: (35.7128, 139.7619), Date: 2018-02-11\nSample 1 ENVO: ENVO:00002030 | ENVO:00002006 | ENVO:00000241\nSample 2 ENVO: ENVO:00002030 | ENVO:00002006 | ENVO:00000241\n</code></pre> <p>These are essentially the same sample! Same location, same date, same ENVO terms \u2192 1.0 similarity on both axes.</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#why-this-inflates-correlation","title":"Why This Inflates Correlation","text":"<p>When you have many (1,1) points in a correlation analysis: - Pearson correlation is extremely sensitive to these perfect-agreement points - They create an artificial \"anchor\" at (1,1) that pulls the correlation line upward - This can make a weak/moderate correlation appear strong</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#evidence-in-the-data","title":"Evidence in the Data","text":"<p>From the current run with 246 samples: - 212/1000 pairs (21%) have GE &gt; 0.9 AND ENVO broad &gt; 0.9 - Many of these are likely self-pairs (same location, same ENVO terms) - The dataset has many samples from same locations (e.g., (35.7128, 139.7619) appears multiple times)</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#the-solution","title":"The Solution","text":"<p>Exclude self-pairs and near-duplicates:</p> <ol> <li>Remove identical samples: Don't compare samples with identical coordinates + ENVO terms</li> <li>Apply similarity threshold: Exclude pairs where EITHER metric &gt; 0.95 (likely duplicates)</li> <li>Re-calculate correlations on the cleaned dataset</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#2-sample-size-is-1000-pairs-enough","title":"2. Sample Size: Is 1000 Pairs Enough?","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#current-situation","title":"Current Situation","text":"<ul> <li>246 samples total</li> <li>1000 random pairs generated</li> <li>Maximum possible unique pairs: C(246, 2) = 30,135 pairs</li> <li>Current sampling: 1000/30,135 = 3.3% of all possible pairs</li> </ul>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#guidelines-for-pair-sampling","title":"Guidelines for Pair Sampling","text":"<p>From statistical literature on correlation analysis:</p> <p>Power Analysis for Correlation: - To detect r=0.8 with 80% power and \u03b1=0.05: n \u2265 12 pairs - To detect r=0.5 with 80% power: n \u2265 29 pairs - To detect r=0.3 with 80% power: n \u2265 84 pairs</p> <p>So 1000 pairs is MORE than sufficient for statistical power.</p> <p>But...</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#the-real-question-representative-sampling","title":"The Real Question: Representative Sampling","text":"<p>With 246 samples and 1000 pairs: - Average sample appears in: 1000 \u00d7 2 / 246 \u2248 8 pairs - Some samples may appear 0 times, others 20+ times (random variation) - This creates dependency between pairs (not truly independent observations)</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#recommendations","title":"Recommendations","text":"<p>Option 1: Sample ALL pairs (computational cost) - 30,135 pairs is not computationally prohibitive - Ensures complete coverage - Removes sampling bias</p> <p>Option 2: Stratified random sampling - Ensure each sample appears in at least N pairs (e.g., 5-10) - Better coverage than pure random sampling - Still computationally efficient</p> <p>Option 3: Increase to 5000-10000 pairs - Covers ~17-33% of all possible pairs - More stable estimates - Since computation is fast (you mentioned \"doesn't take too much time\"), this is easy to do</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#3-sampling-methodology-random-pairs-with-replacement","title":"3. Sampling Methodology: Random Pairs with Replacement","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#current-implementation-cell-8","title":"Current Implementation (Cell 8)","text":"<pre><code>n_pairs = 1000\nfor i in range(n_pairs):\n    idx1, idx2 = random.sample(range(n_samples), 2)\n    pairs.append((idx1, idx2))\n</code></pre> <p>This is sampling WITHOUT replacement within each pair (idx1 \u2260 idx2), but WITH replacement across pairs (same pair can be selected multiple times).</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#problems","title":"Problems","text":"<ol> <li>Duplicate pairs: (A, B) could be selected multiple times</li> <li>Asymmetric pairs: Both (A, B) and (B, A) could be selected (though similarity is symmetric, this wastes samples)</li> <li>Non-uniform coverage: Some pairs over-sampled, others never sampled</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#better-approach","title":"Better Approach","text":"<p>For small datasets (like 246 samples):</p> <pre><code># Generate ALL unique pairs\nfrom itertools import combinations\nall_pairs = list(combinations(range(n_samples), 2))\n\n# Optional: Sample from all_pairs if needed\nif len(all_pairs) &gt; max_pairs:\n    pairs = random.sample(all_pairs, max_pairs)\nelse:\n    pairs = all_pairs\n</code></pre> <p>Advantages: - No duplicate pairs - Guaranteed unique coverage - More statistically rigorous</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#4-dependency-between-pairs","title":"4. Dependency Between Pairs","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#the-problem","title":"The Problem","text":"<p>Pairs are not independent observations when samples are reused:</p> <p>Example: - Pair 1: (A, B) \u2192 similarity = 0.8 - Pair 2: (A, C) \u2192 similarity = 0.6 - Pair 3: (B, C) \u2192 similarity = 0.7</p> <p>All three pairs share samples, so they're not independent. This violates the independence assumption of correlation tests (Pearson, Spearman).</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#impact-on-p-values","title":"Impact on p-values","text":"<p>The reported p-values (e.g., p=6.81e-235) assume independent observations. With dependent pairs: - p-values are too small (overconfident) - Standard errors are underestimated - Confidence intervals are too narrow</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#correction-approaches","title":"Correction Approaches","text":"<p>Option 1: Permutation testing - Shuffle one embedding type, recalculate correlation, repeat 10,000 times - Empirical p-value from permutation distribution - Doesn't assume independence</p> <p>Option 2: Bootstrap confidence intervals - Resample samples (not pairs), generate new pairs, calculate correlation - Repeat 10,000 times - 95% CI from bootstrap distribution</p> <p>Option 3: Report effect size only - Don't rely on p-values - Focus on correlation magnitude and practical significance</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#5-proposed-sensitivity-analysis","title":"5. Proposed Sensitivity Analysis","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#test-1-exclude-high-similarity-pairs","title":"Test 1: Exclude High-Similarity Pairs","text":"<p>Remove pairs where either GE or ENVO similarity &gt; 0.95:</p> <pre><code># Exclude likely duplicates\npairs_filtered = pairs_df[\n    (pairs_df['ge_similarity'] &lt; 0.95) &amp;\n    (pairs_df['envo_broad_similarity'] &lt; 0.95)\n]\n\n# Recalculate correlation\npearson_filtered, p_filtered = pearsonr(\n    pairs_filtered['ge_similarity'],\n    pairs_filtered['envo_broad_similarity']\n)\n</code></pre> <p>Expected outcome: - If correlation drops significantly (e.g., 0.811 \u2192 0.4), the result is driven by duplicates - If correlation stays strong (0.811 \u2192 0.7+), the relationship is robust</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#test-2-exclude-perfect-matches","title":"Test 2: Exclude Perfect Matches","text":"<p>Remove pairs where both GE AND ENVO similarity = 1.0:</p> <pre><code>pairs_no_perfect = pairs_df[\n    ~((pairs_df['ge_similarity'] == 1.0) &amp;\n      (pairs_df['envo_broad_similarity'] == 1.0))\n]\n</code></pre>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#test-3-increase-sample-size","title":"Test 3: Increase Sample Size","text":"<p>Run with 10,000 pairs or ALL possible pairs:</p> <pre><code># Generate all unique pairs\nfrom itertools import combinations\nall_pairs = list(combinations(range(len(df_clean)), 2))\nprint(f\"Total unique pairs: {len(all_pairs)}\")\n</code></pre> <p>Check if correlation coefficient stabilizes or changes.</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#test-4-stratified-sampling","title":"Test 4: Stratified Sampling","text":"<p>Ensure diverse pairs by sampling across environmental categories:</p> <pre><code># Group by ENVO broad_scale term\ngroups = df_clean.groupby('env_broad_scale')\n\n# Sample pairs within and across groups\nwithin_group_pairs = []  # Same ENVO term\nacross_group_pairs = []  # Different ENVO terms\n</code></pre>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#6-additional-statistical-concerns","title":"6. Additional Statistical Concerns","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#multiple-testing","title":"Multiple Testing","text":"<p>The notebook tests 3 ENVO types (broad, medium, local) against GE. - With no correction, we expect 1/20 false positives at \u03b1=0.05 - Should apply Bonferroni correction: \u03b1_corrected = 0.05/3 = 0.0167 - Or report False Discovery Rate (FDR) adjusted p-values</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#distribution-of-similarities","title":"Distribution of Similarities","text":"<p>From cell-11 output:</p> <pre><code>Google Earth Embeddings:\n  Mean: 0.381\n  Range: [-0.088, 1.000]\n\nENVO broad_scale:\n  Mean: 0.636\n  Range: [0.210, 1.000]\n</code></pre> <p>GE similarities can be negative (cosine similarity range: [-1, 1]) ENVO similarities are all positive (OpenAI embeddings are typically [0, 1])</p> <p>This asymmetry might affect correlation: - Pearson correlation assumes linear relationship - If relationship is non-linear, Pearson may be misleading - Spearman (rank-based) is more robust</p> <p>Observation: Spearman is consistently lower (0.677 vs 0.811 for broad_scale) - Suggests relationship may not be perfectly linear - Or there are outliers influencing Pearson</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#7-recommendations","title":"7. Recommendations","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#immediate-actions-easy-wins","title":"Immediate Actions (Easy Wins)","text":"<ol> <li>Add sensitivity analysis cell:</li> <li>Exclude pairs with similarity &gt; 0.95 on either metric</li> <li>Exclude perfect (1.0, 1.0) pairs</li> <li> <p>Report correlations before/after filtering</p> </li> <li> <p>Increase to ALL pairs or 10,000 pairs:</p> </li> <li>You said computation is fast</li> <li>This is scientifically more rigorous</li> <li> <p>30,135 pairs is not computationally expensive</p> </li> <li> <p>Add robustness checks:</p> </li> <li>Bootstrap confidence intervals (1000 iterations)</li> <li>Permutation test for p-values</li> <li>Report effect sizes with CIs, not just p-values</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#medium-term-improvements","title":"Medium-Term Improvements","text":"<ol> <li>Stratified analysis:</li> <li>Separate correlations by ENVO term frequency</li> <li> <p>Do rare ENVO terms show different patterns?</p> </li> <li> <p>Visualize outliers:</p> </li> <li>Identify pairs driving the correlation</li> <li> <p>Are high-correlation points real or duplicates?</p> </li> <li> <p>Multiple testing correction:</p> </li> <li>Bonferroni or FDR correction for 3 ENVO types</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#long-term-requires-more-data","title":"Long-Term (Requires More Data)","text":"<ol> <li>Process more samples:</li> <li>Current: 246 samples</li> <li>You mentioned \"we can easily provide more now\"</li> <li> <p>Target: 1000+ samples for robust analysis</p> </li> <li> <p>Cross-validation:</p> </li> <li>Split data into train/test sets</li> <li>Check if correlation generalizes</li> </ol>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#8-concrete-next-steps","title":"8. Concrete Next Steps","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#create-new-notebook-cell-sensitivity-analysis","title":"Create New Notebook Cell: Sensitivity Analysis","text":"<pre><code>print(\"=== SENSITIVITY ANALYSIS ===\")\n\n# Original correlation\nprint(f\"\\nOriginal (all {len(pairs_df)} pairs):\")\nprint(f\"  Pearson r = {pearson_broad:.3f}\")\n\n# Test 1: Remove high-similarity pairs\npairs_filtered = pairs_df[\n    (pairs_df['ge_similarity'] &lt; 0.95) &amp;\n    (pairs_df['envo_broad_similarity'] &lt; 0.95)\n]\nr_filtered, p_filtered = pearsonr(\n    pairs_filtered['ge_similarity'],\n    pairs_filtered['envo_broad_similarity']\n)\nprint(f\"\\nAfter removing similarity &gt; 0.95 ({len(pairs_filtered)} pairs):\")\nprint(f\"  Pearson r = {r_filtered:.3f}\")\nprint(f\"  Change: {r_filtered - pearson_broad:.3f}\")\n\n# Test 2: Remove perfect matches\npairs_no_perfect = pairs_df[\n    ~((pairs_df['ge_similarity'] == 1.0) &amp;\n      (pairs_df['envo_broad_similarity'] == 1.0))\n]\nr_no_perfect, p_no_perfect = pearsonr(\n    pairs_no_perfect['ge_similarity'],\n    pairs_no_perfect['envo_broad_similarity']\n)\nprint(f\"\\nAfter removing (1.0, 1.0) pairs ({len(pairs_no_perfect)} pairs):\")\nprint(f\"  Pearson r = {r_no_perfect:.3f}\")\nprint(f\"  Change: {r_no_perfect - pearson_broad:.3f}\")\n\n# Test 3: Bootstrap confidence intervals\nfrom scipy.stats import bootstrap\ndata = (pairs_df['ge_similarity'].values, pairs_df['envo_broad_similarity'].values)\ndef correlation_statistic(x, y):\n    return pearsonr(x, y)[0]\n\n# This requires scipy &gt;= 1.7\n# res = bootstrap(data, correlation_statistic, n_resamples=1000, method='percentile')\n# print(f\"\\n95% Bootstrap CI: [{res.confidence_interval.low:.3f}, {res.confidence_interval.high:.3f}]\")\n</code></pre>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#modify-cell-8-sample-all-pairs","title":"Modify Cell 8: Sample ALL Pairs","text":"<pre><code>from itertools import combinations\n\n# Generate ALL unique pairs instead of random sample\nprint(f\"Generating all unique pairs from {n_samples} samples...\")\nall_possible_pairs = list(combinations(range(n_samples), 2))\nprint(f\"Total unique pairs: {len(all_possible_pairs)}\")\n\n# Use all pairs (computation is fast)\npairs = all_possible_pairs\n\n# Or limit if needed (but 30K pairs should be fast)\n# max_pairs = 10000\n# if len(all_possible_pairs) &gt; max_pairs:\n#     pairs = random.sample(all_possible_pairs, max_pairs)\n# else:\n#     pairs = all_possible_pairs\n</code></pre>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#9-expected-outcomes","title":"9. Expected Outcomes","text":""},{"location":"analysis/SCIENTIFIC_ANALYSIS/#if-correlation-remains-strong-after-filtering-r-07","title":"If correlation remains strong after filtering (r &gt; 0.7):","text":"<p>\u2705 The relationship is real and robust \u2705 Geographic and ontological similarities truly correlate \u2705 Results are publishable with proper caveats</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#if-correlation-drops-substantially-r-05","title":"If correlation drops substantially (r &lt; 0.5):","text":"<p>\u274c Original result was driven by duplicates/self-pairs \u274c Need to investigate why so many near-identical samples \u274c May need to deduplicate dataset before analysis</p>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#most-likely-scenario","title":"Most Likely Scenario:","text":"<ul> <li>Correlation drops moderately (0.811 \u2192 0.6-0.7)</li> <li>Still significant and meaningful</li> <li>More honest assessment of relationship strength</li> </ul>"},{"location":"analysis/SCIENTIFIC_ANALYSIS/#10-summary","title":"10. Summary","text":"<p>Critical Flaw: 21% of pairs are near-perfect matches (&gt;0.9, &gt;0.9), likely inflating correlation</p> <p>Quick Fix: 1. Filter out high-similarity pairs (&gt;0.95) 2. Use ALL 30,135 pairs instead of 1000 random pairs 3. Report correlation with and without filtering</p> <p>Scientific Rigor: - Current p-values are likely too optimistic (dependency issue) - Need bootstrap CIs or permutation tests - Should report effect sizes, not just significance</p> <p>Data Size: - 1000 pairs is statistically sufficient - But ALL pairs is better (more stable, no sampling bias) - Processing more samples (1000+ vs current 246) would strengthen conclusions</p> <p>Bottom Line: The high correlation (r=0.811) is suspicious until we verify it's not driven by duplicate/near-duplicate samples. The sensitivity analysis will reveal the truth.</p>"},{"location":"development/CI_SETUP/","title":"CI/CD and Quality Assurance Setup","text":"<p>This document describes the comprehensive CI/CD and code quality setup for the env-embeddings project.</p>"},{"location":"development/CI_SETUP/#overview","title":"Overview","text":"<p>The project now has a complete quality assurance pipeline that runs: - Tests with coverage reporting - Type checking with mypy - Linting with ruff - Dependency checking with deptry</p> <p>These checks run: 1. Locally via <code>just test</code> 2. On commit via pre-commit hooks 3. On push via pre-push hooks 4. On CI via GitHub Actions</p>"},{"location":"development/CI_SETUP/#quick-start","title":"Quick Start","text":""},{"location":"development/CI_SETUP/#install-pre-commit-hooks","title":"Install Pre-commit Hooks","text":"<pre><code># Install pre-commit tool (if not already installed)\nuv tool install pre-commit\n\n# Install the hooks\npre-commit install\npre-commit install --hook-type pre-push\n</code></pre>"},{"location":"development/CI_SETUP/#run-all-quality-checks","title":"Run All Quality Checks","text":"<pre><code>just test\n</code></pre> <p>This runs: 1. <code>pytest</code> with coverage (minimum 50% coverage) 2. <code>mypy</code> for type checking 3. <code>ruff</code> for linting 4. <code>deptry</code> for dependency analysis</p>"},{"location":"development/CI_SETUP/#test-commands","title":"Test Commands","text":""},{"location":"development/CI_SETUP/#core-commands","title":"Core Commands","text":"Command Description <code>just test</code> Run all quality checks (pytest-cov + mypy + ruff + deptry) <code>just pytest-cov</code> Run tests with coverage and timing <code>just pytest</code> Run tests only (no coverage) <code>just mypy</code> Run type checking <code>just ruff</code> Run linting <code>just deptry</code> Check for unused/missing dependencies"},{"location":"development/CI_SETUP/#advanced-commands","title":"Advanced Commands","text":"Command Description <code>just test-full</code> Run all checks including integration tests <code>just pytest-integration</code> Run integration tests <code>just doctest</code> Run doctests in src/"},{"location":"development/CI_SETUP/#test-coverage","title":"Test Coverage","text":"<p>Current coverage: 9% (baseline from existing tests)</p> <p>Coverage reports: - Terminal: Shows missing lines after test run - HTML: Generated in <code>htmlcov/</code> directory - Minimum: 50% coverage required for pre-push</p> <p>View HTML coverage:</p> <pre><code>open htmlcov/index.html\n</code></pre>"},{"location":"development/CI_SETUP/#pre-commit-hooks","title":"Pre-commit Hooks","text":""},{"location":"development/CI_SETUP/#installed-hooks","title":"Installed Hooks","text":"<p>On every commit: - <code>check-toml</code> - Validate TOML files - <code>check-yaml</code> - Validate YAML files - <code>end-of-file-fixer</code> - Ensure files end with newline - <code>trailing-whitespace</code> - Remove trailing whitespace - <code>yamllint</code> - Lint YAML files - <code>codespell</code> - Spell checking - <code>typos</code> - Typo detection - <code>ruff</code> - Linting with auto-fix - <code>ruff-format</code> - Code formatting - <code>uv-lock</code> - Keep uv.lock in sync - <code>mypy</code> - Type checking - <code>pytest</code> - Run tests</p> <p>On push: - <code>pytest-cov</code> - Run tests with 50% minimum coverage</p>"},{"location":"development/CI_SETUP/#manual-pre-commit-run","title":"Manual Pre-commit Run","text":"<pre><code># Run on all files\npre-commit run --all-files\n\n# Run specific hook\npre-commit run mypy --all-files\n\n# Skip hooks for a commit (not recommended)\ngit commit --no-verify\n</code></pre>"},{"location":"development/CI_SETUP/#github-actions","title":"GitHub Actions","text":"<p>The CI pipeline runs on: - Push to main - Pull requests</p> <p>Workflow: <code>.github/workflows/main.yaml</code></p> <p>Matrix testing across Python versions: - 3.10 - 3.11 - 3.12 - 3.13</p>"},{"location":"development/CI_SETUP/#configuration-files","title":"Configuration Files","text":""},{"location":"development/CI_SETUP/#pytest-pytestini-or-pyprojecttoml","title":"pytest (<code>pytest.ini</code> or <code>pyproject.toml</code>)","text":"<pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\n</code></pre>"},{"location":"development/CI_SETUP/#mypy-mypyini","title":"mypy (<code>mypy.ini</code>)","text":"<p>Key settings: - Type checking for <code>src/</code> and <code>tests/</code> - Ignores missing imports for external libraries without type stubs:   - pandas, pytest, typer, diskcache, tqdm, ols_client, global_land_mask, ee</p>"},{"location":"development/CI_SETUP/#ruff-pyprojecttoml","title":"ruff (<code>pyproject.toml</code>)","text":"<ul> <li>Modern, fast Python linter</li> <li>Replaces flake8, isort, and more</li> <li>Auto-fixes many issues</li> </ul>"},{"location":"development/CI_SETUP/#deptry-pyprojecttoml","title":"deptry (<code>pyproject.toml</code>)","text":"<pre><code>[tool.deptry]\nper_rule_ignores = {\n  DEP002 = [\"linkml-runtime\", \"numpy\", \"requests-cache\", \"global-land-mask\"],\n  DEP003 = [\"typing_extensions\"]\n}\n</code></pre>"},{"location":"development/CI_SETUP/#dependencies","title":"Dependencies","text":""},{"location":"development/CI_SETUP/#development-dependencies","title":"Development Dependencies","text":"<p>All testing tools are installed as dev dependencies:</p> <pre><code>uv sync --group dev\n</code></pre> <p>Installed tools: - <code>pytest</code> - Testing framework - <code>pytest-cov</code> - Coverage plugin - <code>mypy</code> - Type checker - <code>ruff</code> - Linter/formatter - <code>deptry</code> - Dependency checker</p>"},{"location":"development/CI_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/CI_SETUP/#failed-to-spawn-pytest","title":"\"Failed to spawn: pytest\"","text":"<p>Problem: Virtual environment may be corrupt or pointing to wrong path</p> <p>Solution:</p> <pre><code>rm -rf .venv\nuv sync --group dev\n</code></pre>"},{"location":"development/CI_SETUP/#mypy-cant-find-type-stubs","title":"Mypy can't find type stubs","text":"<p>Problem: External libraries missing type information</p> <p>Solution: Add to <code>mypy.ini</code>:</p> <pre><code>[mypy-package_name.*]\nignore_missing_imports = True\n</code></pre>"},{"location":"development/CI_SETUP/#deptry-false-positives","title":"Deptry false positives","text":"<p>Problem: Legitimate dependencies flagged as unused</p> <p>Solution: Add to <code>pyproject.toml</code>:</p> <pre><code>[tool.deptry]\nper_rule_ignores = {DEP002 = [\"package_name\"]}\n</code></pre>"},{"location":"development/CI_SETUP/#coverage-too-low","title":"Coverage too low","text":"<p>Problem: Tests don't cover enough code</p> <p>Solution: Write more tests or adjust minimum:</p> <pre><code># Adjust minimum in .pre-commit-config.yaml\nentry: uv run pytest --cov=src/env_embeddings --cov-fail-under=40\n</code></pre>"},{"location":"development/CI_SETUP/#best-practices","title":"Best Practices","text":""},{"location":"development/CI_SETUP/#1-run-tests-before-committing","title":"1. Run tests before committing","text":"<pre><code>just test\n</code></pre>"},{"location":"development/CI_SETUP/#2-let-pre-commit-auto-fix-issues","title":"2. Let pre-commit auto-fix issues","text":"<p>Pre-commit will auto-fix many issues (trailing whitespace, formatting, etc.)</p>"},{"location":"development/CI_SETUP/#3-dont-skip-hooks","title":"3. Don't skip hooks","text":"<p>Only use <code>--no-verify</code> in emergencies</p>"},{"location":"development/CI_SETUP/#4-keep-coverage-high","title":"4. Keep coverage high","text":"<p>Aim for &gt;80% coverage on new code</p>"},{"location":"development/CI_SETUP/#5-fix-type-errors","title":"5. Fix type errors","text":"<p>Don't just add <code># type: ignore</code> - fix the actual issue</p>"},{"location":"development/CI_SETUP/#6-update-dependencies-regularly","title":"6. Update dependencies regularly","text":"<pre><code>uv lock --upgrade\n</code></pre>"},{"location":"development/CI_SETUP/#example-workflow","title":"Example Workflow","text":"<pre><code># Make changes to code\nvim src/env_embeddings/earth_engine.py\n\n# Run tests locally\njust test\n\n# Tests pass! Stage changes\ngit add src/env_embeddings/earth_engine.py\n\n# Commit (pre-commit hooks run automatically)\ngit commit -m \"Add retry logic to earth engine\"\n\n# Push (pre-push hooks run automatically including coverage check)\ngit push origin feature-branch\n\n# GitHub Actions runs on the PR\n# All checks must pass before merge\n</code></pre>"},{"location":"development/CI_SETUP/#summary","title":"Summary","text":"<p>\u2705 Complete test suite with coverage reporting \u2705 Type safety with mypy \u2705 Code quality with ruff \u2705 Dependency health with deptry \u2705 Pre-commit hooks for immediate feedback \u2705 CI/CD with GitHub Actions \u2705 No workarounds - clean <code>uv run</code> commands</p> <p>The setup ensures high code quality without being overly restrictive. All checks are fast and provide clear error messages.</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/","title":"Performance Optimizations","text":""},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#problem-slow-google-earth-engine-failures","title":"Problem: Slow Google Earth Engine Failures","text":"<p>When processing large datasets, failures were taking a long time because: 1. Each failure required a network request to Google Earth Engine 2. Fallback year attempts doubled the network calls 3. Repeated failures for the same location/year kept making the same slow requests</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#solution-cached-failures-verbose-logging","title":"Solution: Cached Failures + Verbose Logging","text":""},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#1-cache-failures-fast-fail","title":"1. Cache Failures (Fast-Fail)","text":"<p>Before: - Ocean location at (34.24, 144.31, 2010) \u2192 slow API call \u2192 failure - Same location at (34.24, 144.31, 2010) \u2192 slow API call again \u2192 failure again</p> <p>After: - Ocean location at (34.24, 144.31, 2010) \u2192 slow API call \u2192 failure \u2192 cached as None - Same location at (34.24, 144.31, 2010) \u2192 instant cache lookup \u2192 failure (cached)</p> <p>Implementation:</p> <pre><code># earth_engine.py\nif use_cache and cache_key in _cache:\n    cached_value = _cache[cache_key]\n    if cached_value is None:\n        raise ValueError(f\"No embedding found... (cached failure)\")\n    return cached_value\n\n# Cache failures\nexcept ValueError as e:\n    if use_cache and \"No embedding found\" in str(e):\n        _cache[cache_key] = None  # Cache the failure\n    raise\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#2-verbose-logging","title":"2. Verbose Logging","text":"<p>Before (with tqdm):</p> <pre><code>Processing samples: 45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | 135/300 [02:15&lt;02:45, 1.00row/s]\n</code></pre> <p>No visibility into: - What coordinates are being processed - Whether cache or API was used - Why failures occurred</p> <p>After:</p> <pre><code>Processing samples: 45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              | 135/300 [02:15&lt;02:45, 1.00row/s]\nRow 132: \u2713 Got embedding for (35.1180,138.9370) year=2017 [CACHE]\nRow 133: \u2717 No embedding for (34.2400,144.3100) year=1998 [API] - trying fallback year 2020\nRow 133: \u2717 No embedding for (34.2400,144.3100) year=2020 [API - FALLBACK] - SKIPPING\nRow 134: \u2713 Got embedding for (36.0540,140.1230) year=2015 [API]\nRow 135: \u2717 No embedding for (34.2400,144.3100) year=1998 [CACHED FAILURE] - trying fallback year 2020\nRow 135: \u2717 No embedding for (34.2400,144.3100) year=2020 [CACHED FAILURE - FALLBACK] - SKIPPING\n</code></pre> <p>Now you can see: - Coordinates: Exact lat/lon being processed - Year: Original year and fallback year - Source: <code>[CACHE]</code> vs <code>[API]</code> - instant vs slow - Status: \u2713 success vs \u2717 failure - Cached Failures: Instant failure on second attempt</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#3-performance-impact","title":"3. Performance Impact","text":"<p>Scenario: 300 rows with 50% ocean locations</p> <p>Before: - 150 ocean locations \u00d7 2 API calls (original + fallback) \u00d7 2 seconds = 600 seconds of API calls - Processing ocean samples multiple times \u2192 same slow failures</p> <p>After (first run): - 150 ocean locations \u00d7 2 API calls \u00d7 2 seconds = 600 seconds (same on first run) - But failures are cached</p> <p>After (second run or duplicate coordinates): - 150 ocean locations \u00d7 instant cache lookup = &lt; 1 second - 600x speedup for cached failures</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#4-statistics-tracking","title":"4. Statistics Tracking","text":"<p>The <code>ProcessingStats</code> class now correctly tracks: - Cache hits: Includes both successful cache hits AND cached failures - Cache misses: Only new API calls - API success: Successful new embeddings - API failures: Categorized by type (no coverage, rate limit, other)</p> <p>Example output:</p> <pre><code>=== Google Earth Embeddings Processing ===\nSuccessfully retrieved embeddings: 245\n  - From cache: 180\n  - From API: 65\n\nFailed to retrieve embeddings: 55\n  - No coverage/ocean: 50\n  - Rate limit (429): 0\n  - Other errors: 5\n\nSkipped rows:\n  - Already had embeddings: 0\n  - Invalid data (missing coords/dates): 0\n\nCache information:\n  - Total entries in cache: 350 (includes 50 cached failures)\n  - Cache directory: /Users/MAM/.cache/env-embeddings/google_earth\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#usage","title":"Usage","text":""},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#commands","title":"Commands","text":"<pre><code># Process 300 RANDOM rows - failures are now fast after first run\n# Note: Uses random sampling (not sequential) to avoid bias\nuv run env-embeddings add-google-embeddings-csv \\\n  data/satisfying_biosamples_normalized.csv \\\n  --max-rows 300 \\\n  --output data/temp_with_google_300.csv\n\nuv run env-embeddings add-envo-embeddings-csv \\\n  data/temp_with_google_300.csv \\\n  --max-rows 300 \\\n  --output data/with_both_embeddings_300.csv\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#random-sampling-new-in-v2","title":"Random Sampling (New in v2)","text":"<p>Both <code>add-google-embeddings-csv</code> and <code>add-envo-embeddings-csv</code> now use random sampling instead of sequential processing:</p> <p>Before:</p> <pre><code>df.head(max_rows)  # First N rows\n</code></pre> <p>After:</p> <pre><code>df.sample(n=min(max_rows, len(df)), random_state=42)  # Random N rows\n</code></pre> <p>Why this matters: - Avoids bias from data ordering (e.g., all samples from one region at start of file) - Provides representative sample across geographic/temporal diversity - Uses fixed random seed (42) for reproducibility - Critical for scientific validity when sampling subsets</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#what-to-expect","title":"What to Expect","text":"<p>First run with new data: - Ocean/no-coverage locations will be slow (2 API calls each) - But you'll see exactly which coordinates are failing and why - Failures are cached for next time</p> <p>Subsequent runs or duplicate coordinates: - Cached failures are instant - Progress bar moves much faster through known-bad locations - Verbose output shows <code>[CACHED FAILURE]</code> for instant failures</p>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#debugging-failed-requests","title":"Debugging Failed Requests","text":"<p>With the new verbose logging, you can:</p> <ol> <li> <p>Identify problem coordinates: <code>Row 133: \u2717 No embedding for (34.2400,144.3100) year=1998 [API] - SKIPPING</code>    \u2192 This is an ocean location, no coverage</p> </li> <li> <p>See year issues: <code>Row 142: \u2717 No embedding for (51.5074,-0.1278) year=1985 [API] - trying fallback year 2020    Row 142: \u2713 Got embedding for (51.5074,-0.1278) year=2020 [API - FALLBACK]</code>    \u2192 1985 has no data, but 2020 works</p> </li> <li> <p>Spot patterns:</p> </li> <li>Multiple failures with similar coordinates \u2192 might be ocean region</li> <li>All failures in certain year range \u2192 dataset coverage issue</li> <li>Specific error messages \u2192 can debug with Google Earth Engine docs</li> </ol>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#cache-management","title":"Cache Management","text":""},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#view-cache","title":"View Cache","text":"<pre><code># Google Earth cache\nls -lh ~/.cache/env-embeddings/google_earth/\n\n# ENVO cache\nls -lh ~/.cache/env-embeddings/envo/\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#clear-cache-if-needed","title":"Clear Cache (if needed)","text":"<pre><code>from env_embeddings.earth_engine import clear_cache\nclear_cache()\n</code></pre> <p>Or manually:</p> <pre><code>rm -rf ~/.cache/env-embeddings/google_earth/\nrm -rf ~/.cache/env-embeddings/envo/\n</code></pre>"},{"location":"development/PERFORMANCE_OPTIMIZATIONS/#benefits","title":"Benefits","text":"<p>\u2705 Fast failures - Cached failures are instant (600x faster) \u2705 Detailed logging - See exactly what's happening with each request \u2705 Better debugging - Coordinates, years, and error types visible \u2705 Cache visibility - Know when cache vs API is used \u2705 Accurate statistics - Proper tracking of cache hits, API calls, failures \u2705 No behavior change - Same results, just faster and more visible</p>"},{"location":"development/improvements_summary/","title":"Improvements Summary","text":""},{"location":"development/improvements_summary/#overview","title":"Overview","text":"<p>This document summarizes the major improvements made to the env-embeddings project to enhance performance, reliability, and usability when processing large-scale biological sample datasets.</p>"},{"location":"development/improvements_summary/#key-improvements","title":"Key Improvements","text":""},{"location":"development/improvements_summary/#1-removed-ocean-pre-filtering","title":"1. Removed Ocean Pre-filtering \u2705","text":"<p>Problem: The <code>global-land-mask</code> pre-filtering was too aggressive and skipped valid coastal samples that might have satellite coverage.</p> <p>Solution: Removed the ocean filtering step entirely. Google Earth Engine is now the source of truth for coverage - if a location has no satellite data, GEE will tell us directly. This maximizes coverage while still being efficient since failures are cached.</p> <p>Impact: Maximal coverage of samples, including coastal and shallow water locations that have satellite imagery.</p>"},{"location":"development/improvements_summary/#2-added-retry-logic-with-exponential-backoff","title":"2. Added Retry Logic with Exponential Backoff \u2705","text":"<p>Problem: No handling for HTTP 429 \"Too Many Requests\" errors from Google Earth Engine API.</p> <p>Solution: Implemented <code>_retry_with_exponential_backoff()</code> function in <code>earth_engine.py</code> that: - Detects rate limit errors (429, \"too many requests\", \"quota\") - Retries up to 5 times with exponential backoff (1s, 2s, 4s, 8s, 16s) - Immediately raises non-rate-limit errors without retry - Wraps all Earth Engine API calls</p> <p>Code Location: <code>src/env_embeddings/earth_engine.py:57-93</code></p> <p>Impact: Robust handling of rate limits, automatic recovery from temporary quota issues.</p>"},{"location":"development/improvements_summary/#3-comprehensive-statistics-tracking","title":"3. Comprehensive Statistics Tracking \u2705","text":"<p>Problem: Limited visibility into processing - only final counts were shown.</p> <p>Solution: Created <code>ProcessingStats</code> dataclass in <code>sample_processor.py</code> that tracks: - Cache hits vs cache misses - API successes - API failures by type:   - No coverage/ocean locations   - Rate limit (429) errors   - Other errors - Rows skipped (existing embeddings) - Rows skipped (invalid data)</p> <p>Code Location: <code>src/env_embeddings/sample_processor.py:16-46</code></p> <p>Impact: Detailed visibility into processing efficiency, cache effectiveness, and failure reasons.</p>"},{"location":"development/improvements_summary/#4-coordinate-normalization-for-cache-consistency","title":"4. Coordinate Normalization for Cache Consistency \u2705","text":"<p>Problem: Coordinates with different representations (35.1180 vs 35.118) would create duplicate cache entries for semantically identical locations.</p> <p>Solution: All coordinates are normalized to 4 decimal places before: - Creating cache keys - Making Earth Engine API calls</p> <p>Code Location: <code>src/env_embeddings/earth_engine.py:126-132</code></p> <p>Example:</p> <pre><code>lat_normalized = round(float(lat), 4)  # 35.1180 \u2192 35.118\nlon_normalized = round(float(lon), 4)  # 138.9370 \u2192 138.937\ncache_key = (lat_normalized, lon_normalized, year)\n</code></pre> <p>Impact: Reduced duplicate API calls, more efficient caching, consistent coordinate representation.</p>"},{"location":"development/improvements_summary/#5-automatic-filtering-of-incomplete-rows","title":"5. Automatic Filtering of Incomplete Rows \u2705","text":"<p>Problem: Output files contained rows with missing embeddings, requiring manual cleanup.</p> <p>Solution: Both <code>add_google_earth_embeddings_to_csv()</code> and <code>add_envo_embeddings_to_csv()</code> now: - Automatically filter out rows with missing embeddings before saving - Report filtering statistics (rows before/after, rows removed) - For ENVO: only keep rows with ALL 3 embeddings (broad_scale, medium, local_scale) - For Google Earth: only keep rows with valid satellite embeddings</p> <p>Code Locations: - Google Earth: <code>src/env_embeddings/sample_processor.py:604-626</code> - ENVO: <code>src/env_embeddings/sample_processor.py:791-815</code></p> <p>Impact: Clean output files ready for analysis, no manual post-processing needed.</p>"},{"location":"development/improvements_summary/#updated-workflow","title":"Updated Workflow","text":""},{"location":"development/improvements_summary/#step-1-add-google-earth-embeddings","title":"Step 1: Add Google Earth Embeddings","text":"<pre><code>uv run env-embeddings add-google-embeddings-csv \\\n  data/satisfying_biosamples_normalized.csv \\\n  --max-rows 50 \\\n  --output data/temp_with_google.csv\n</code></pre> <p>Output Statistics:</p> <pre><code>=== Google Earth Embeddings Processing ===\nSuccessfully retrieved embeddings: 45\n  - From cache: 12\n  - From API: 33\n\nFailed to retrieve embeddings: 5\n  - No coverage/ocean: 3\n  - Rate limit (429): 0\n  - Other errors: 2\n\nSkipped rows:\n  - Already had embeddings: 0\n  - Invalid data (missing coords/dates): 0\n\nCache information:\n  - Total entries in cache: 145\n  - Cache directory: /Users/MAM/.cache/env-embeddings/google_earth\n\nOutput filtering:\n  - Rows before filtering: 50\n  - Rows after filtering: 45\n  - Rows removed (incomplete embeddings): 5\n</code></pre>"},{"location":"development/improvements_summary/#step-2-add-envo-embeddings","title":"Step 2: Add ENVO Embeddings","text":"<pre><code>uv run env-embeddings add-envo-embeddings-csv \\\n  data/temp_with_google.csv \\\n  --max-rows 50 \\\n  --output data/with_both_embeddings.csv\n</code></pre> <p>Output Statistics:</p> <pre><code>=== ENVO Embeddings Processing ===\nSuccessfully retrieved embeddings: 135 (45 rows \u00d7 3 columns)\n  - From cache: 87\n  - From API: 48\n\nFailed to retrieve embeddings: 15\n  - No coverage/ocean: 0\n  - Rate limit (429): 0\n  - Other errors: 15\n\nCache information:\n  - Total unique ENVO terms cached: 78\n  - Successful embeddings: 65\n  - Failed lookups: 13\n  - Cache directory: /Users/MAM/.cache/env-embeddings/envo\n\nOutput filtering:\n  - Rows before filtering: 45\n  - Rows after filtering: 40\n  - Rows removed (incomplete embeddings): 5\n</code></pre>"},{"location":"development/improvements_summary/#combined-workflow-just-recipe","title":"Combined Workflow (Just Recipe)","text":"<pre><code>just add-both-embeddings\n</code></pre> <p>This runs both steps automatically with 50 test rows.</p>"},{"location":"development/improvements_summary/#technical-details","title":"Technical Details","text":""},{"location":"development/improvements_summary/#cache-implementation","title":"Cache Implementation","text":"<ul> <li>Library: <code>diskcache</code> (SQLite-backed persistent cache)</li> <li>Google Earth Cache: <code>~/.cache/env-embeddings/google_earth/</code></li> <li>ENVO Cache: <code>~/.cache/env-embeddings/envo/</code></li> <li>Cache Keys:</li> <li>Google Earth: <code>(lat_norm, lon_norm, year)</code></li> <li>ENVO: <code>envo_term</code> string (e.g., \"ENVO:00000428\")</li> </ul>"},{"location":"development/improvements_summary/#progress-bar-updates","title":"Progress Bar Updates","text":"<p>Real-time progress information shows: - Success count - Cache hits - Failed count</p> <p>Example: <code>Success: 42, Cache: 28, Failed: 3</code></p>"},{"location":"development/improvements_summary/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"development/improvements_summary/#cache-efficiency","title":"Cache Efficiency","text":"<ul> <li>First run: ~0.4 rows/second (making API calls)</li> <li>Subsequent runs: ~906 rows/second (from cache)</li> <li>2265x speedup with cache</li> </ul>"},{"location":"development/improvements_summary/#rate-limiting","title":"Rate Limiting","text":"<ul> <li>Automatic exponential backoff for 429 errors</li> <li>Default retry parameters: 5 attempts, initial 1s delay</li> <li>Maximum total wait time: ~31 seconds (1+2+4+8+16)</li> </ul>"},{"location":"development/improvements_summary/#coordinate-normalization-benefits","title":"Coordinate Normalization Benefits","text":"<ul> <li>Reduces cache size by eliminating duplicates</li> <li>Ensures consistent results across different input formats</li> <li>4 decimal places = ~11 meters precision (sufficient for 10m satellite resolution)</li> </ul>"},{"location":"development/improvements_summary/#files-modified","title":"Files Modified","text":"<ol> <li><code>src/env_embeddings/earth_engine.py</code></li> <li>Added retry logic with exponential backoff</li> <li>Added coordinate normalization</li> <li> <p>Improved error messages</p> </li> <li> <p><code>src/env_embeddings/sample_processor.py</code></p> </li> <li>Added <code>ProcessingStats</code> dataclass</li> <li>Removed ocean pre-filtering</li> <li>Added comprehensive statistics tracking</li> <li>Added automatic filtering of incomplete rows</li> <li> <p>Updated both CSV processing functions</p> </li> <li> <p><code>src/env_embeddings/envo_embeddings.py</code></p> </li> <li> <p>No changes (already had caching)</p> </li> <li> <p><code>project.justfile</code></p> </li> <li>Simplified <code>add-both-embeddings</code> recipe (hardcoded 50 rows)</li> </ol>"},{"location":"development/improvements_summary/#next-steps","title":"Next Steps","text":""},{"location":"development/improvements_summary/#for-production-runs","title":"For Production Runs","text":"<ol> <li>Remove <code>--max-rows</code> limit to process all samples</li> <li>Monitor cache statistics to track efficiency</li> <li>Check rate limit statistics to ensure no quota issues</li> <li>Verify final row counts match expectations</li> </ol>"},{"location":"development/improvements_summary/#potential-future-improvements","title":"Potential Future Improvements","text":"<ol> <li>Parallel processing for faster throughput</li> <li>Batch API requests to reduce round-trips</li> <li>Configurable retry parameters (max_retries, initial_delay)</li> <li>Progress persistence (resume from failure)</li> <li>Separate cache validation/cleaning commands</li> </ol>"},{"location":"development/improvements_summary/#testing-recommendations","title":"Testing Recommendations","text":"<pre><code># Test with small dataset (50 rows)\njust add-both-embeddings\n\n# Test with larger dataset (500 rows)\nuv run env-embeddings add-google-embeddings-csv data/satisfying_biosamples_normalized.csv \\\n  --max-rows 500 --output data/test_google_500.csv\n\nuv run env-embeddings add-envo-embeddings-csv data/test_google_500.csv \\\n  --max-rows 500 --output data/test_both_500.csv\n\n# Full production run (no max-rows limit)\nuv run env-embeddings add-google-embeddings-csv data/satisfying_biosamples_normalized.csv \\\n  --output data/full_with_google.csv\n\nuv run env-embeddings add-envo-embeddings-csv data/full_with_google.csv \\\n  --output data/full_with_both.csv\n</code></pre>"},{"location":"development/improvements_summary/#summary","title":"Summary","text":"<p>All requested improvements have been implemented:</p> <p>\u2705 Remove ocean pre-filter - Maximal coverage, let GEE be source of truth \u2705 Add retry logic - Exponential backoff for 429 errors \u2705 Detailed reporting - Cache hits, API success/failures by type \u2705 Track rate limits - 429 errors tracked separately \u2705 Filter incomplete rows - Automatic cleanup of output \u2705 Coordinate normalization - 4 decimal places for cache consistency</p> <p>The system is now fast, efficient, provides maximum coverage, and gives comprehensive visibility into the processing pipeline.</p>"},{"location":"research/RESEARCH_VISION/","title":"Research Vision: Environmental Context Quality Control and Prediction","text":""},{"location":"research/RESEARCH_VISION/#the-big-picture","title":"The Big Picture","text":"<p>You're building a quality control and recommendation system for environmental metadata using the divergence between satellite imagery (objective, physical) and ontological annotations (subjective, curator-provided).</p>"},{"location":"research/RESEARCH_VISION/#core-insight","title":"Core Insight","text":"<p>Disagreement between Google Earth embeddings and ENVO embeddings is informative:</p> <ul> <li>High GE similarity + High ENVO similarity \u2192 Good metadata, samples are genuinely similar</li> <li>High GE similarity + Low ENVO similarity \u2192 Metadata quality issue - physically similar locations have inconsistent annotations</li> <li>Low GE similarity + High ENVO similarity \u2192 Different locations can share environmental context (e.g., both \"forest\" but different continents)</li> </ul>"},{"location":"research/RESEARCH_VISION/#three-phase-research-pipeline","title":"Three-Phase Research Pipeline","text":""},{"location":"research/RESEARCH_VISION/#phase-1-find-optimal-envo-combination-current-work","title":"Phase 1: Find Optimal ENVO Combination \u2705 (Current Work)","text":"<p>Goal: Determine which combination of MIxS environmental context variables best predicts satellite-based similarity</p> <p>Approaches: 1. Individual scales: <code>broad_scale</code>, <code>medium</code>, <code>local_scale</code> 2. Concatenation: <code>[broad; medium; local]</code> (4608-dim) 3. Weighted average: <code>(broad + medium + local) / 3</code> (1536-dim) 4. Learned weights: <code>w1\u00b7broad + w2\u00b7medium + w3\u00b7local</code> (future)</p> <p>Success Metric: Highest Pearson correlation with GE embeddings after removing degenerate pairs</p> <p>Current Status: Implemented in notebook, will reveal best combination</p>"},{"location":"research/RESEARCH_VISION/#phase-2-identify-and-analyze-outliers-next-step","title":"Phase 2: Identify and Analyze Outliers \ud83c\udfaf (Next Step)","text":"<p>Goal: Find samples with suspicious metadata by detecting divergence between physical and semantic similarity</p>"},{"location":"research/RESEARCH_VISION/#2a-high-ge-low-envo-outliers-metadata-quality-issues","title":"2A. High GE / Low ENVO Outliers (Metadata Quality Issues)","text":"<p>Example Scenario:</p> <pre><code>Sample A: (35.7\u00b0N, 139.8\u00b0E, 2018) - ENVO: \"terrestrial biome | soil | agricultural field\"\nSample B: (35.7\u00b0N, 139.8\u00b0E, 2018) - ENVO: \"marine biome | sea water | coastal zone\"\n\nGE Similarity: 0.99 (same location, same date \u2192 nearly identical satellite view)\nENVO Similarity: 0.25 (completely different biomes!)\n\n\u2192 FLAG: One of these annotations is wrong\n</code></pre> <p>Detection Method:</p> <pre><code># Define outliers as pairs in the upper-right quadrant\noutliers_metadata_issue = pairs_filtered[\n    (pairs_filtered['ge_similarity'] &gt; 0.8) &amp;  # Physically very similar\n    (pairs_filtered['envo_best_similarity'] &lt; 0.3)  # Semantically very different\n]\n</code></pre> <p>Analysis Questions: 1. How many outlier pairs exist? 2. Do certain biosamples appear repeatedly in outlier pairs? (chronic bad metadata) 3. Are there patterns? (e.g., marine/terrestrial confusion, missing terms)</p>"},{"location":"research/RESEARCH_VISION/#2b-low-ge-high-envo-outliers-interesting-biology","title":"2B. Low GE / High ENVO Outliers (Interesting Biology)","text":"<p>Example Scenario:</p> <pre><code>Sample A: (34.0\u00b0N, 118.2\u00b0W) - California desert\nSample B: (31.2\u00b0N, 29.9\u00b0E) - Egyptian desert\n\nGE Similarity: 0.15 (different continents, different geology)\nENVO Similarity: 0.95 (both \"desert biome | sand | arid soil\")\n\n\u2192 VALID: Similar environment types in different locations\n</code></pre> <p>These are not errors - they show that ENVO terms successfully capture environmental similarity across geography.</p>"},{"location":"research/RESEARCH_VISION/#phase-3-metadata-prediction-and-improvement-future-work","title":"Phase 3: Metadata Prediction and Improvement \ud83d\ude80 (Future Work)","text":"<p>Goal: Use satellite imagery to propose or validate ENVO triads</p>"},{"location":"research/RESEARCH_VISION/#3a-predict-missing-triads","title":"3A. Predict Missing Triads","text":"<p>Scenario: Sample has coordinates/date but missing ENVO terms</p> <p>Method:</p> <pre><code>def predict_envo_triad(sample_coords, sample_date, training_data):\n    \"\"\"\n    Predict ENVO triad for a sample with missing metadata.\n\n    1. Get Google Earth embedding for target sample\n    2. Find k-nearest neighbors by GE embedding similarity\n    3. Aggregate their ENVO triads (majority vote or weighted average)\n    4. Return predicted broad/medium/local scale terms\n    \"\"\"\n\n    # Get GE embedding for target\n    target_ge_embedding = get_embedding(lat, lon, year)\n\n    # Find similar samples by satellite imagery\n    similarities = []\n    for train_sample in training_data:\n        sim = cosine_similarity(target_ge_embedding, train_sample.ge_embedding)\n        similarities.append((sim, train_sample))\n\n    # Get top-k most similar samples\n    top_k = sorted(similarities, reverse=True)[:10]\n\n    # Aggregate their ENVO terms (majority vote)\n    broad_candidates = [s.envo_broad for _, s in top_k]\n    medium_candidates = [s.envo_medium for _, s in top_k]\n    local_candidates = [s.envo_local for _, s in top_k]\n\n    return {\n        'broad_scale': most_common(broad_candidates),\n        'medium': most_common(medium_candidates),\n        'local_scale': most_common(local_candidates),\n        'confidence': top_k[0][0]  # similarity to nearest neighbor\n    }\n</code></pre> <p>Validation: - Hold out 20% of samples with known triads - Predict their triads using only GE embeddings - Measure accuracy: How often do we get the exact triad correct?</p>"},{"location":"research/RESEARCH_VISION/#3b-flag-suspect-triads","title":"3B. Flag Suspect Triads","text":"<p>Scenario: Sample has ENVO triad but it disagrees with satellite imagery</p> <p>Method:</p> <pre><code>def flag_suspect_triads(sample, training_data, threshold=0.7):\n    \"\"\"\n    Flag samples whose ENVO triad is inconsistent with their satellite imagery.\n\n    Returns:\n        List of alternative triad suggestions with confidence scores\n    \"\"\"\n\n    # Find samples with similar GE embeddings (physically similar)\n    similar_by_satellite = find_similar_samples(\n        sample.ge_embedding,\n        training_data,\n        similarity_threshold=0.8\n    )\n\n    # Check if their ENVO triads agree\n    triad_agreement = []\n    for sim_sample in similar_by_satellite:\n        envo_sim = cosine_similarity(\n            sample.envo_embedding,\n            sim_sample.envo_embedding\n        )\n        triad_agreement.append(envo_sim)\n\n    avg_agreement = mean(triad_agreement)\n\n    if avg_agreement &lt; threshold:\n        # This sample's triad disagrees with physically similar samples\n        # Propose alternative triads from the similar samples\n        alternative_triads = aggregate_triads(similar_by_satellite)\n\n        return {\n            'flagged': True,\n            'reason': f'Low ENVO agreement ({avg_agreement:.2f}) with physically similar samples',\n            'current_triad': sample.envo_triad,\n            'suggested_alternatives': alternative_triads,\n            'similar_samples': [s.accession for s in similar_by_satellite[:5]]\n        }\n\n    return {'flagged': False}\n</code></pre> <p>Use Cases: 1. Quality Control: Curators review flagged samples before publication 2. Batch Correction: Identify systematic errors (e.g., swapped broad/local scales) 3. Metadata Enrichment: Suggest triads for older samples lacking structured metadata</p>"},{"location":"research/RESEARCH_VISION/#3c-active-learning-for-metadata-improvement","title":"3C. Active Learning for Metadata Improvement","text":"<p>Goal: Prioritize which samples to manually review</p> <p>Method:</p> <pre><code>def prioritize_for_manual_review(samples):\n    \"\"\"\n    Rank samples by potential metadata impact.\n\n    Priority factors:\n    1. High GE/ENVO disagreement (likely error)\n    2. Many other samples depend on this one (high influence)\n    3. Rare environmental context (filling gaps in training data)\n    \"\"\"\n\n    scores = []\n    for sample in samples:\n        # Factor 1: Disagreement score\n        disagreement = calculate_ge_envo_disagreement(sample)\n\n        # Factor 2: Influence score (how many samples are similar to this one?)\n        influence = count_similar_samples(sample, threshold=0.8)\n\n        # Factor 3: Rarity score (is this an underrepresented environment?)\n        rarity = calculate_envo_rarity(sample.envo_triad)\n\n        # Combined priority score\n        priority = 0.5 * disagreement + 0.3 * influence + 0.2 * rarity\n        scores.append((priority, sample))\n\n    return sorted(scores, reverse=True)\n</code></pre>"},{"location":"research/RESEARCH_VISION/#concrete-next-steps","title":"Concrete Next Steps","text":""},{"location":"research/RESEARCH_VISION/#immediate-this-week","title":"Immediate (This Week)","text":"<ol> <li>Run updated notebook to determine best ENVO combination</li> <li>Add outlier detection cell to identify high-GE/low-ENVO pairs</li> <li>Analyze outlier patterns:</li> <li>Which samples appear most frequently?</li> <li>Are there systematic errors (e.g., marine/terrestrial swaps)?</li> <li>Manual inspection of top 10 outlier pairs</li> </ol>"},{"location":"research/RESEARCH_VISION/#short-term-next-sprint","title":"Short-term (Next Sprint)","text":"<ol> <li>Build triad prediction prototype:</li> <li>K-nearest neighbors by GE embedding</li> <li>Majority vote for ENVO terms</li> <li> <p>Cross-validation to measure accuracy</p> </li> <li> <p>Create suspect triad report:</p> </li> <li>List samples with low ENVO agreement among GE-similar samples</li> <li>Generate suggested alternative triads</li> <li>Export for manual review</li> </ol>"},{"location":"research/RESEARCH_VISION/#medium-term-research-paper","title":"Medium-term (Research Paper)","text":"<ol> <li>Systematic evaluation:</li> <li>Holdout test set (20% of samples)</li> <li>Measure triad prediction accuracy</li> <li> <p>Compare to baseline (random, most-frequent, etc.)</p> </li> <li> <p>Case studies:</p> </li> <li>Manual review of flagged samples</li> <li>Work with domain experts to validate predictions</li> <li> <p>Quantify metadata improvement</p> </li> <li> <p>Scale up:</p> </li> <li>Process full BioSample database (not just 246 samples)</li> <li>Build web interface for curators</li> <li>Integrate with submission pipelines</li> </ol>"},{"location":"research/RESEARCH_VISION/#example-analysis-outlier-investigation","title":"Example Analysis: Outlier Investigation","text":"<p>Notebook cell to add:</p> <pre><code>print(\"=== OUTLIER ANALYSIS: METADATA QUALITY ISSUES ===\")\n\n# Define outliers: High GE similarity but Low ENVO similarity\noutliers = pairs_filtered[\n    (pairs_filtered['ge_similarity'] &gt; 0.8) &amp;\n    (pairs_filtered['envo_best_similarity'] &lt; 0.3)  # Use best ENVO combination\n]\n\nprint(f\"\\nFound {len(outliers)} outlier pairs (high GE, low ENVO)\")\nprint(f\"  {len(outliers)/len(pairs_filtered)*100:.1f}% of all pairs\\n\")\n\n# Identify samples that appear frequently in outlier pairs\nfrom collections import Counter\n\noutlier_samples = []\nfor _, pair in outliers.iterrows():\n    outlier_samples.append(pair['accession_1'])\n    outlier_samples.append(pair['accession_2'])\n\nsample_counts = Counter(outlier_samples)\nchronic_outliers = sample_counts.most_common(10)\n\nprint(\"Samples appearing most frequently in outlier pairs:\")\nprint(\"(These may have incorrect metadata)\\n\")\nfor accession, count in chronic_outliers:\n    sample = df_clean[df_clean['accession'] == accession].iloc[0]\n    print(f\"{accession}: {count} outlier pairs\")\n    print(f\"  Location: ({sample['latitude']}, {sample['longitude']})\")\n    print(f\"  ENVO: {sample['env_broad_scale']} | {sample['env_medium']} | {sample['env_local_scale']}\")\n    print()\n\n# Show example outlier pairs\nprint(\"\\n=== EXAMPLE OUTLIER PAIRS (for manual inspection) ===\")\nfor idx, (_, pair) in enumerate(outliers.head(5).iterrows()):\n    print(f\"\\nOutlier {idx+1}:\")\n    print(f\"  GE Similarity: {pair['ge_similarity']:.3f}\")\n    print(f\"  ENVO Similarity: {pair['envo_best_similarity']:.3f}\")\n    print(f\"  \u0394 (disagreement): {pair['ge_similarity'] - pair['envo_best_similarity']:.3f}\")\n\n    sample1 = df_clean[df_clean['accession'] == pair['accession_1']].iloc[0]\n    sample2 = df_clean[df_clean['accession'] == pair['accession_2']].iloc[0]\n\n    print(f\"\\n  Sample 1: {sample1['accession']}\")\n    print(f\"    Location: ({sample1['latitude']:.4f}, {sample1['longitude']:.4f})\")\n    print(f\"    ENVO: {sample1['env_broad_scale']} | {sample1['env_medium']} | {sample1['env_local_scale']}\")\n\n    print(f\"\\n  Sample 2: {sample2['accession']}\")\n    print(f\"    Location: ({sample2['latitude']:.4f}, {sample2['longitude']:.4f})\")\n    print(f\"    ENVO: {sample2['env_broad_scale']} | {sample2['env_medium']} | {sample2['env_local_scale']}\")\n\n    print(f\"\\n  \u2192 DIAGNOSIS: Samples are physically very similar (GE={pair['ge_similarity']:.2f})\")\n    print(f\"               but have very different ENVO terms (ENVO={pair['envo_best_similarity']:.2f})\")\n    print(f\"               One of these metadata annotations is likely incorrect.\")\n</code></pre>"},{"location":"research/RESEARCH_VISION/#expected-outcomes","title":"Expected Outcomes","text":""},{"location":"research/RESEARCH_VISION/#scientific-contributions","title":"Scientific Contributions","text":"<ol> <li> <p>Quantify metadata quality: What % of BioSample metadata is inconsistent with satellite imagery?</p> </li> <li> <p>Automate quality control: Flag suspect samples before publication</p> </li> <li> <p>Metadata prediction: Provide triads for samples lacking structured annotations</p> </li> <li> <p>Training data curation: Identify high-quality samples for ML training</p> </li> </ol>"},{"location":"research/RESEARCH_VISION/#practical-impact","title":"Practical Impact","text":"<ol> <li> <p>Reduce curator burden: Automated suggestions reduce manual annotation time</p> </li> <li> <p>Improve data quality: Catch errors early in submission pipeline</p> </li> <li> <p>Enable better science: Higher quality metadata \u2192 better meta-analyses</p> </li> <li> <p>Fill metadata gaps: Millions of samples lack ENVO triads - we can predict them</p> </li> </ol>"},{"location":"research/RESEARCH_VISION/#technical-considerations","title":"Technical Considerations","text":""},{"location":"research/RESEARCH_VISION/#challenges","title":"Challenges","text":"<ol> <li>ENVO term granularity:</li> <li>\"forest\" vs \"temperate forest\" vs \"temperate coniferous forest\"</li> <li> <p>Should we compare at different ontology depths?</p> </li> <li> <p>Temporal changes:</p> </li> <li>Satellite imagery from 2018, sample collected in 2010</li> <li> <p>Land use changes (urban development, deforestation)</p> </li> <li> <p>Scale mismatch:</p> </li> <li>GE embeddings: 10m resolution</li> <li>ENVO broad_scale: biome-level (100s of km)</li> <li> <p>Which scale should agree?</p> </li> <li> <p>Training data bias:</p> </li> <li>Current 246 samples may not represent all environments</li> <li>Need larger, more diverse training set</li> </ol>"},{"location":"research/RESEARCH_VISION/#solutions","title":"Solutions","text":"<ol> <li>Hierarchical ENVO comparison:</li> <li>Compare at multiple ontology depths</li> <li> <p>Weight by specificity (more specific = higher weight)</p> </li> <li> <p>Temporal filtering:</p> </li> <li>Only use samples where collection_date \u2248 satellite_date</li> <li> <p>Or model temporal changes explicitly</p> </li> <li> <p>Multi-scale analysis:</p> </li> <li>Compare local_scale to local GE features</li> <li> <p>Compare broad_scale to regional GE patterns</p> </li> <li> <p>Active learning:</p> </li> <li>Iteratively expand training set</li> <li>Prioritize diverse environments</li> </ol>"},{"location":"research/RESEARCH_VISION/#summary-your-vision","title":"Summary: Your Vision","text":"<p>You're building a virtuous cycle for environmental metadata:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Satellite Imagery (Objective, Physical)            \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Google Earth Embeddings                            \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Compare with ENVO Embeddings (Semantic, Curated)   \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Identify Outliers (High GE / Low ENVO)             \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Flag Suspect Metadata                              \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Predict/Suggest Corrections                        \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Human Review &amp; Correction                          \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Improved Training Data                             \u2502\n\u2502  \u2193                                                   \u2502\n\u2502  Better Predictions (loop back to top)              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>This is quality control through cross-validation between independent data sources (satellite vs curator), using embedding similarity as the bridge.</p> <p>Brilliant research direction! \ud83d\ude80</p>"},{"location":"research/column_mapping/","title":"Column Mapping: Old TSV to New CSV","text":""},{"location":"research/column_mapping/#overview","title":"Overview","text":"<p>Mapping from <code>date_and_latlon_samples_extended_ONLY_ENVO_SAMPLE_100_with_both_embeddings.tsv</code> (old) to <code>satisfying_biosamples_normalized.csv</code> (new).</p>"},{"location":"research/column_mapping/#key-differences","title":"Key Differences","text":""},{"location":"research/column_mapping/#file-format","title":"File Format","text":"<ul> <li>Old: TSV (tab-separated)</li> <li>New: CSV (comma-separated)</li> </ul>"},{"location":"research/column_mapping/#dataset-size","title":"Dataset Size","text":"<ul> <li>Old: 99 rows</li> <li>New: 436,337 rows (much larger dataset)</li> </ul>"},{"location":"research/column_mapping/#column-mappings","title":"Column Mappings","text":""},{"location":"research/column_mapping/#date-fields","title":"Date Fields","text":"Old Column New Column Notes <code>date</code> <code>collection_date</code> Normalized date format <code>collection_date</code> <code>collection_date_raw</code> Raw date from source <p>Format: Both use ISO format (YYYY-MM-DD or YYYY-MM)</p>"},{"location":"research/column_mapping/#location-fields","title":"Location Fields","text":"Old Column New Column Notes <code>lat_lon</code> <code>lat_lon_raw</code> Combined lat/lon string (e.g., \"35.118 N 138.937 E\") <code>ncbi_lat_lon</code> <code>lat_lon_raw</code> NCBI version of lat/lon string N/A <code>latitude</code> NEW: Separate numeric latitude N/A <code>longitude</code> NEW: Separate numeric longitude <p>Key Difference: - Old file stores latitude and longitude in a single column as a formatted string (e.g., \"42.36 N 71.06 W\") - New file has both:   - <code>lat_lon_raw</code>: Original formatted string   - <code>latitude</code>: Numeric latitude (e.g., 35.118)   - <code>longitude</code>: Numeric longitude (e.g., 138.937)</p>"},{"location":"research/column_mapping/#accessionid-fields","title":"Accession/ID Fields","text":"Old Column New Column Notes <code>ncbi_biosample_accession_id</code> <code>accession</code> Biosample accession ID <code>genome_id</code> N/A MISSING in new file"},{"location":"research/column_mapping/#environmental-fields","title":"Environmental Fields","text":"Old Column New Column Notes <code>env_broad_med_local</code> <code>env_broad_scale</code>, <code>env_local_scale</code>, <code>env_medium</code> Split into 3 separate ENVO columns <p>Key Difference: - Old file: Combined environmental ontology terms in one column - New file: Separate columns for broad scale, local scale, and medium ENVO terms</p>"},{"location":"research/column_mapping/#missing-columns-in-new-file","title":"Missing Columns in New File","text":"<p>These columns exist in the old file but are NOT present in the new file: - <code>genome_id</code> - <code>ncbi_bioproject</code> - <code>domain</code>, <code>phylum</code>, <code>class</code>, <code>order</code>, <code>family</code>, <code>genus</code>, <code>species</code> (taxonomic hierarchy) - <code>geographic_location_harmonized</code> - <code>host_harmonized</code> - <code>isolation_source_harmonized</code> - <code>project_name</code> - <code>misc_attributes</code> - <code>google_earth_embeddings</code> - <code>envo_embeddings</code></p>"},{"location":"research/column_mapping/#notes-for-integration","title":"Notes for Integration","text":"<ol> <li>Embeddings will need to be generated for the new file (google_earth_embeddings and envo_embeddings)</li> <li>Latitude/longitude parsing is already done in new file (no need to parse string format)</li> <li>Genome ID missing - may need to join with another data source if genome info is required</li> <li>Environmental terms are more structured in new file (separate ENVO columns)</li> <li>Much larger dataset (436k vs 99 rows) - may impact processing time</li> </ol>"},{"location":"research/google_earth_embeddings/","title":"Google Earth Engine Embeddings","text":"<p>This document shows how to download Google Earth Engine embeddings using the methods from this codebase.</p>"},{"location":"research/google_earth_embeddings/#self-contained-code-snippet","title":"Self-Contained Code Snippet","text":"<pre><code>import ee\nfrom typing import List, Optional\n\ndef initialize_ee(project: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize Google Earth Engine authentication and session.\"\"\"\n    try:\n        ee.Authenticate()\n        if project:\n            ee.Initialize(project=project)\n        else:\n            ee.Initialize()\n    except Exception as e:\n        raise Exception(f\"Earth Engine initialization failed: {e}\")\n\ndef get_embedding(lat: float, lon: float, year: int, project: Optional[str] = None) -&gt; List[float]:\n    \"\"\"Return the 64-dimensional AlphaEarth embedding for the given lat/lon and year.\"\"\"\n    # Initialize if project provided\n    if project:\n        initialize_ee(project)\n\n    # Create point geometry\n    point = ee.Geometry.Point(lon, lat)\n\n    # Load Google's satellite embedding collection\n    collection = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL')\n\n    # Filter by year and location\n    start = f\"{year}-01-01\"\n    end = f\"{year+1}-01-01\"\n    filtered_collection = collection.filterDate(start, end).filterBounds(point)\n\n    # Get the first (and likely only) image for that year\n    image_for_year = filtered_collection.first()\n\n    # Sample the point to get embedding vector (bands A00-A63)\n    sampled = image_for_year.sample(region=point, scale=10).first()\n    band_dict = sampled.toDictionary().getInfo()\n\n    # Extract 64-dimensional embedding vector\n    embedding = [band_dict.get(f\"A{str(i).zfill(2)}\") for i in range(64)]\n\n    return embedding\n\n# Usage example:\n# initialize_ee(\"your-project-id\")\n# embedding = get_embedding(39.0372, -121.8036, 2024)\n# print(f\"64-dimensional vector: {embedding}\")\n</code></pre>"},{"location":"research/google_earth_embeddings/#key-points","title":"Key Points","text":"<ul> <li>Uses Google's <code>GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL</code> dataset</li> <li>Returns 64-dimensional AlphaEarth embeddings (bands A00-A63)</li> <li>Requires Earth Engine authentication and project setup</li> <li>Works with years 2017-2024 for best coverage</li> </ul>"},{"location":"research/google_earth_embeddings/#cli-usage","title":"CLI Usage","text":"<p>The codebase also provides a CLI interface:</p> <pre><code># Get embedding for specific coordinates and year\nuv run env-embeddings embedding --lat 39.0372 --lon -121.8036 --year 2024 --project env-embeddings-2025\n</code></pre> <p>This returns a 64-dimensional embedding vector representing satellite imagery features for that location and time.</p>"},{"location":"research/google_earth_embeddings/#setup-requirements","title":"Setup Requirements","text":"<ol> <li>Google Cloud project with Earth Engine API enabled</li> <li>Earth Engine authentication (<code>ee.Authenticate()</code>)</li> <li>Project ID for initialization</li> <li>Python package: <code>earthengine-api&gt;=1.6.8</code></li> </ol>"}]}